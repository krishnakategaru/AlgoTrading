{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM REGRESSION WITH OHLC AND 30 DAYS FEATURES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119 33\n",
      "Epoch 1/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 174ms/step - loss: 7.7857e-04 - mse: 7.7857e-04 - val_loss: 4.7307e-04 - val_mse: 4.7307e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 136ms/step - loss: 9.5424e-04 - mse: 9.5424e-04 - val_loss: 4.5200e-04 - val_mse: 4.5200e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - loss: 8.2470e-04 - mse: 8.2470e-04 - val_loss: 4.9062e-04 - val_mse: 4.9062e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 129ms/step - loss: 8.8748e-04 - mse: 8.8748e-04 - val_loss: 5.8304e-04 - val_mse: 5.8304e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 140ms/step - loss: 9.0810e-04 - mse: 9.0810e-04 - val_loss: 5.8767e-04 - val_mse: 5.8767e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 7.7219e-04 - val_mse: 7.7219e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 131ms/step - loss: 8.0023e-04 - mse: 8.0023e-04 - val_loss: 8.3473e-04 - val_mse: 8.3473e-04\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 295ms/step - loss: 7.9810e-04 - mse: 7.9810e-04\n",
      "[0.000672093708999455, 0.000672093708999455]\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 628ms/step\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-05-01'\n",
    "# Parameters\n",
    "batch_size = 256\n",
    "sequence_length = 30\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1980-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['pr_change_on_last_day'] = data['Close'].pct_change()\n",
    "    data['pr_change_on_current_day'] = data['pr_change_on_last_day'].shift(-1)\n",
    "    data.iloc[0,-2] = 0\n",
    "    # data['sentiment'] = pd.Series(np.where(data['pr_change_on_current_day'] > 0.025, 1, np.where(data['pr_change_on_current_day'] < -0.025, -1, 0)), index=data.index)\n",
    "    # data['perc_change'] = data['Percentage Change']\n",
    "    # # Drop any rows with missing values\n",
    "    # data.dropna(inplace=True)\n",
    "    # data.drop('pr_change_on_current_day',axis=1 , inplace=True)\n",
    "    return data\n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# Calculate deltas, moving averages, and Bollinger Bands\n",
    "for i in range(1, 30):\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "    stock[f\"rolling_mean_open_{i}day\"] = stock[\"Open\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_high_{i}day\"] = stock[\"High\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_low_{i}day\"] = stock[\"Low\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_close_{i}day\"] = stock[\"Close\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_std_open_{i}day\"] = stock[\"Open\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_high_{i}day\"] = stock[\"High\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_low_{i}day\"] = stock[\"Low\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_close_{i}day\"] = stock[\"Close\"].rolling(window=i).std()\n",
    "\n",
    "stock['fast_ma'] = stock['Close'].rolling(window=20).mean()\n",
    "stock['slow_ma'] = stock['Close'].rolling(window=50).mean()\n",
    "stock['bollinger_high'] = stock['Close'].rolling(window=20).mean() + (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['bollinger_low'] = stock['Close'].rolling(window=20).mean() - (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['ema'] = stock['Close'].ewm(span=20, adjust=False).mean()\n",
    "stock['envelope_high'] = stock['Close'].rolling(window=20).mean() * (1 + 0.05)\n",
    "stock['envelope_low'] = stock['Close'].rolling(window=20).mean() * (1 - 0.05)\n",
    "stock['macd_line'] = stock['Close'].ewm(span=12, adjust=False).mean() - stock['Close'].ewm(span=26, adjust=False).mean()\n",
    "stock['macd_signal'] = stock['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# RSI calculation\n",
    "def calculate_rsi(data, rsi_period):\n",
    "    delta = data['Close'].diff().dropna()\n",
    "    gain = delta.where(delta > 0, 0).dropna()\n",
    "    loss = -delta.where(delta < 0, 0).dropna()\n",
    "    avg_gain = gain.rolling(window=rsi_period).mean()\n",
    "    avg_loss = loss.rolling(window=rsi_period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "stock['rsi'] = calculate_rsi(stock, 14)\n",
    "\n",
    "# # Stochastic Oscillator calculation\n",
    "# def calculate_stochastic(data, k_window, d_window):\n",
    "#     high_low = data[['High', 'Low']]\n",
    "#     c = data['Close']\n",
    "#     highest = high_low.rolling(window=k_window).max()\n",
    "#     lowest = high_low.rolling(window=k_window).min()\n",
    "#     print(((c - lowest) / (highest - lowest)) * 100)\n",
    "#     stochastic_k = ((c - lowest) / (highest - lowest)) * 100\n",
    "#     stochastic_d = stochastic_k.rolling(window=d_window).mean()\n",
    "#     return stochastic_k, stochastic_d\n",
    "# stock['stochastic_k'], stock['stochastic_d'] = calculate_stochastic(stock, 14, 3)\n",
    "\n",
    "# stock['stochastic_k']= calculate_stochastic(stock, 14, 3)[0]\n",
    "# stock['stochastic_d']= calculate_stochastic(stock, 14, 3)[1]\n",
    "stock['day'] = pd.to_datetime(stock.index).day\n",
    "stock['month'] = pd.to_datetime(stock.index).month\n",
    "stock['year'] = pd.to_datetime(stock.index).year\n",
    "stock['weekday'] = pd.to_datetime(stock.index).weekday\n",
    "stock['dayofyear'] = pd.to_datetime(stock.index).dayofyear\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "stock.index = stock.index.date\n",
    "# Split the data into training and test sets\n",
    "\n",
    "# df = stock.copy()\n",
    "\n",
    "# # Calculate pairwise correlation\n",
    "# corr_matrix = df.corr()\n",
    "\n",
    "# # Identify highly correlated columns\n",
    "# redundant_cols = set()\n",
    "# for i in range(5,len(corr_matrix.columns)-1):\n",
    "#     for j in range(i+1, len(corr_matrix.columns)):\n",
    "#         if corr_matrix.iloc[i,j] > 0.8 and corr_matrix.columns[i] not in redundant_cols:\n",
    "#             redundant_cols.add(corr_matrix.columns[j])\n",
    "\n",
    "# # Remove one of the redundant columns\n",
    "# for col in redundant_cols:\n",
    "#     df = df.drop(col, axis=1)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df)\n",
    "\n",
    "stock = df.copy()\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:int(0.9*train_data_index)].copy()\n",
    "val_data  = stock.iloc[int(0.9*train_data_index)-sequence_length:train_data_index].copy()\n",
    "test_data = stock.iloc[train_data_index-sequence_length:].copy()\n",
    "train_data = label_data(train_data)\n",
    "val_data = label_data(val_data)\n",
    "test_data = label_data(test_data)\n",
    "train_data.fillna(0,axis = 0, inplace=True)\n",
    "val_data.fillna(0,axis = 0, inplace=True)\n",
    "test_data.fillna(0,axis = 0, inplace=True)\n",
    "\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "#trian & test data\n",
    "X_val_data = val_data.iloc[:,:-1]\n",
    "y_val_data = val_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data), len(X_test_data.columns))\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_val_data_normalizer = normalizer.fit_transform(X_val_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # # Reshape X_train_data_normalizer\n",
    "X_train_reshaped = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_val_reshaped = X_val_data_normalizer.reshape(X_val_data_normalizer.shape[0], X_val_data_normalizer.shape[1], 1)\n",
    "X_test_reshaped = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_train_data_normalizer,\n",
    "    y_train_data,\n",
    "    length = 3,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_val_data_normalizer,\n",
    "    y_val_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_test_data_normalizer,\n",
    "    y_test_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert targets to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_data, num_classes=3)\n",
    "y_val_onehot = to_categorical(y_val_data, num_classes=3)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    LSTM(256, input_shape=(sequence_length, X_train_data_normalizer.shape[1]), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(16, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(8, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(4, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    # Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1)  # Adjust num_classes according to your problem\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(patience=5, monitor='val_loss', mode='min', restore_best_weights=True)\n",
    "\n",
    "# Define model checkpoint to save the best model during training\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Train the model\n",
    "history = lstm_model.fit(train_dataset, epochs=100, batch_size=64, validation_data=val_dataset, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Load the best saved model\n",
    "best_model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "print(best_model.evaluate(test_dataset))\n",
    "test_predictions = best_model.predict(test_dataset)\n",
    "print(test_predictions.min())\n",
    "test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008436078\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM CLASSIFICATION MODEL WITH NO FEATURE ENGINEERING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119 23\n",
      "Epoch 1/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 91ms/step - accuracy: 0.4847 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.4937 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.5000 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.4807 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.4889 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - accuracy: 0.4852 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 7/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.4875 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 8/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.4836 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 9/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.4914 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 10/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.4944 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 11/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.4849 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 287ms/step\n",
      "Accuracy: 0.5160697887970616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       523\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.52      1.00      0.68       562\n",
      "\n",
      "    accuracy                           0.52      1089\n",
      "   macro avg       0.17      0.33      0.23      1089\n",
      "weighted avg       0.27      0.52      0.35      1089\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-05-01'\n",
    "# Parameters\n",
    "batch_size = 256\n",
    "sequence_length = 30\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1980-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['pr_change_on_last_day'] = data['Close'].pct_change()\n",
    "    data['pr_change_on_current_day'] = data['pr_change_on_last_day'].shift(-1)\n",
    "    data.iloc[0,-2] = 0\n",
    "    data['sentiment'] = pd.Series(np.where(data['pr_change_on_current_day'] > 0, 1, np.where(data['pr_change_on_current_day'] < 0, -1, 0)), index=data.index)\n",
    "    # data['perc_change'] = data['Percentage Change']\n",
    "    # # Drop any rows with missing values\n",
    "    # data.dropna(inplace=True)\n",
    "    data.drop('pr_change_on_current_day',axis=1 , inplace=True)\n",
    "    return data\n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# # Calculate deltas, moving averages, and Bollinger Bands\n",
    "# for i in range(1, 30):\n",
    "#     stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "#     stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "#     stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "#     stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "#     stock[f\"rolling_mean_open_{i}day\"] = stock[\"Open\"].rolling(window=i).mean()\n",
    "#     stock[f\"rolling_mean_high_{i}day\"] = stock[\"High\"].rolling(window=i).mean()\n",
    "#     stock[f\"rolling_mean_low_{i}day\"] = stock[\"Low\"].rolling(window=i).mean()\n",
    "#     stock[f\"rolling_mean_close_{i}day\"] = stock[\"Close\"].rolling(window=i).mean()\n",
    "#     stock[f\"rolling_std_open_{i}day\"] = stock[\"Open\"].rolling(window=i).std()\n",
    "#     stock[f\"rolling_std_high_{i}day\"] = stock[\"High\"].rolling(window=i).std()\n",
    "#     stock[f\"rolling_std_low_{i}day\"] = stock[\"Low\"].rolling(window=i).std()\n",
    "#     stock[f\"rolling_std_close_{i}day\"] = stock[\"Close\"].rolling(window=i).std()\n",
    "\n",
    "stock['fast_ma'] = stock['Close'].rolling(window=20).mean()\n",
    "stock['slow_ma'] = stock['Close'].rolling(window=50).mean()\n",
    "stock['bollinger_high'] = stock['Close'].rolling(window=20).mean() + (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['bollinger_low'] = stock['Close'].rolling(window=20).mean() - (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['ema'] = stock['Close'].ewm(span=20, adjust=False).mean()\n",
    "stock['envelope_high'] = stock['Close'].rolling(window=20).mean() * (1 + 0.05)\n",
    "stock['envelope_low'] = stock['Close'].rolling(window=20).mean() * (1 - 0.05)\n",
    "stock['macd_line'] = stock['Close'].ewm(span=12, adjust=False).mean() - stock['Close'].ewm(span=26, adjust=False).mean()\n",
    "stock['macd_signal'] = stock['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# RSI calculation\n",
    "def calculate_rsi(data, rsi_period):\n",
    "    delta = data['Close'].diff().dropna()\n",
    "    gain = delta.where(delta > 0, 0).dropna()\n",
    "    loss = -delta.where(delta < 0, 0).dropna()\n",
    "    avg_gain = gain.rolling(window=rsi_period).mean()\n",
    "    avg_loss = loss.rolling(window=rsi_period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "stock['rsi'] = calculate_rsi(stock, 14)\n",
    "\n",
    "# # Stochastic Oscillator calculation\n",
    "# def calculate_stochastic(data, k_window, d_window):\n",
    "#     high_low = data[['High', 'Low']]\n",
    "#     c = data['Close']\n",
    "#     highest = high_low.rolling(window=k_window).max()\n",
    "#     lowest = high_low.rolling(window=k_window).min()\n",
    "#     print(((c - lowest) / (highest - lowest)) * 100)\n",
    "#     stochastic_k = ((c - lowest) / (highest - lowest)) * 100\n",
    "#     stochastic_d = stochastic_k.rolling(window=d_window).mean()\n",
    "#     return stochastic_k, stochastic_d\n",
    "# stock['stochastic_k'], stock['stochastic_d'] = calculate_stochastic(stock, 14, 3)\n",
    "\n",
    "# stock['stochastic_k']= calculate_stochastic(stock, 14, 3)[0]\n",
    "# stock['stochastic_d']= calculate_stochastic(stock, 14, 3)[1]\n",
    "stock['day'] = pd.to_datetime(stock.index).day\n",
    "stock['month'] = pd.to_datetime(stock.index).month\n",
    "stock['year'] = pd.to_datetime(stock.index).year\n",
    "stock['weekday'] = pd.to_datetime(stock.index).weekday\n",
    "stock['dayofyear'] = pd.to_datetime(stock.index).dayofyear\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "stock.index = stock.index.date\n",
    "# Split the data into training and test sets\n",
    "\n",
    "# df = stock.copy()\n",
    "\n",
    "# # Calculate pairwise correlation\n",
    "# corr_matrix = df.corr()\n",
    "\n",
    "# # Identify highly correlated columns\n",
    "# redundant_cols = set()\n",
    "# for i in range(5,len(corr_matrix.columns)-1):\n",
    "#     for j in range(i+1, len(corr_matrix.columns)):\n",
    "#         if corr_matrix.iloc[i,j] > 0.8 and corr_matrix.columns[i] not in redundant_cols:\n",
    "#             redundant_cols.add(corr_matrix.columns[j])\n",
    "\n",
    "# # Remove one of the redundant columns\n",
    "# for col in redundant_cols:\n",
    "#     df = df.drop(col, axis=1)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df)\n",
    "\n",
    "# stock = df.copy()\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:int(0.9*train_data_index)].copy()\n",
    "val_data  = stock.iloc[int(0.9*train_data_index)-sequence_length:train_data_index].copy()\n",
    "test_data = stock.iloc[train_data_index-sequence_length:].copy()\n",
    "train_data = label_data(train_data)\n",
    "val_data = label_data(val_data)\n",
    "test_data = label_data(test_data)\n",
    "train_data.fillna(0,axis = 0, inplace=True)\n",
    "val_data.fillna(0,axis = 0, inplace=True)\n",
    "test_data.fillna(0,axis = 0, inplace=True)\n",
    "\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "#trian & test data\n",
    "X_val_data = val_data.iloc[:,:-1]\n",
    "y_val_data = val_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data), len(X_test_data.columns))\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_val_data_normalizer = normalizer.fit_transform(X_val_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # # Reshape X_train_data_normalizer\n",
    "X_train_reshaped = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_val_reshaped = X_val_data_normalizer.reshape(X_val_data_normalizer.shape[0], X_val_data_normalizer.shape[1], 1)\n",
    "X_test_reshaped = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_train_data_normalizer,\n",
    "    y_train_data,\n",
    "    length = 3,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_val_data_normalizer,\n",
    "    y_val_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_test_data_normalizer,\n",
    "    y_test_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert targets to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_data, num_classes=3)\n",
    "y_val_onehot = to_categorical(y_val_data, num_classes=3)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    LSTM(256, input_shape=(sequence_length, X_train_data_normalizer.shape[1]), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(16, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(8, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(4, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='softmax')  # 3 neurons for the 3 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='max', restore_best_weights=True)\n",
    "\n",
    "# Define model checkpoint to save the best model during training\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='max')\n",
    "\n",
    "# Train the model\n",
    "history = lstm_model.fit(train_dataset, epochs=100, batch_size=64, validation_data=val_dataset, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Load the best saved model\n",
    "best_model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = best_model.predict(test_dataset)\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data[sequence_length:], test_predictions)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print(classification_report(y_test_data[sequence_length:], test_predictions))\n",
    "test_predictions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM CLASSIFICATION WITH FEATURE ENGINEERING + INDICATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119 239\n",
      "Epoch 1/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 92ms/step - accuracy: 0.4898 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.5004 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4841 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.4947 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.4808 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - accuracy: 0.4976 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 7/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.4970 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 8/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - accuracy: 0.5022 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 9/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.4923 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 10/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - accuracy: 0.5039 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "Epoch 11/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.4858 - loss: 0.0000e+00 - val_accuracy: 0.5442 - val_loss: 0.0000e+00\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 299ms/step\n",
      "Accuracy: 0.5160697887970616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       523\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.52      1.00      0.68       562\n",
      "\n",
      "    accuracy                           0.52      1089\n",
      "   macro avg       0.17      0.33      0.23      1089\n",
      "weighted avg       0.27      0.52      0.35      1089\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-05-01'\n",
    "# Parameters\n",
    "batch_size = 256\n",
    "sequence_length = 30\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1980-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['pr_change_on_last_day'] = data['Close'].pct_change()\n",
    "    data['pr_change_on_current_day'] = data['pr_change_on_last_day'].shift(-1)\n",
    "    data.iloc[0,-2] = 0\n",
    "    data['sentiment'] = pd.Series(np.where(data['pr_change_on_current_day'] > 0, 1, np.where(data['pr_change_on_current_day'] < 0, -1, 0)), index=data.index)\n",
    "    # data['perc_change'] = data['Percentage Change']\n",
    "    # # Drop any rows with missing values\n",
    "    # data.dropna(inplace=True)\n",
    "    data.drop('pr_change_on_current_day',axis=1 , inplace=True)\n",
    "    return data\n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# Calculate deltas, moving averages, and Bollinger Bands\n",
    "for i in range(1, 90,5):\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "    stock[f\"rolling_mean_open_{i}day\"] = stock[\"Open\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_high_{i}day\"] = stock[\"High\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_low_{i}day\"] = stock[\"Low\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_close_{i}day\"] = stock[\"Close\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_std_open_{i}day\"] = stock[\"Open\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_high_{i}day\"] = stock[\"High\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_low_{i}day\"] = stock[\"Low\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_close_{i}day\"] = stock[\"Close\"].rolling(window=i).std()\n",
    "\n",
    "stock['fast_ma'] = stock['Close'].rolling(window=20).mean()\n",
    "stock['slow_ma'] = stock['Close'].rolling(window=50).mean()\n",
    "stock['bollinger_high'] = stock['Close'].rolling(window=20).mean() + (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['bollinger_low'] = stock['Close'].rolling(window=20).mean() - (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['ema'] = stock['Close'].ewm(span=20, adjust=False).mean()\n",
    "stock['envelope_high'] = stock['Close'].rolling(window=20).mean() * (1 + 0.05)\n",
    "stock['envelope_low'] = stock['Close'].rolling(window=20).mean() * (1 - 0.05)\n",
    "stock['macd_line'] = stock['Close'].ewm(span=12, adjust=False).mean() - stock['Close'].ewm(span=26, adjust=False).mean()\n",
    "stock['macd_signal'] = stock['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# RSI calculation\n",
    "def calculate_rsi(data, rsi_period):\n",
    "    delta = data['Close'].diff().dropna()\n",
    "    gain = delta.where(delta > 0, 0).dropna()\n",
    "    loss = -delta.where(delta < 0, 0).dropna()\n",
    "    avg_gain = gain.rolling(window=rsi_period).mean()\n",
    "    avg_loss = loss.rolling(window=rsi_period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "stock['rsi'] = calculate_rsi(stock, 14)\n",
    "\n",
    "# # Stochastic Oscillator calculation\n",
    "# def calculate_stochastic(data, k_window, d_window):\n",
    "#     high_low = data[['High', 'Low']]\n",
    "#     c = data['Close']\n",
    "#     highest = high_low.rolling(window=k_window).max()\n",
    "#     lowest = high_low.rolling(window=k_window).min()\n",
    "#     print(((c - lowest) / (highest - lowest)) * 100)\n",
    "#     stochastic_k = ((c - lowest) / (highest - lowest)) * 100\n",
    "#     stochastic_d = stochastic_k.rolling(window=d_window).mean()\n",
    "#     return stochastic_k, stochastic_d\n",
    "# stock['stochastic_k'], stock['stochastic_d'] = calculate_stochastic(stock, 14, 3)\n",
    "\n",
    "# stock['stochastic_k']= calculate_stochastic(stock, 14, 3)[0]\n",
    "# stock['stochastic_d']= calculate_stochastic(stock, 14, 3)[1]\n",
    "stock['day'] = pd.to_datetime(stock.index).day\n",
    "stock['month'] = pd.to_datetime(stock.index).month\n",
    "stock['year'] = pd.to_datetime(stock.index).year\n",
    "stock['weekday'] = pd.to_datetime(stock.index).weekday\n",
    "stock['dayofyear'] = pd.to_datetime(stock.index).dayofyear\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "stock.index = stock.index.date\n",
    "# Split the data into training and test sets\n",
    "\n",
    "# df = stock.copy()\n",
    "\n",
    "# # Calculate pairwise correlation\n",
    "# corr_matrix = df.corr()\n",
    "\n",
    "# # Identify highly correlated columns\n",
    "# redundant_cols = set()\n",
    "# for i in range(5,len(corr_matrix.columns)-1):\n",
    "#     for j in range(i+1, len(corr_matrix.columns)):\n",
    "#         if corr_matrix.iloc[i,j] > 0.8 and corr_matrix.columns[i] not in redundant_cols:\n",
    "#             redundant_cols.add(corr_matrix.columns[j])\n",
    "\n",
    "# # Remove one of the redundant columns\n",
    "# for col in redundant_cols:\n",
    "#     df = df.drop(col, axis=1)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df)\n",
    "\n",
    "# stock = df.copy()\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:int(0.9*train_data_index)].copy()\n",
    "val_data  = stock.iloc[int(0.9*train_data_index)-sequence_length:train_data_index].copy()\n",
    "test_data = stock.iloc[train_data_index-sequence_length:].copy()\n",
    "train_data = label_data(train_data)\n",
    "val_data = label_data(val_data)\n",
    "test_data = label_data(test_data)\n",
    "train_data.fillna(0,axis = 0, inplace=True)\n",
    "val_data.fillna(0,axis = 0, inplace=True)\n",
    "test_data.fillna(0,axis = 0, inplace=True)\n",
    "\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "#trian & test data\n",
    "X_val_data = val_data.iloc[:,:-1]\n",
    "y_val_data = val_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data), len(X_test_data.columns))\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_val_data_normalizer = normalizer.fit_transform(X_val_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # # Reshape X_train_data_normalizer\n",
    "X_train_reshaped = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_val_reshaped = X_val_data_normalizer.reshape(X_val_data_normalizer.shape[0], X_val_data_normalizer.shape[1], 1)\n",
    "X_test_reshaped = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_train_data_normalizer,\n",
    "    y_train_data,\n",
    "    length = 3,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_val_data_normalizer,\n",
    "    y_val_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_test_data_normalizer,\n",
    "    y_test_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert targets to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_data, num_classes=3)\n",
    "y_val_onehot = to_categorical(y_val_data, num_classes=3)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    LSTM(256, input_shape=(sequence_length, X_train_data_normalizer.shape[1]), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(16, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(8, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(4, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='softmax')  # 3 neurons for the 3 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='max', restore_best_weights=True)\n",
    "\n",
    "# Define model checkpoint to save the best model during training\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='max')\n",
    "\n",
    "# Train the model\n",
    "history = lstm_model.fit(train_dataset, epochs=100, batch_size=64, validation_data=val_dataset, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Load the best saved model\n",
    "best_model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = best_model.predict(test_dataset)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data[sequence_length:], test_predictions)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print(classification_report(y_test_data[sequence_length:], test_predictions))\n",
    "test_predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM CLASSIFICATION WITH ONLY TWO CLASSES WITH FEATURE ENGINEERING + INDICATORS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119 239\n",
      "Epoch 1/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 88ms/step - binary_accuracy: 0.5116 - f1_score: 0.3851 - loss: 0.6930 - recall: 0.4778 - val_binary_accuracy: 0.4558 - val_f1_score: 0.3131 - val_loss: 0.6976 - val_recall: 0.4558\n",
      "Epoch 2/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - binary_accuracy: 0.5059 - f1_score: 0.3359 - loss: 0.6931 - recall: 0.5059 - val_binary_accuracy: 0.4558 - val_f1_score: 0.3131 - val_loss: 0.6976 - val_recall: 0.4558\n",
      "Epoch 3/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - binary_accuracy: 0.5017 - f1_score: 0.3340 - loss: 0.6934 - recall: 0.5017 - val_binary_accuracy: 0.4558 - val_f1_score: 0.3131 - val_loss: 0.6982 - val_recall: 0.4558\n",
      "Epoch 4/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - binary_accuracy: 0.5145 - f1_score: 0.3397 - loss: 0.6927 - recall: 0.5145 - val_binary_accuracy: 0.4558 - val_f1_score: 0.3131 - val_loss: 0.6970 - val_recall: 0.4558\n",
      "Epoch 5/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - binary_accuracy: 0.5080 - f1_score: 0.3368 - loss: 0.6931 - recall: 0.5080 - val_binary_accuracy: 0.4609 - val_f1_score: 0.3131 - val_loss: 0.6942 - val_recall: 0.4447\n",
      "Epoch 6/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - binary_accuracy: 0.5141 - f1_score: 0.3395 - loss: 0.6928 - recall: 0.5141 - val_binary_accuracy: 0.4863 - val_f1_score: 0.3631 - val_loss: 0.6946 - val_recall: 0.1391\n",
      "Epoch 7/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - binary_accuracy: 0.5155 - f1_score: 0.3401 - loss: 0.6927 - recall: 0.5155 - val_binary_accuracy: 0.5000 - val_f1_score: 0.3524 - val_loss: 0.6935 - val_recall: 0.0000e+00\n",
      "Epoch 8/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - binary_accuracy: 0.5137 - f1_score: 0.3393 - loss: 0.6927 - recall: 0.5130 - val_binary_accuracy: 0.5000 - val_f1_score: 0.3131 - val_loss: 0.6944 - val_recall: 0.0000e+00\n",
      "Epoch 9/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - binary_accuracy: 0.5159 - f1_score: 0.3403 - loss: 0.6927 - recall: 0.5153 - val_binary_accuracy: 0.4893 - val_f1_score: 0.3131 - val_loss: 0.6958 - val_recall: 0.1147\n",
      "Epoch 10/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - binary_accuracy: 0.5132 - f1_score: 0.3430 - loss: 0.6926 - recall: 0.5062 - val_binary_accuracy: 0.5482 - val_f1_score: 0.4270 - val_loss: 0.6905 - val_recall: 0.5431\n",
      "Epoch 11/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - binary_accuracy: 0.5223 - f1_score: 0.4175 - loss: 0.6921 - recall: 0.5081 - val_binary_accuracy: 0.5223 - val_f1_score: 0.4646 - val_loss: 0.6946 - val_recall: 0.5218\n",
      "Epoch 12/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - binary_accuracy: 0.5238 - f1_score: 0.4459 - loss: 0.6919 - recall: 0.5243 - val_binary_accuracy: 0.5442 - val_f1_score: 0.3790 - val_loss: 0.6895 - val_recall: 0.5442\n",
      "Epoch 13/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - binary_accuracy: 0.5225 - f1_score: 0.4756 - loss: 0.6922 - recall: 0.5192 - val_binary_accuracy: 0.5442 - val_f1_score: 0.3524 - val_loss: 0.6899 - val_recall: 0.5442\n",
      "Epoch 14/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - binary_accuracy: 0.5240 - f1_score: 0.5205 - loss: 0.6924 - recall: 0.5192 - val_binary_accuracy: 0.5442 - val_f1_score: 0.3807 - val_loss: 0.6894 - val_recall: 0.5442\n",
      "Epoch 15/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - binary_accuracy: 0.5260 - f1_score: 0.4631 - loss: 0.6917 - recall: 0.5221 - val_binary_accuracy: 0.5574 - val_f1_score: 0.4290 - val_loss: 0.6868 - val_recall: 0.5574\n",
      "Epoch 16/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - binary_accuracy: 0.5267 - f1_score: 0.4901 - loss: 0.6920 - recall: 0.5241 - val_binary_accuracy: 0.5442 - val_f1_score: 0.3524 - val_loss: 0.6893 - val_recall: 0.5442\n",
      "Epoch 17/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - binary_accuracy: 0.5197 - f1_score: 0.4615 - loss: 0.6924 - recall: 0.5169 - val_binary_accuracy: 0.5442 - val_f1_score: 0.3524 - val_loss: 0.6896 - val_recall: 0.5442\n",
      "Epoch 18/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - binary_accuracy: 0.5275 - f1_score: 0.4916 - loss: 0.6921 - recall: 0.5232 - val_binary_accuracy: 0.5472 - val_f1_score: 0.3694 - val_loss: 0.6901 - val_recall: 0.5472\n",
      "Epoch 19/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - binary_accuracy: 0.5224 - f1_score: 0.4437 - loss: 0.6924 - recall: 0.5198 - val_binary_accuracy: 0.5000 - val_f1_score: 0.4862 - val_loss: 0.6999 - val_recall: 0.4985\n",
      "Epoch 20/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - binary_accuracy: 0.5169 - f1_score: 0.4661 - loss: 0.6924 - recall: 0.5131 - val_binary_accuracy: 0.5355 - val_f1_score: 0.3679 - val_loss: 0.6931 - val_recall: 0.5350\n",
      "Epoch 21/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - binary_accuracy: 0.5344 - f1_score: 0.4802 - loss: 0.6914 - recall: 0.5331 - val_binary_accuracy: 0.5096 - val_f1_score: 0.4735 - val_loss: 0.6990 - val_recall: 0.5086\n",
      "Epoch 22/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - binary_accuracy: 0.5069 - f1_score: 0.4810 - loss: 0.6931 - recall: 0.4969 - val_binary_accuracy: 0.5360 - val_f1_score: 0.3715 - val_loss: 0.6911 - val_recall: 0.5360\n",
      "Epoch 23/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - binary_accuracy: 0.5283 - f1_score: 0.4846 - loss: 0.6917 - recall: 0.5258 - val_binary_accuracy: 0.4944 - val_f1_score: 0.4791 - val_loss: 0.7030 - val_recall: 0.4934\n",
      "Epoch 24/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - binary_accuracy: 0.5282 - f1_score: 0.4881 - loss: 0.6916 - recall: 0.5264 - val_binary_accuracy: 0.5401 - val_f1_score: 0.3566 - val_loss: 0.6935 - val_recall: 0.5401\n",
      "Epoch 25/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - binary_accuracy: 0.5246 - f1_score: 0.5016 - loss: 0.6926 - recall: 0.5229 - val_binary_accuracy: 0.5249 - val_f1_score: 0.4003 - val_loss: 0.6960 - val_recall: 0.5249\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 302ms/step\n",
      "Accuracy: 0.5160697887970616\n",
      "[[0.4380347  0.5582665 ]\n",
      " [0.4380347  0.5582665 ]\n",
      " [0.4380347  0.5582665 ]\n",
      " ...\n",
      " [0.4380052  0.5583334 ]\n",
      " [0.4380052  0.5583333 ]\n",
      " [0.43800527 0.55833304]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       527\n",
      "           1       0.52      1.00      0.68       562\n",
      "\n",
      "    accuracy                           0.52      1089\n",
      "   macro avg       0.26      0.50      0.34      1089\n",
      "weighted avg       0.27      0.52      0.35      1089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-05-01'\n",
    "# Parameters\n",
    "batch_size = 256\n",
    "sequence_length = 30\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1980-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['pr_change_on_last_day'] = data['Close'].pct_change()\n",
    "    data['pr_change_on_current_day'] = data['pr_change_on_last_day'].shift(-1)\n",
    "    data.iloc[0,-2] = 0\n",
    "    data['sentiment'] = pd.Series(np.where(data['pr_change_on_current_day'] > 0, 1, 0), index=data.index)\n",
    "    # data['perc_change'] = data['Percentage Change']\n",
    "    # # Drop any rows with missing values\n",
    "    # data.dropna(inplace=True)\n",
    "    data.drop('pr_change_on_current_day',axis=1 , inplace=True)\n",
    "    return data\n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# Calculate deltas, moving averages, and Bollinger Bands\n",
    "for i in range(1, 90,5):\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "    stock[f\"rolling_mean_open_{i}day\"] = stock[\"Open\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_high_{i}day\"] = stock[\"High\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_low_{i}day\"] = stock[\"Low\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_close_{i}day\"] = stock[\"Close\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_std_open_{i}day\"] = stock[\"Open\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_high_{i}day\"] = stock[\"High\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_low_{i}day\"] = stock[\"Low\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_close_{i}day\"] = stock[\"Close\"].rolling(window=i).std()\n",
    "\n",
    "stock['fast_ma'] = stock['Close'].rolling(window=20).mean()\n",
    "stock['slow_ma'] = stock['Close'].rolling(window=50).mean()\n",
    "stock['bollinger_high'] = stock['Close'].rolling(window=20).mean() + (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['bollinger_low'] = stock['Close'].rolling(window=20).mean() - (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['ema'] = stock['Close'].ewm(span=20, adjust=False).mean()\n",
    "stock['envelope_high'] = stock['Close'].rolling(window=20).mean() * (1 + 0.05)\n",
    "stock['envelope_low'] = stock['Close'].rolling(window=20).mean() * (1 - 0.05)\n",
    "stock['macd_line'] = stock['Close'].ewm(span=12, adjust=False).mean() - stock['Close'].ewm(span=26, adjust=False).mean()\n",
    "stock['macd_signal'] = stock['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# RSI calculation\n",
    "def calculate_rsi(data, rsi_period):\n",
    "    delta = data['Close'].diff().dropna()\n",
    "    gain = delta.where(delta > 0, 0).dropna()\n",
    "    loss = -delta.where(delta < 0, 0).dropna()\n",
    "    avg_gain = gain.rolling(window=rsi_period).mean()\n",
    "    avg_loss = loss.rolling(window=rsi_period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "stock['rsi'] = calculate_rsi(stock, 14)\n",
    "\n",
    "# # Stochastic Oscillator calculation\n",
    "# def calculate_stochastic(data, k_window, d_window):\n",
    "#     high_low = data[['High', 'Low']]\n",
    "#     c = data['Close']\n",
    "#     highest = high_low.rolling(window=k_window).max()\n",
    "#     lowest = high_low.rolling(window=k_window).min()\n",
    "#     print(((c - lowest) / (highest - lowest)) * 100)\n",
    "#     stochastic_k = ((c - lowest) / (highest - lowest)) * 100\n",
    "#     stochastic_d = stochastic_k.rolling(window=d_window).mean()\n",
    "#     return stochastic_k, stochastic_d\n",
    "# stock['stochastic_k'], stock['stochastic_d'] = calculate_stochastic(stock, 14, 3)\n",
    "\n",
    "# stock['stochastic_k']= calculate_stochastic(stock, 14, 3)[0]\n",
    "# stock['stochastic_d']= calculate_stochastic(stock, 14, 3)[1]\n",
    "stock['day'] = pd.to_datetime(stock.index).day\n",
    "stock['month'] = pd.to_datetime(stock.index).month\n",
    "stock['year'] = pd.to_datetime(stock.index).year\n",
    "stock['weekday'] = pd.to_datetime(stock.index).weekday\n",
    "stock['dayofyear'] = pd.to_datetime(stock.index).dayofyear\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "stock.index = stock.index.date\n",
    "# Split the data into training and test sets\n",
    "\n",
    "# df = stock.copy()\n",
    "\n",
    "# # Calculate pairwise correlation\n",
    "# corr_matrix = df.corr()\n",
    "\n",
    "# # Identify highly correlated columns\n",
    "# redundant_cols = set()\n",
    "# for i in range(5,len(corr_matrix.columns)-1):\n",
    "#     for j in range(i+1, len(corr_matrix.columns)):\n",
    "#         if corr_matrix.iloc[i,j] > 0.8 and corr_matrix.columns[i] not in redundant_cols:\n",
    "#             redundant_cols.add(corr_matrix.columns[j])\n",
    "\n",
    "# # Remove one of the redundant columns\n",
    "# for col in redundant_cols:\n",
    "#     df = df.drop(col, axis=1)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df)\n",
    "\n",
    "# stock = df.copy()\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:int(0.9*train_data_index)].copy()\n",
    "val_data  = stock.iloc[int(0.9*train_data_index)-sequence_length:train_data_index].copy()\n",
    "test_data = stock.iloc[train_data_index-sequence_length:].copy()\n",
    "train_data = label_data(train_data)\n",
    "val_data = label_data(val_data)\n",
    "test_data = label_data(test_data)\n",
    "train_data.fillna(0,axis = 0, inplace=True)\n",
    "val_data.fillna(0,axis = 0, inplace=True)\n",
    "test_data.fillna(0,axis = 0, inplace=True)\n",
    "from keras.utils import to_categorical\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "#trian & test data\n",
    "X_val_data = val_data.iloc[:,:-1]\n",
    "y_val_data = val_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "\n",
    "y_train_data = to_categorical(y_train_data)\n",
    "y_test_data = to_categorical(y_test_data)\n",
    "y_val_data = to_categorical(y_val_data)\n",
    "\n",
    "print(len(X_test_data), len(X_test_data.columns))\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_val_data_normalizer = normalizer.fit_transform(X_val_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # # Reshape X_train_data_normalizer\n",
    "X_train_reshaped = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_val_reshaped = X_val_data_normalizer.reshape(X_val_data_normalizer.shape[0], X_val_data_normalizer.shape[1], 1)\n",
    "X_test_reshaped = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_train_data_normalizer,\n",
    "    y_train_data,\n",
    "    length = 3,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_val_data_normalizer,\n",
    "    y_val_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_test_data_normalizer,\n",
    "    y_test_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert targets to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_data, num_classes=3)\n",
    "y_val_onehot = to_categorical(y_val_data, num_classes=3)\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    LSTM(256, input_shape=(sequence_length, X_train_data_normalizer.shape[1]), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(16, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(8, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(4, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(2, activation='sigmoid')  # 3 neurons for the 3 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy', 'recall','f1_score'])\n",
    "\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='min', restore_best_weights=True)\n",
    "\n",
    "# Define model checkpoint to save the best model during training\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Train the model\n",
    "history = lstm_model.fit(train_dataset, epochs=100, batch_size=64, validation_data=val_dataset, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Load the best saved model\n",
    "best_model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = best_model.predict(test_dataset)\n",
    "test_predictions_binary = np.argmax(test_predictions, axis=1)\n",
    "y_test_data_binary = np.argmax(y_test_data, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data_binary[sequence_length:], test_predictions_binary)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "print(test_predictions )\n",
    "print(classification_report(y_test_data_binary[sequence_length:], test_predictions_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gru MODEL with classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119 239\n",
      "Epoch 1/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.5123 - f1_score: 0.4063 - loss: 0.6930 - precision: 0.5120 - recall: 0.6203 - val_accuracy: 0.5442 - val_f1_score: 0.3524 - val_loss: 0.6920 - val_precision: 0.5000 - val_recall: 1.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.5135 - f1_score: 0.4655 - loss: 0.6932 - precision: 0.5061 - recall: 0.5601 - val_accuracy: 0.5208 - val_f1_score: 0.4750 - val_loss: 0.6929 - val_precision: 0.5202 - val_recall: 0.5360\n",
      "Epoch 3/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5090 - f1_score: 0.4625 - loss: 0.6935 - precision: 0.5090 - recall: 0.5111 - val_accuracy: 0.5401 - val_f1_score: 0.3872 - val_loss: 0.6919 - val_precision: 0.5028 - val_recall: 0.9025\n",
      "Epoch 4/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5147 - f1_score: 0.4572 - loss: 0.6926 - precision: 0.5162 - recall: 0.5334 - val_accuracy: 0.5442 - val_f1_score: 0.3524 - val_loss: 0.6927 - val_precision: 0.5192 - val_recall: 0.6873\n",
      "Epoch 5/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5106 - f1_score: 0.4813 - loss: 0.6928 - precision: 0.5112 - recall: 0.5082 - val_accuracy: 0.5015 - val_f1_score: 0.4984 - val_loss: 0.6936 - val_precision: 0.5095 - val_recall: 0.4102\n",
      "Epoch 6/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.4949 - f1_score: 0.4401 - loss: 0.6939 - precision: 0.4916 - recall: 0.4681 - val_accuracy: 0.5056 - val_f1_score: 0.5048 - val_loss: 0.6934 - val_precision: 0.4945 - val_recall: 0.7360\n",
      "Epoch 7/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5176 - f1_score: 0.3920 - loss: 0.6928 - precision: 0.5138 - recall: 0.5247 - val_accuracy: 0.4680 - val_f1_score: 0.3926 - val_loss: 0.6940 - val_precision: 0.4735 - val_recall: 0.5624\n",
      "Epoch 8/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5164 - f1_score: 0.4206 - loss: 0.6929 - precision: 0.5145 - recall: 0.5098 - val_accuracy: 0.5279 - val_f1_score: 0.4632 - val_loss: 0.6924 - val_precision: 0.5271 - val_recall: 0.5330\n",
      "Epoch 9/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.5106 - f1_score: 0.4026 - loss: 0.6928 - precision: 0.5123 - recall: 0.5106 - val_accuracy: 0.5442 - val_f1_score: 0.3524 - val_loss: 0.6917 - val_precision: 0.5439 - val_recall: 0.5472\n",
      "Epoch 10/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5144 - f1_score: 0.4079 - loss: 0.6928 - precision: 0.5129 - recall: 0.5109 - val_accuracy: 0.5381 - val_f1_score: 0.4390 - val_loss: 0.6913 - val_precision: 0.5372 - val_recall: 0.4761\n",
      "Epoch 11/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.5308 - f1_score: 0.4761 - loss: 0.6915 - precision: 0.5306 - recall: 0.5287 - val_accuracy: 0.5442 - val_f1_score: 0.3524 - val_loss: 0.6899 - val_precision: 0.5442 - val_recall: 0.5442\n",
      "Epoch 12/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5107 - f1_score: 0.4937 - loss: 0.6929 - precision: 0.5128 - recall: 0.5099 - val_accuracy: 0.5350 - val_f1_score: 0.4427 - val_loss: 0.6921 - val_precision: 0.5352 - val_recall: 0.5249\n",
      "Epoch 13/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.5026 - f1_score: 0.3949 - loss: 0.6936 - precision: 0.5030 - recall: 0.5093 - val_accuracy: 0.5431 - val_f1_score: 0.3731 - val_loss: 0.6909 - val_precision: 0.5423 - val_recall: 0.5401\n",
      "Epoch 14/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5178 - f1_score: 0.4701 - loss: 0.6926 - precision: 0.5169 - recall: 0.5292 - val_accuracy: 0.5421 - val_f1_score: 0.3762 - val_loss: 0.6905 - val_precision: 0.5427 - val_recall: 0.5421\n",
      "Epoch 15/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.5187 - f1_score: 0.4689 - loss: 0.6928 - precision: 0.5150 - recall: 0.5138 - val_accuracy: 0.5391 - val_f1_score: 0.4442 - val_loss: 0.6907 - val_precision: 0.5397 - val_recall: 0.5381\n",
      "Epoch 16/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.5078 - f1_score: 0.4471 - loss: 0.6932 - precision: 0.5089 - recall: 0.5120 - val_accuracy: 0.5411 - val_f1_score: 0.3861 - val_loss: 0.6901 - val_precision: 0.5412 - val_recall: 0.5401\n",
      "Epoch 17/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.5244 - f1_score: 0.4697 - loss: 0.6926 - precision: 0.5235 - recall: 0.5214 - val_accuracy: 0.5401 - val_f1_score: 0.3527 - val_loss: 0.6897 - val_precision: 0.5401 - val_recall: 0.5401\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 328ms/step\n",
      "Accuracy: 0.4839302112029385\n",
      "[[0.51088023 0.4987764 ]\n",
      " [0.5108924  0.49876633]\n",
      " [0.5109002  0.49875998]\n",
      " ...\n",
      " [0.5112998  0.49814045]\n",
      " [0.5113011  0.49814165]\n",
      " [0.51129735 0.49815935]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       527\n",
      "           1       0.00      0.00      0.00       562\n",
      "\n",
      "    accuracy                           0.48      1089\n",
      "   macro avg       0.24      0.50      0.33      1089\n",
      "weighted avg       0.23      0.48      0.32      1089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-05-01'\n",
    "# Parameters\n",
    "batch_size = 256\n",
    "sequence_length = 30\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1980-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['pr_change_on_last_day'] = data['Close'].pct_change()\n",
    "    data['pr_change_on_current_day'] = data['pr_change_on_last_day'].shift(-1)\n",
    "    data.iloc[0,-2] = 0\n",
    "    data['sentiment'] = pd.Series(np.where(data['pr_change_on_current_day'] > 0, 1, 0), index=data.index)\n",
    "    # data['perc_change'] = data['Percentage Change']\n",
    "    # # Drop any rows with missing values\n",
    "    # data.dropna(inplace=True)\n",
    "    data.drop('pr_change_on_current_day',axis=1 , inplace=True)\n",
    "    return data\n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# Calculate deltas, moving averages, and Bollinger Bands\n",
    "for i in range(1, 90,5):\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "    stock[f\"rolling_mean_open_{i}day\"] = stock[\"Open\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_high_{i}day\"] = stock[\"High\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_low_{i}day\"] = stock[\"Low\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_close_{i}day\"] = stock[\"Close\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_std_open_{i}day\"] = stock[\"Open\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_high_{i}day\"] = stock[\"High\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_low_{i}day\"] = stock[\"Low\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_close_{i}day\"] = stock[\"Close\"].rolling(window=i).std()\n",
    "\n",
    "stock['fast_ma'] = stock['Close'].rolling(window=20).mean()\n",
    "stock['slow_ma'] = stock['Close'].rolling(window=50).mean()\n",
    "stock['bollinger_high'] = stock['Close'].rolling(window=20).mean() + (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['bollinger_low'] = stock['Close'].rolling(window=20).mean() - (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['ema'] = stock['Close'].ewm(span=20, adjust=False).mean()\n",
    "stock['envelope_high'] = stock['Close'].rolling(window=20).mean() * (1 + 0.05)\n",
    "stock['envelope_low'] = stock['Close'].rolling(window=20).mean() * (1 - 0.05)\n",
    "stock['macd_line'] = stock['Close'].ewm(span=12, adjust=False).mean() - stock['Close'].ewm(span=26, adjust=False).mean()\n",
    "stock['macd_signal'] = stock['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# RSI calculation\n",
    "def calculate_rsi(data, rsi_period):\n",
    "    delta = data['Close'].diff().dropna()\n",
    "    gain = delta.where(delta > 0, 0).dropna()\n",
    "    loss = -delta.where(delta < 0, 0).dropna()\n",
    "    avg_gain = gain.rolling(window=rsi_period).mean()\n",
    "    avg_loss = loss.rolling(window=rsi_period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "stock['rsi'] = calculate_rsi(stock, 14)\n",
    "\n",
    "# # Stochastic Oscillator calculation\n",
    "# def calculate_stochastic(data, k_window, d_window):\n",
    "#     high_low = data[['High', 'Low']]\n",
    "#     c = data['Close']\n",
    "#     highest = high_low.rolling(window=k_window).max()\n",
    "#     lowest = high_low.rolling(window=k_window).min()\n",
    "#     print(((c - lowest) / (highest - lowest)) * 100)\n",
    "#     stochastic_k = ((c - lowest) / (highest - lowest)) * 100\n",
    "#     stochastic_d = stochastic_k.rolling(window=d_window).mean()\n",
    "#     return stochastic_k, stochastic_d\n",
    "# stock['stochastic_k'], stock['stochastic_d'] = calculate_stochastic(stock, 14, 3)\n",
    "\n",
    "# stock['stochastic_k']= calculate_stochastic(stock, 14, 3)[0]\n",
    "# stock['stochastic_d']= calculate_stochastic(stock, 14, 3)[1]\n",
    "stock['day'] = pd.to_datetime(stock.index).day\n",
    "stock['month'] = pd.to_datetime(stock.index).month\n",
    "stock['year'] = pd.to_datetime(stock.index).year\n",
    "stock['weekday'] = pd.to_datetime(stock.index).weekday\n",
    "stock['dayofyear'] = pd.to_datetime(stock.index).dayofyear\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "stock.index = stock.index.date\n",
    "# Split the data into training and test sets\n",
    "\n",
    "# df = stock.copy()\n",
    "\n",
    "# # Calculate pairwise correlation\n",
    "# corr_matrix = df.corr()\n",
    "\n",
    "# # Identify highly correlated columns\n",
    "# redundant_cols = set()\n",
    "# for i in range(5,len(corr_matrix.columns)-1):\n",
    "#     for j in range(i+1, len(corr_matrix.columns)):\n",
    "#         if corr_matrix.iloc[i,j] > 0.8 and corr_matrix.columns[i] not in redundant_cols:\n",
    "#             redundant_cols.add(corr_matrix.columns[j])\n",
    "\n",
    "# # Remove one of the redundant columns\n",
    "# for col in redundant_cols:\n",
    "#     df = df.drop(col, axis=1)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df)\n",
    "\n",
    "# stock = df.copy()\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:int(0.9*train_data_index)].copy()\n",
    "val_data  = stock.iloc[int(0.9*train_data_index)-sequence_length:train_data_index].copy()\n",
    "test_data = stock.iloc[train_data_index-sequence_length:].copy()\n",
    "train_data = label_data(train_data)\n",
    "val_data = label_data(val_data)\n",
    "test_data = label_data(test_data)\n",
    "train_data.fillna(0,axis = 0, inplace=True)\n",
    "val_data.fillna(0,axis = 0, inplace=True)\n",
    "test_data.fillna(0,axis = 0, inplace=True)\n",
    "\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "#trian & test data\n",
    "X_val_data = val_data.iloc[:,:-1]\n",
    "y_val_data = val_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data), len(X_test_data.columns))\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert targets to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_data, num_classes=3)\n",
    "y_val_onehot = to_categorical(y_val_data, num_classes=3)\n",
    "\n",
    "y_train_data = to_categorical(y_train_data)\n",
    "y_test_data = to_categorical(y_test_data)\n",
    "y_val_data = to_categorical(y_val_data)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_val_data_normalizer = normalizer.fit_transform(X_val_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # # Reshape X_train_data_normalizer\n",
    "X_train_reshaped = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_val_reshaped = X_val_data_normalizer.reshape(X_val_data_normalizer.shape[0], X_val_data_normalizer.shape[1], 1)\n",
    "X_test_reshaped = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_train_data_normalizer,\n",
    "    y_train_data,\n",
    "    length = 3,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_val_data_normalizer,\n",
    "    y_val_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_test_data_normalizer,\n",
    "    y_test_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define the GRU model\n",
    "gru_model = tf.keras.Sequential([\n",
    "tf.keras.layers.GRU(256, input_shape=(sequence_length, X_train_data_normalizer.shape[1]), return_sequences=True),\n",
    "tf.keras.layers.Dropout(0.3),\n",
    "tf.keras.layers.GRU(128, return_sequences=True),\n",
    "tf.keras.layers.Dropout(0.3),\n",
    "tf.keras.layers.GRU(64, return_sequences=True),\n",
    "tf.keras.layers.Dropout(0.3),\n",
    "tf.keras.layers.GRU(32, return_sequences=True),\n",
    "tf.keras.layers.Dropout(0.3),\n",
    "tf.keras.layers.GRU(16, return_sequences=True),\n",
    "tf.keras.layers.Dropout(0.3),\n",
    "tf.keras.layers.GRU(8, return_sequences=True),\n",
    "tf.keras.layers.Dropout(0.3),\n",
    "tf.keras.layers.GRU(4, return_sequences=False),\n",
    "tf.keras.layers.Dropout(0.3),\n",
    "tf.keras.layers.Flatten(),\n",
    "tf.keras.layers.Dense(64, activation='relu'),\n",
    "tf.keras.layers.Dropout(0.3),\n",
    "tf.keras.layers.Dense(2, activation='sigmoid') # 3 neurons for the 3 classes\n",
    "])\n",
    "# Compile the model\n",
    "gru_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'recall','f1_score','precision'],)\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='max', restore_best_weights=True)\n",
    "\n",
    "# Define model checkpoint to save the best model during training\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='max')\n",
    "\n",
    "# Train the model\n",
    "history = gru_model.fit(train_dataset, epochs=100, batch_size=64, validation_data=val_dataset, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Load the best saved model\n",
    "best_model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = best_model.predict(test_dataset)\n",
    "test_predictions_binary = np.argmax(test_predictions, axis=1)\n",
    "y_test_data_binary = np.argmax(y_test_data, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data_binary[sequence_length:], test_predictions_binary)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "print(test_predictions )\n",
    "print(classification_report(y_test_data_binary[sequence_length:], test_predictions_binary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression classifier + all features + technical indicators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119 239\n",
      "Accuracy: 0.483467381590706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.48      0.79      0.59       535\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.51      0.20      0.29       580\n",
      "\n",
      "    accuracy                           0.48      1119\n",
      "   macro avg       0.33      0.33      0.29      1119\n",
      "weighted avg       0.49      0.48      0.43      1119\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1,  1, -1, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-05-01'\n",
    "# Parameters\n",
    "batch_size = 256\n",
    "sequence_length = 30\n",
    "stride = 1\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1980-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['pr_change_on_last_day'] = data['Close'].pct_change()\n",
    "    data['pr_change_on_current_day'] = data['pr_change_on_last_day'].shift(-1)\n",
    "    data.iloc[0,-2] = 0\n",
    "    data['sentiment'] = pd.Series(np.where(data['pr_change_on_current_day'] > 0, 1, np.where(data['pr_change_on_current_day'] < 0, -1, 0)), index=data.index)\n",
    "    # data['perc_change'] = data['Percentage Change']\n",
    "    # # Drop any rows with missing values\n",
    "    # data.dropna(inplace=True)\n",
    "    data.drop('pr_change_on_current_day',axis=1 , inplace=True)\n",
    "    return data\n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# Calculate deltas, moving averages, and Bollinger Bands\n",
    "for i in range(1, 90,5):\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "    stock[f\"rolling_mean_open_{i}day\"] = stock[\"Open\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_high_{i}day\"] = stock[\"High\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_low_{i}day\"] = stock[\"Low\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_close_{i}day\"] = stock[\"Close\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_std_open_{i}day\"] = stock[\"Open\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_high_{i}day\"] = stock[\"High\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_low_{i}day\"] = stock[\"Low\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_close_{i}day\"] = stock[\"Close\"].rolling(window=i).std()\n",
    "\n",
    "stock['fast_ma'] = stock['Close'].rolling(window=20).mean()\n",
    "stock['slow_ma'] = stock['Close'].rolling(window=50).mean()\n",
    "stock['bollinger_high'] = stock['Close'].rolling(window=20).mean() + (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['bollinger_low'] = stock['Close'].rolling(window=20).mean() - (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['ema'] = stock['Close'].ewm(span=20, adjust=False).mean()\n",
    "stock['envelope_high'] = stock['Close'].rolling(window=20).mean() * (1 + 0.05)\n",
    "stock['envelope_low'] = stock['Close'].rolling(window=20).mean() * (1 - 0.05)\n",
    "stock['macd_line'] = stock['Close'].ewm(span=12, adjust=False).mean() - stock['Close'].ewm(span=26, adjust=False).mean()\n",
    "stock['macd_signal'] = stock['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# RSI calculation\n",
    "def calculate_rsi(data, rsi_period):\n",
    "    delta = data['Close'].diff().dropna()\n",
    "    gain = delta.where(delta > 0, 0).dropna()\n",
    "    loss = -delta.where(delta < 0, 0).dropna()\n",
    "    avg_gain = gain.rolling(window=rsi_period).mean()\n",
    "    avg_loss = loss.rolling(window=rsi_period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "stock['rsi'] = calculate_rsi(stock, 14)\n",
    "\n",
    "# # Stochastic Oscillator calculation\n",
    "# def calculate_stochastic(data, k_window, d_window):\n",
    "#     high_low = data[['High', 'Low']]\n",
    "#     c = data['Close']\n",
    "#     highest = high_low.rolling(window=k_window).max()\n",
    "#     lowest = high_low.rolling(window=k_window).min()\n",
    "#     print(((c - lowest) / (highest - lowest)) * 100)\n",
    "#     stochastic_k = ((c - lowest) / (highest - lowest)) * 100\n",
    "#     stochastic_d = stochastic_k.rolling(window=d_window).mean()\n",
    "#     return stochastic_k, stochastic_d\n",
    "# stock['stochastic_k'], stock['stochastic_d'] = calculate_stochastic(stock, 14, 3)\n",
    "\n",
    "# stock['stochastic_k']= calculate_stochastic(stock, 14, 3)[0]\n",
    "# stock['stochastic_d']= calculate_stochastic(stock, 14, 3)[1]\n",
    "stock['day'] = pd.to_datetime(stock.index).day\n",
    "stock['month'] = pd.to_datetime(stock.index).month\n",
    "stock['year'] = pd.to_datetime(stock.index).year\n",
    "stock['weekday'] = pd.to_datetime(stock.index).weekday\n",
    "stock['dayofyear'] = pd.to_datetime(stock.index).dayofyear\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "stock.index = stock.index.date\n",
    "# Split the data into training and test sets\n",
    "\n",
    "# df = stock.copy()\n",
    "\n",
    "# # Calculate pairwise correlation\n",
    "# corr_matrix = df.corr()\n",
    "\n",
    "# # Identify highly correlated columns\n",
    "# redundant_cols = set()\n",
    "# for i in range(5,len(corr_matrix.columns)-1):\n",
    "#     for j in range(i+1, len(corr_matrix.columns)):\n",
    "#         if corr_matrix.iloc[i,j] > 0.8 and corr_matrix.columns[i] not in redundant_cols:\n",
    "#             redundant_cols.add(corr_matrix.columns[j])\n",
    "\n",
    "# # Remove one of the redundant columns\n",
    "# for col in redundant_cols:\n",
    "#     df = df.drop(col, axis=1)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df)\n",
    "\n",
    "# stock = df.copy()\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:int(0.9*train_data_index)].copy()\n",
    "val_data  = stock.iloc[int(0.9*train_data_index)-sequence_length:train_data_index].copy()\n",
    "test_data = stock.iloc[train_data_index-sequence_length:].copy()\n",
    "train_data = label_data(train_data)\n",
    "val_data = label_data(val_data)\n",
    "test_data = label_data(test_data)\n",
    "train_data.fillna(0,axis = 0, inplace=True)\n",
    "val_data.fillna(0,axis = 0, inplace=True)\n",
    "test_data.fillna(0,axis = 0, inplace=True)\n",
    "\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "#trian & test data\n",
    "X_val_data = val_data.iloc[:,:-1]\n",
    "y_val_data = val_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data), len(X_test_data.columns))\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert targets to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_data, num_classes=3)\n",
    "y_val_onehot = to_categorical(y_val_data, num_classes=3)\n",
    "\n",
    "y_train_data_onehot = to_categorical(y_train_data)\n",
    "y_test_data_onehot = to_categorical(y_test_data)\n",
    "y_val_data_onehot = to_categorical(y_val_data)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_val_data_normalizer = normalizer.fit_transform(X_val_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # # Reshape X_train_data_normalizer\n",
    "X_train_reshaped = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_val_reshaped = X_val_data_normalizer.reshape(X_val_data_normalizer.shape[0], X_val_data_normalizer.shape[1], 1)\n",
    "X_test_reshaped = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "\n",
    "def create_sequences(x,y,sequence_length,stride):\n",
    "    sequence_length  = sequence_length\n",
    "    X_test_data_normalizer_sequences = []\n",
    "    y_test_data_sequences = []\n",
    "    stride = stride\n",
    "    no_of_rows = len(x)\n",
    "    no_of_columns = len(x[0])\n",
    "    for i in range(sequence_length, no_of_rows-1 , stride):\n",
    "        X_test_data_normalizer_sequences.append(x[i-sequence_length: i])\n",
    "        y_test_data_sequences.append(y[i-1])\n",
    "    return np.array(X_test_data_normalizer_sequences),np.array(y_test_data_sequences)\n",
    "        \n",
    "X_train_data_normalizer_sequences,y_train_data_sequences = create_sequences(X_train_data_normalizer,y_train_data,sequence_length,stride)\n",
    "X_test_data_normalizer_sequences,y_test_data_sequences = create_sequences(X_test_data_normalizer,y_test_data,sequence_length,stride)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_train_data_normalizer,\n",
    "    y_train_data,\n",
    "    length = 3,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_val_data_normalizer,\n",
    "    y_val_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_test_data_normalizer,\n",
    "    y_test_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "logistic_model.fit(X_train_data_normalizer, y_train_data)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = logistic_model.predict(X_test_data_normalizer)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test_data, y_pred))\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classification + Technical Indicators +  Features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119 239\n",
      "Accuracy: 0.4959785522788204\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.48      0.54      0.51       535\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.52      0.46      0.49       580\n",
      "\n",
      "    accuracy                           0.50      1119\n",
      "   macro avg       0.33      0.33      0.33      1119\n",
      "weighted avg       0.50      0.50      0.49      1119\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ...,  1, -1,  1])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-05-01'\n",
    "# Parameters\n",
    "batch_size = 256\n",
    "sequence_length = 30\n",
    "stride = 1\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1980-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['pr_change_on_last_day'] = data['Close'].pct_change()\n",
    "    data['pr_change_on_current_day'] = data['pr_change_on_last_day'].shift(-1)\n",
    "    data.iloc[0,-2] = 0\n",
    "    data['sentiment'] = pd.Series(np.where(data['pr_change_on_current_day'] > 0, 1, np.where(data['pr_change_on_current_day'] < 0, -1, 0)), index=data.index)\n",
    "    # data['perc_change'] = data['Percentage Change']\n",
    "    # # Drop any rows with missing values\n",
    "    # data.dropna(inplace=True)\n",
    "    data.drop('pr_change_on_current_day',axis=1 , inplace=True)\n",
    "    return data\n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# Calculate deltas, moving averages, and Bollinger Bands\n",
    "for i in range(1, 90,5):\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "    stock[f\"rolling_mean_open_{i}day\"] = stock[\"Open\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_high_{i}day\"] = stock[\"High\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_low_{i}day\"] = stock[\"Low\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_close_{i}day\"] = stock[\"Close\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_std_open_{i}day\"] = stock[\"Open\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_high_{i}day\"] = stock[\"High\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_low_{i}day\"] = stock[\"Low\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_close_{i}day\"] = stock[\"Close\"].rolling(window=i).std()\n",
    "\n",
    "stock['fast_ma'] = stock['Close'].rolling(window=20).mean()\n",
    "stock['slow_ma'] = stock['Close'].rolling(window=50).mean()\n",
    "stock['bollinger_high'] = stock['Close'].rolling(window=20).mean() + (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['bollinger_low'] = stock['Close'].rolling(window=20).mean() - (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['ema'] = stock['Close'].ewm(span=20, adjust=False).mean()\n",
    "stock['envelope_high'] = stock['Close'].rolling(window=20).mean() * (1 + 0.05)\n",
    "stock['envelope_low'] = stock['Close'].rolling(window=20).mean() * (1 - 0.05)\n",
    "stock['macd_line'] = stock['Close'].ewm(span=12, adjust=False).mean() - stock['Close'].ewm(span=26, adjust=False).mean()\n",
    "stock['macd_signal'] = stock['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# RSI calculation\n",
    "def calculate_rsi(data, rsi_period):\n",
    "    delta = data['Close'].diff().dropna()\n",
    "    gain = delta.where(delta > 0, 0).dropna()\n",
    "    loss = -delta.where(delta < 0, 0).dropna()\n",
    "    avg_gain = gain.rolling(window=rsi_period).mean()\n",
    "    avg_loss = loss.rolling(window=rsi_period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "stock['rsi'] = calculate_rsi(stock, 14)\n",
    "\n",
    "# # Stochastic Oscillator calculation\n",
    "# def calculate_stochastic(data, k_window, d_window):\n",
    "#     high_low = data[['High', 'Low']]\n",
    "#     c = data['Close']\n",
    "#     highest = high_low.rolling(window=k_window).max()\n",
    "#     lowest = high_low.rolling(window=k_window).min()\n",
    "#     print(((c - lowest) / (highest - lowest)) * 100)\n",
    "#     stochastic_k = ((c - lowest) / (highest - lowest)) * 100\n",
    "#     stochastic_d = stochastic_k.rolling(window=d_window).mean()\n",
    "#     return stochastic_k, stochastic_d\n",
    "# stock['stochastic_k'], stock['stochastic_d'] = calculate_stochastic(stock, 14, 3)\n",
    "\n",
    "# stock['stochastic_k']= calculate_stochastic(stock, 14, 3)[0]\n",
    "# stock['stochastic_d']= calculate_stochastic(stock, 14, 3)[1]\n",
    "stock['day'] = pd.to_datetime(stock.index).day\n",
    "stock['month'] = pd.to_datetime(stock.index).month\n",
    "stock['year'] = pd.to_datetime(stock.index).year\n",
    "stock['weekday'] = pd.to_datetime(stock.index).weekday\n",
    "stock['dayofyear'] = pd.to_datetime(stock.index).dayofyear\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "stock.index = stock.index.date\n",
    "# Split the data into training and test sets\n",
    "\n",
    "# df = stock.copy()\n",
    "\n",
    "# # Calculate pairwise correlation\n",
    "# corr_matrix = df.corr()\n",
    "\n",
    "# # Identify highly correlated columns\n",
    "# redundant_cols = set()\n",
    "# for i in range(5,len(corr_matrix.columns)-1):\n",
    "#     for j in range(i+1, len(corr_matrix.columns)):\n",
    "#         if corr_matrix.iloc[i,j] > 0.8 and corr_matrix.columns[i] not in redundant_cols:\n",
    "#             redundant_cols.add(corr_matrix.columns[j])\n",
    "\n",
    "# # Remove one of the redundant columns\n",
    "# for col in redundant_cols:\n",
    "#     df = df.drop(col, axis=1)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df)\n",
    "\n",
    "# stock = df.copy()\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:int(0.9*train_data_index)].copy()\n",
    "val_data  = stock.iloc[int(0.9*train_data_index)-sequence_length:train_data_index].copy()\n",
    "test_data = stock.iloc[train_data_index-sequence_length:].copy()\n",
    "train_data = label_data(train_data)\n",
    "val_data = label_data(val_data)\n",
    "test_data = label_data(test_data)\n",
    "train_data.fillna(0,axis = 0, inplace=True)\n",
    "val_data.fillna(0,axis = 0, inplace=True)\n",
    "test_data.fillna(0,axis = 0, inplace=True)\n",
    "\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "#trian & test data\n",
    "X_val_data = val_data.iloc[:,:-1]\n",
    "y_val_data = val_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data), len(X_test_data.columns))\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert targets to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_data, num_classes=3)\n",
    "y_val_onehot = to_categorical(y_val_data, num_classes=3)\n",
    "\n",
    "y_train_data_onehot = to_categorical(y_train_data)\n",
    "y_test_data_onehot = to_categorical(y_test_data)\n",
    "y_val_data_onehot = to_categorical(y_val_data)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_val_data_normalizer = normalizer.fit_transform(X_val_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # # Reshape X_train_data_normalizer\n",
    "X_train_reshaped = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_val_reshaped = X_val_data_normalizer.reshape(X_val_data_normalizer.shape[0], X_val_data_normalizer.shape[1], 1)\n",
    "X_test_reshaped = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "\n",
    "def create_sequences(x,y,sequence_length,stride):\n",
    "    sequence_length  = sequence_length\n",
    "    X_test_data_normalizer_sequences = []\n",
    "    y_test_data_sequences = []\n",
    "    stride = stride\n",
    "    no_of_rows = len(x)\n",
    "    no_of_columns = len(x[0])\n",
    "    for i in range(sequence_length, no_of_rows-1 , stride):\n",
    "        X_test_data_normalizer_sequences.append(x[i-sequence_length: i])\n",
    "        y_test_data_sequences.append(y[i-1])\n",
    "    return np.array(X_test_data_normalizer_sequences),np.array(y_test_data_sequences)\n",
    "        \n",
    "X_train_data_normalizer_sequences,y_train_data_sequences = create_sequences(X_train_data_normalizer,y_train_data,sequence_length,stride)\n",
    "X_test_data_normalizer_sequences,y_test_data_sequences = create_sequences(X_test_data_normalizer,y_test_data,sequence_length,stride)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_train_data_normalizer,\n",
    "    y_train_data,\n",
    "    length = 3,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_val_data_normalizer,\n",
    "    y_val_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_test_data_normalizer,\n",
    "    y_test_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train_data_normalizer, y_train_data)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = rf_model.predict(X_test_data_normalizer)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test_data, y_pred))\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM CLASSIFIER +  TECHNICAL INDICATORS + FEATURES ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119 239\n",
      "Accuracy: 0.48614834673815904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.48      0.59      0.53       535\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.52      0.39      0.45       580\n",
      "\n",
      "    accuracy                           0.49      1119\n",
      "   macro avg       0.33      0.33      0.33      1119\n",
      "weighted avg       0.50      0.49      0.49      1119\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ...,  1,  1,  1])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-05-01'\n",
    "# Parameters\n",
    "batch_size = 256\n",
    "sequence_length = 30\n",
    "stride = 1\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1980-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['pr_change_on_last_day'] = data['Close'].pct_change()\n",
    "    data['pr_change_on_current_day'] = data['pr_change_on_last_day'].shift(-1)\n",
    "    data.iloc[0,-2] = 0\n",
    "    data['sentiment'] = pd.Series(np.where(data['pr_change_on_current_day'] > 0, 1, np.where(data['pr_change_on_current_day'] < 0, -1, 0)), index=data.index)\n",
    "    # data['perc_change'] = data['Percentage Change']\n",
    "    # # Drop any rows with missing values\n",
    "    # data.dropna(inplace=True)\n",
    "    data.drop('pr_change_on_current_day',axis=1 , inplace=True)\n",
    "    return data\n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# Calculate deltas, moving averages, and Bollinger Bands\n",
    "for i in range(1, 90,5):\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "    stock[f\"rolling_mean_open_{i}day\"] = stock[\"Open\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_high_{i}day\"] = stock[\"High\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_low_{i}day\"] = stock[\"Low\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_close_{i}day\"] = stock[\"Close\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_std_open_{i}day\"] = stock[\"Open\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_high_{i}day\"] = stock[\"High\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_low_{i}day\"] = stock[\"Low\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_close_{i}day\"] = stock[\"Close\"].rolling(window=i).std()\n",
    "\n",
    "stock['fast_ma'] = stock['Close'].rolling(window=20).mean()\n",
    "stock['slow_ma'] = stock['Close'].rolling(window=50).mean()\n",
    "stock['bollinger_high'] = stock['Close'].rolling(window=20).mean() + (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['bollinger_low'] = stock['Close'].rolling(window=20).mean() - (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['ema'] = stock['Close'].ewm(span=20, adjust=False).mean()\n",
    "stock['envelope_high'] = stock['Close'].rolling(window=20).mean() * (1 + 0.05)\n",
    "stock['envelope_low'] = stock['Close'].rolling(window=20).mean() * (1 - 0.05)\n",
    "stock['macd_line'] = stock['Close'].ewm(span=12, adjust=False).mean() - stock['Close'].ewm(span=26, adjust=False).mean()\n",
    "stock['macd_signal'] = stock['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# RSI calculation\n",
    "def calculate_rsi(data, rsi_period):\n",
    "    delta = data['Close'].diff().dropna()\n",
    "    gain = delta.where(delta > 0, 0).dropna()\n",
    "    loss = -delta.where(delta < 0, 0).dropna()\n",
    "    avg_gain = gain.rolling(window=rsi_period).mean()\n",
    "    avg_loss = loss.rolling(window=rsi_period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "stock['rsi'] = calculate_rsi(stock, 14)\n",
    "\n",
    "# # Stochastic Oscillator calculation\n",
    "# def calculate_stochastic(data, k_window, d_window):\n",
    "#     high_low = data[['High', 'Low']]\n",
    "#     c = data['Close']\n",
    "#     highest = high_low.rolling(window=k_window).max()\n",
    "#     lowest = high_low.rolling(window=k_window).min()\n",
    "#     print(((c - lowest) / (highest - lowest)) * 100)\n",
    "#     stochastic_k = ((c - lowest) / (highest - lowest)) * 100\n",
    "#     stochastic_d = stochastic_k.rolling(window=d_window).mean()\n",
    "#     return stochastic_k, stochastic_d\n",
    "# stock['stochastic_k'], stock['stochastic_d'] = calculate_stochastic(stock, 14, 3)\n",
    "\n",
    "# stock['stochastic_k']= calculate_stochastic(stock, 14, 3)[0]\n",
    "# stock['stochastic_d']= calculate_stochastic(stock, 14, 3)[1]\n",
    "stock['day'] = pd.to_datetime(stock.index).day\n",
    "stock['month'] = pd.to_datetime(stock.index).month\n",
    "stock['year'] = pd.to_datetime(stock.index).year\n",
    "stock['weekday'] = pd.to_datetime(stock.index).weekday\n",
    "stock['dayofyear'] = pd.to_datetime(stock.index).dayofyear\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "stock.index = stock.index.date\n",
    "# Split the data into training and test sets\n",
    "\n",
    "# df = stock.copy()\n",
    "\n",
    "# # Calculate pairwise correlation\n",
    "# corr_matrix = df.corr()\n",
    "\n",
    "# # Identify highly correlated columns\n",
    "# redundant_cols = set()\n",
    "# for i in range(5,len(corr_matrix.columns)-1):\n",
    "#     for j in range(i+1, len(corr_matrix.columns)):\n",
    "#         if corr_matrix.iloc[i,j] > 0.8 and corr_matrix.columns[i] not in redundant_cols:\n",
    "#             redundant_cols.add(corr_matrix.columns[j])\n",
    "\n",
    "# # Remove one of the redundant columns\n",
    "# for col in redundant_cols:\n",
    "#     df = df.drop(col, axis=1)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df)\n",
    "\n",
    "# stock = df.copy()\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:int(0.9*train_data_index)].copy()\n",
    "val_data  = stock.iloc[int(0.9*train_data_index)-sequence_length:train_data_index].copy()\n",
    "test_data = stock.iloc[train_data_index-sequence_length:].copy()\n",
    "train_data = label_data(train_data)\n",
    "val_data = label_data(val_data)\n",
    "test_data = label_data(test_data)\n",
    "train_data.fillna(0,axis = 0, inplace=True)\n",
    "val_data.fillna(0,axis = 0, inplace=True)\n",
    "test_data.fillna(0,axis = 0, inplace=True)\n",
    "\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "#trian & test data\n",
    "X_val_data = val_data.iloc[:,:-1]\n",
    "y_val_data = val_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data), len(X_test_data.columns))\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert targets to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_data, num_classes=3)\n",
    "y_val_onehot = to_categorical(y_val_data, num_classes=3)\n",
    "\n",
    "y_train_data_onehot = to_categorical(y_train_data)\n",
    "y_test_data_onehot = to_categorical(y_test_data)\n",
    "y_val_data_onehot = to_categorical(y_val_data)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_val_data_normalizer = normalizer.fit_transform(X_val_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # # Reshape X_train_data_normalizer\n",
    "X_train_reshaped = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_val_reshaped = X_val_data_normalizer.reshape(X_val_data_normalizer.shape[0], X_val_data_normalizer.shape[1], 1)\n",
    "X_test_reshaped = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "\n",
    "def create_sequences(x,y,sequence_length,stride):\n",
    "    sequence_length  = sequence_length\n",
    "    X_test_data_normalizer_sequences = []\n",
    "    y_test_data_sequences = []\n",
    "    stride = stride\n",
    "    no_of_rows = len(x)\n",
    "    no_of_columns = len(x[0])\n",
    "    for i in range(sequence_length, no_of_rows-1 , stride):\n",
    "        X_test_data_normalizer_sequences.append(x[i-sequence_length: i])\n",
    "        y_test_data_sequences.append(y[i-1])\n",
    "    return np.array(X_test_data_normalizer_sequences),np.array(y_test_data_sequences)\n",
    "        \n",
    "X_train_data_normalizer_sequences,y_train_data_sequences = create_sequences(X_train_data_normalizer,y_train_data,sequence_length,stride)\n",
    "X_test_data_normalizer_sequences,y_test_data_sequences = create_sequences(X_test_data_normalizer,y_test_data,sequence_length,stride)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_train_data_normalizer,\n",
    "    y_train_data,\n",
    "    length = 3,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_val_data_normalizer,\n",
    "    y_val_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_test_data_normalizer,\n",
    "    y_test_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_model = SVC(kernel='sigmoid', random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "svm_model.fit(X_train_data_normalizer, y_train_data)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = svm_model.predict(X_test_data_normalizer)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test_data, y_pred))\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119 239\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020792 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 58479\n",
      "[LightGBM] [Info] Number of data points in the train set: 8862, number of used features: 234\n",
      "[LightGBM] [Info] Start training from score -0.756017\n",
      "[LightGBM] [Info] Start training from score -3.186894\n",
      "[LightGBM] [Info] Start training from score -0.715051\n",
      "Accuracy: 0.5040214477211796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.49      0.79      0.60       535\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.56      0.24      0.34       580\n",
      "\n",
      "    accuracy                           0.50      1119\n",
      "   macro avg       0.35      0.34      0.31      1119\n",
      "weighted avg       0.52      0.50      0.46      1119\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1, -1,  1, ..., -1, -1,  1])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-05-01'\n",
    "# Parameters\n",
    "batch_size = 256\n",
    "sequence_length = 30\n",
    "stride = 1\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1980-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['pr_change_on_last_day'] = data['Close'].pct_change()\n",
    "    data['pr_change_on_current_day'] = data['pr_change_on_last_day'].shift(-1)\n",
    "    data.iloc[0,-2] = 0\n",
    "    data['sentiment'] = pd.Series(np.where(data['pr_change_on_current_day'] > 0, 1, np.where(data['pr_change_on_current_day'] < 0, -1, 0)), index=data.index)\n",
    "    # data['perc_change'] = data['Percentage Change']\n",
    "    # # Drop any rows with missing values\n",
    "    # data.dropna(inplace=True)\n",
    "    data.drop('pr_change_on_current_day',axis=1 , inplace=True)\n",
    "    return data\n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# Calculate deltas, moving averages, and Bollinger Bands\n",
    "for i in range(1, 90,5):\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "    stock[f\"rolling_mean_open_{i}day\"] = stock[\"Open\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_high_{i}day\"] = stock[\"High\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_low_{i}day\"] = stock[\"Low\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_close_{i}day\"] = stock[\"Close\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_std_open_{i}day\"] = stock[\"Open\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_high_{i}day\"] = stock[\"High\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_low_{i}day\"] = stock[\"Low\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_close_{i}day\"] = stock[\"Close\"].rolling(window=i).std()\n",
    "\n",
    "stock['fast_ma'] = stock['Close'].rolling(window=20).mean()\n",
    "stock['slow_ma'] = stock['Close'].rolling(window=50).mean()\n",
    "stock['bollinger_high'] = stock['Close'].rolling(window=20).mean() + (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['bollinger_low'] = stock['Close'].rolling(window=20).mean() - (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['ema'] = stock['Close'].ewm(span=20, adjust=False).mean()\n",
    "stock['envelope_high'] = stock['Close'].rolling(window=20).mean() * (1 + 0.05)\n",
    "stock['envelope_low'] = stock['Close'].rolling(window=20).mean() * (1 - 0.05)\n",
    "stock['macd_line'] = stock['Close'].ewm(span=12, adjust=False).mean() - stock['Close'].ewm(span=26, adjust=False).mean()\n",
    "stock['macd_signal'] = stock['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# RSI calculation\n",
    "def calculate_rsi(data, rsi_period):\n",
    "    delta = data['Close'].diff().dropna()\n",
    "    gain = delta.where(delta > 0, 0).dropna()\n",
    "    loss = -delta.where(delta < 0, 0).dropna()\n",
    "    avg_gain = gain.rolling(window=rsi_period).mean()\n",
    "    avg_loss = loss.rolling(window=rsi_period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "stock['rsi'] = calculate_rsi(stock, 14)\n",
    "\n",
    "# # Stochastic Oscillator calculation\n",
    "# def calculate_stochastic(data, k_window, d_window):\n",
    "#     high_low = data[['High', 'Low']]\n",
    "#     c = data['Close']\n",
    "#     highest = high_low.rolling(window=k_window).max()\n",
    "#     lowest = high_low.rolling(window=k_window).min()\n",
    "#     print(((c - lowest) / (highest - lowest)) * 100)\n",
    "#     stochastic_k = ((c - lowest) / (highest - lowest)) * 100\n",
    "#     stochastic_d = stochastic_k.rolling(window=d_window).mean()\n",
    "#     return stochastic_k, stochastic_d\n",
    "# stock['stochastic_k'], stock['stochastic_d'] = calculate_stochastic(stock, 14, 3)\n",
    "\n",
    "# stock['stochastic_k']= calculate_stochastic(stock, 14, 3)[0]\n",
    "# stock['stochastic_d']= calculate_stochastic(stock, 14, 3)[1]\n",
    "stock['day'] = pd.to_datetime(stock.index).day\n",
    "stock['month'] = pd.to_datetime(stock.index).month\n",
    "stock['year'] = pd.to_datetime(stock.index).year\n",
    "stock['weekday'] = pd.to_datetime(stock.index).weekday\n",
    "stock['dayofyear'] = pd.to_datetime(stock.index).dayofyear\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "stock.index = stock.index.date\n",
    "# Split the data into training and test sets\n",
    "\n",
    "# df = stock.copy()\n",
    "\n",
    "# # Calculate pairwise correlation\n",
    "# corr_matrix = df.corr()\n",
    "\n",
    "# # Identify highly correlated columns\n",
    "# redundant_cols = set()\n",
    "# for i in range(5,len(corr_matrix.columns)-1):\n",
    "#     for j in range(i+1, len(corr_matrix.columns)):\n",
    "#         if corr_matrix.iloc[i,j] > 0.8 and corr_matrix.columns[i] not in redundant_cols:\n",
    "#             redundant_cols.add(corr_matrix.columns[j])\n",
    "\n",
    "# # Remove one of the redundant columns\n",
    "# for col in redundant_cols:\n",
    "#     df = df.drop(col, axis=1)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df)\n",
    "\n",
    "# stock = df.copy()\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:int(0.9*train_data_index)].copy()\n",
    "val_data  = stock.iloc[int(0.9*train_data_index)-sequence_length:train_data_index].copy()\n",
    "test_data = stock.iloc[train_data_index-sequence_length:].copy()\n",
    "train_data = label_data(train_data)\n",
    "val_data = label_data(val_data)\n",
    "test_data = label_data(test_data)\n",
    "train_data.fillna(0,axis = 0, inplace=True)\n",
    "val_data.fillna(0,axis = 0, inplace=True)\n",
    "test_data.fillna(0,axis = 0, inplace=True)\n",
    "\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "#trian & test data\n",
    "X_val_data = val_data.iloc[:,:-1]\n",
    "y_val_data = val_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data), len(X_test_data.columns))\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert targets to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_data, num_classes=3)\n",
    "y_val_onehot = to_categorical(y_val_data, num_classes=3)\n",
    "\n",
    "y_train_data_onehot = to_categorical(y_train_data)\n",
    "y_test_data_onehot = to_categorical(y_test_data)\n",
    "y_val_data_onehot = to_categorical(y_val_data)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_val_data_normalizer = normalizer.fit_transform(X_val_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # # Reshape X_train_data_normalizer\n",
    "X_train_reshaped = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_val_reshaped = X_val_data_normalizer.reshape(X_val_data_normalizer.shape[0], X_val_data_normalizer.shape[1], 1)\n",
    "X_test_reshaped = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "\n",
    "def create_sequences(x,y,sequence_length,stride):\n",
    "    sequence_length  = sequence_length\n",
    "    X_test_data_normalizer_sequences = []\n",
    "    y_test_data_sequences = []\n",
    "    stride = stride\n",
    "    no_of_rows = len(x)\n",
    "    no_of_columns = len(x[0])\n",
    "    for i in range(sequence_length, no_of_rows-1 , stride):\n",
    "        X_test_data_normalizer_sequences.append(x[i-sequence_length: i])\n",
    "        y_test_data_sequences.append(y[i-1])\n",
    "    return np.array(X_test_data_normalizer_sequences),np.array(y_test_data_sequences)\n",
    "        \n",
    "X_train_data_normalizer_sequences,y_train_data_sequences = create_sequences(X_train_data_normalizer,y_train_data,sequence_length,stride)\n",
    "X_test_data_normalizer_sequences,y_test_data_sequences = create_sequences(X_test_data_normalizer,y_test_data,sequence_length,stride)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_train_data_normalizer,\n",
    "    y_train_data,\n",
    "    length = 3,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_val_data_normalizer,\n",
    "    y_val_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_test_data_normalizer,\n",
    "    y_test_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a LightGBM classifier\n",
    "lgbm_model = LGBMClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "lgbm_model.fit(X_train_data_normalizer, y_train_data)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = lgbm_model.predict(X_test_data_normalizer)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test_data, y_pred))\n",
    "\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting + Features + TA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119 239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.47989276139410186\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.49      0.99      0.65       535\n",
      "           0       0.04      0.25      0.07         4\n",
      "           1       0.88      0.01      0.02       580\n",
      "\n",
      "    accuracy                           0.48      1119\n",
      "   macro avg       0.47      0.42      0.25      1119\n",
      "weighted avg       0.69      0.48      0.32      1119\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-05-01'\n",
    "# Parameters\n",
    "batch_size = 256\n",
    "sequence_length = 30\n",
    "stride = 1\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1980-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['pr_change_on_last_day'] = data['Close'].pct_change()\n",
    "    data['pr_change_on_current_day'] = data['pr_change_on_last_day'].shift(-1)\n",
    "    data.iloc[0,-2] = 0\n",
    "    data['sentiment'] = pd.Series(np.where(data['pr_change_on_current_day'] > 0, 1, np.where(data['pr_change_on_current_day'] < 0, -1, 0)), index=data.index)\n",
    "    # data['perc_change'] = data['Percentage Change']\n",
    "    # # Drop any rows with missing values\n",
    "    # data.dropna(inplace=True)\n",
    "    data.drop('pr_change_on_current_day',axis=1 , inplace=True)\n",
    "    return data\n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# Calculate deltas, moving averages, and Bollinger Bands\n",
    "for i in range(1, 90,5):\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "    stock[f\"rolling_mean_open_{i}day\"] = stock[\"Open\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_high_{i}day\"] = stock[\"High\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_low_{i}day\"] = stock[\"Low\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_close_{i}day\"] = stock[\"Close\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_std_open_{i}day\"] = stock[\"Open\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_high_{i}day\"] = stock[\"High\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_low_{i}day\"] = stock[\"Low\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_close_{i}day\"] = stock[\"Close\"].rolling(window=i).std()\n",
    "\n",
    "stock['fast_ma'] = stock['Close'].rolling(window=20).mean()\n",
    "stock['slow_ma'] = stock['Close'].rolling(window=50).mean()\n",
    "stock['bollinger_high'] = stock['Close'].rolling(window=20).mean() + (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['bollinger_low'] = stock['Close'].rolling(window=20).mean() - (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['ema'] = stock['Close'].ewm(span=20, adjust=False).mean()\n",
    "stock['envelope_high'] = stock['Close'].rolling(window=20).mean() * (1 + 0.05)\n",
    "stock['envelope_low'] = stock['Close'].rolling(window=20).mean() * (1 - 0.05)\n",
    "stock['macd_line'] = stock['Close'].ewm(span=12, adjust=False).mean() - stock['Close'].ewm(span=26, adjust=False).mean()\n",
    "stock['macd_signal'] = stock['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# RSI calculation\n",
    "def calculate_rsi(data, rsi_period):\n",
    "    delta = data['Close'].diff().dropna()\n",
    "    gain = delta.where(delta > 0, 0).dropna()\n",
    "    loss = -delta.where(delta < 0, 0).dropna()\n",
    "    avg_gain = gain.rolling(window=rsi_period).mean()\n",
    "    avg_loss = loss.rolling(window=rsi_period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "stock['rsi'] = calculate_rsi(stock, 14)\n",
    "\n",
    "# # Stochastic Oscillator calculation\n",
    "# def calculate_stochastic(data, k_window, d_window):\n",
    "#     high_low = data[['High', 'Low']]\n",
    "#     c = data['Close']\n",
    "#     highest = high_low.rolling(window=k_window).max()\n",
    "#     lowest = high_low.rolling(window=k_window).min()\n",
    "#     print(((c - lowest) / (highest - lowest)) * 100)\n",
    "#     stochastic_k = ((c - lowest) / (highest - lowest)) * 100\n",
    "#     stochastic_d = stochastic_k.rolling(window=d_window).mean()\n",
    "#     return stochastic_k, stochastic_d\n",
    "# stock['stochastic_k'], stock['stochastic_d'] = calculate_stochastic(stock, 14, 3)\n",
    "\n",
    "# stock['stochastic_k']= calculate_stochastic(stock, 14, 3)[0]\n",
    "# stock['stochastic_d']= calculate_stochastic(stock, 14, 3)[1]\n",
    "stock['day'] = pd.to_datetime(stock.index).day\n",
    "stock['month'] = pd.to_datetime(stock.index).month\n",
    "stock['year'] = pd.to_datetime(stock.index).year\n",
    "stock['weekday'] = pd.to_datetime(stock.index).weekday\n",
    "stock['dayofyear'] = pd.to_datetime(stock.index).dayofyear\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "stock.index = stock.index.date\n",
    "# Split the data into training and test sets\n",
    "\n",
    "# df = stock.copy()\n",
    "\n",
    "# # Calculate pairwise correlation\n",
    "# corr_matrix = df.corr()\n",
    "\n",
    "# # Identify highly correlated columns\n",
    "# redundant_cols = set()\n",
    "# for i in range(5,len(corr_matrix.columns)-1):\n",
    "#     for j in range(i+1, len(corr_matrix.columns)):\n",
    "#         if corr_matrix.iloc[i,j] > 0.8 and corr_matrix.columns[i] not in redundant_cols:\n",
    "#             redundant_cols.add(corr_matrix.columns[j])\n",
    "\n",
    "# # Remove one of the redundant columns\n",
    "# for col in redundant_cols:\n",
    "#     df = df.drop(col, axis=1)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df)\n",
    "\n",
    "# stock = df.copy()\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:int(0.9*train_data_index)].copy()\n",
    "val_data  = stock.iloc[int(0.9*train_data_index)-sequence_length:train_data_index].copy()\n",
    "test_data = stock.iloc[train_data_index-sequence_length:].copy()\n",
    "train_data = label_data(train_data)\n",
    "val_data = label_data(val_data)\n",
    "test_data = label_data(test_data)\n",
    "train_data.fillna(0,axis = 0, inplace=True)\n",
    "val_data.fillna(0,axis = 0, inplace=True)\n",
    "test_data.fillna(0,axis = 0, inplace=True)\n",
    "\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "#trian & test data\n",
    "X_val_data = val_data.iloc[:,:-1]\n",
    "y_val_data = val_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data), len(X_test_data.columns))\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert targets to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_data, num_classes=3)\n",
    "y_val_onehot = to_categorical(y_val_data, num_classes=3)\n",
    "\n",
    "y_train_data_onehot = to_categorical(y_train_data)\n",
    "y_test_data_onehot = to_categorical(y_test_data)\n",
    "y_val_data_onehot = to_categorical(y_val_data)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_val_data_normalizer = normalizer.fit_transform(X_val_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # # Reshape X_train_data_normalizer\n",
    "X_train_reshaped = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_val_reshaped = X_val_data_normalizer.reshape(X_val_data_normalizer.shape[0], X_val_data_normalizer.shape[1], 1)\n",
    "X_test_reshaped = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "\n",
    "def create_sequences(x,y,sequence_length,stride):\n",
    "    sequence_length  = sequence_length\n",
    "    X_test_data_normalizer_sequences = []\n",
    "    y_test_data_sequences = []\n",
    "    stride = stride\n",
    "    no_of_rows = len(x)\n",
    "    no_of_columns = len(x[0])\n",
    "    for i in range(sequence_length, no_of_rows-1 , stride):\n",
    "        X_test_data_normalizer_sequences.append(x[i-sequence_length: i])\n",
    "        y_test_data_sequences.append(y[i-1])\n",
    "    return np.array(X_test_data_normalizer_sequences),np.array(y_test_data_sequences)\n",
    "        \n",
    "X_train_data_normalizer_sequences,y_train_data_sequences = create_sequences(X_train_data_normalizer,y_train_data,sequence_length,stride)\n",
    "X_test_data_normalizer_sequences,y_test_data_sequences = create_sequences(X_test_data_normalizer,y_test_data,sequence_length,stride)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_train_data_normalizer,\n",
    "    y_train_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_val_data_normalizer,\n",
    "    y_val_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_test_data_normalizer,\n",
    "    y_test_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a Gradient Boosting classifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "gb_model.fit(X_train_data_normalizer, y_train_data)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = gb_model.predict(X_test_data_normalizer)\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test_data, y_pred))\n",
    "\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST + TECHNICAL INDICATORS + FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119 239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49776586237712245\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.71      0.58       539\n",
      "           1       0.53      0.30      0.39       580\n",
      "\n",
      "    accuracy                           0.50      1119\n",
      "   macro avg       0.51      0.51      0.48      1119\n",
      "weighted avg       0.51      0.50      0.48      1119\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-05-01'\n",
    "# Parameters\n",
    "batch_size = 256\n",
    "sequence_length = 30\n",
    "stride = 1\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1980-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['pr_change_on_last_day'] = data['Close'].pct_change()\n",
    "    data['pr_change_on_current_day'] = data['pr_change_on_last_day'].shift(-1)\n",
    "    data.iloc[0,-2] = 0\n",
    "    data['sentiment'] = pd.Series(np.where(data['pr_change_on_current_day'] > 0, 1, 0), index=data.index)\n",
    "    # data['perc_change'] = data['Percentage Change']\n",
    "    # # Drop any rows with missing values\n",
    "    # data.dropna(inplace=True)\n",
    "    data.drop('pr_change_on_current_day',axis=1 , inplace=True)\n",
    "    return data\n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# Calculate deltas, moving averages, and Bollinger Bands\n",
    "for i in range(1, 90,5):\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "    stock[f\"rolling_mean_open_{i}day\"] = stock[\"Open\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_high_{i}day\"] = stock[\"High\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_low_{i}day\"] = stock[\"Low\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_close_{i}day\"] = stock[\"Close\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_std_open_{i}day\"] = stock[\"Open\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_high_{i}day\"] = stock[\"High\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_low_{i}day\"] = stock[\"Low\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_close_{i}day\"] = stock[\"Close\"].rolling(window=i).std()\n",
    "\n",
    "stock['fast_ma'] = stock['Close'].rolling(window=20).mean()\n",
    "stock['slow_ma'] = stock['Close'].rolling(window=50).mean()\n",
    "stock['bollinger_high'] = stock['Close'].rolling(window=20).mean() + (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['bollinger_low'] = stock['Close'].rolling(window=20).mean() - (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['ema'] = stock['Close'].ewm(span=20, adjust=False).mean()\n",
    "stock['envelope_high'] = stock['Close'].rolling(window=20).mean() * (1 + 0.05)\n",
    "stock['envelope_low'] = stock['Close'].rolling(window=20).mean() * (1 - 0.05)\n",
    "stock['macd_line'] = stock['Close'].ewm(span=12, adjust=False).mean() - stock['Close'].ewm(span=26, adjust=False).mean()\n",
    "stock['macd_signal'] = stock['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# RSI calculation\n",
    "def calculate_rsi(data, rsi_period):\n",
    "    delta = data['Close'].diff().dropna()\n",
    "    gain = delta.where(delta > 0, 0).dropna()\n",
    "    loss = -delta.where(delta < 0, 0).dropna()\n",
    "    avg_gain = gain.rolling(window=rsi_period).mean()\n",
    "    avg_loss = loss.rolling(window=rsi_period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "stock['rsi'] = calculate_rsi(stock, 14)\n",
    "\n",
    "# # Stochastic Oscillator calculation\n",
    "# def calculate_stochastic(data, k_window, d_window):\n",
    "#     high_low = data[['High', 'Low']]\n",
    "#     c = data['Close']\n",
    "#     highest = high_low.rolling(window=k_window).max()\n",
    "#     lowest = high_low.rolling(window=k_window).min()\n",
    "#     print(((c - lowest) / (highest - lowest)) * 100)\n",
    "#     stochastic_k = ((c - lowest) / (highest - lowest)) * 100\n",
    "#     stochastic_d = stochastic_k.rolling(window=d_window).mean()\n",
    "#     return stochastic_k, stochastic_d\n",
    "# stock['stochastic_k'], stock['stochastic_d'] = calculate_stochastic(stock, 14, 3)\n",
    "\n",
    "# stock['stochastic_k']= calculate_stochastic(stock, 14, 3)[0]\n",
    "# stock['stochastic_d']= calculate_stochastic(stock, 14, 3)[1]\n",
    "stock['day'] = pd.to_datetime(stock.index).day\n",
    "stock['month'] = pd.to_datetime(stock.index).month\n",
    "stock['year'] = pd.to_datetime(stock.index).year\n",
    "stock['weekday'] = pd.to_datetime(stock.index).weekday\n",
    "stock['dayofyear'] = pd.to_datetime(stock.index).dayofyear\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "stock.index = stock.index.date\n",
    "# Split the data into training and test sets\n",
    "\n",
    "# df = stock.copy()\n",
    "\n",
    "# # Calculate pairwise correlation\n",
    "# corr_matrix = df.corr()\n",
    "\n",
    "# # Identify highly correlated columns\n",
    "# redundant_cols = set()\n",
    "# for i in range(5,len(corr_matrix.columns)-1):\n",
    "#     for j in range(i+1, len(corr_matrix.columns)):\n",
    "#         if corr_matrix.iloc[i,j] > 0.8 and corr_matrix.columns[i] not in redundant_cols:\n",
    "#             redundant_cols.add(corr_matrix.columns[j])\n",
    "\n",
    "# # Remove one of the redundant columns\n",
    "# for col in redundant_cols:\n",
    "#     df = df.drop(col, axis=1)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df)\n",
    "\n",
    "# stock = df.copy()\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:int(0.9*train_data_index)].copy()\n",
    "val_data  = stock.iloc[int(0.9*train_data_index)-sequence_length:train_data_index].copy()\n",
    "test_data = stock.iloc[train_data_index-sequence_length:].copy()\n",
    "train_data = label_data(train_data)\n",
    "val_data = label_data(val_data)\n",
    "test_data = label_data(test_data)\n",
    "train_data.fillna(0,axis = 0, inplace=True)\n",
    "val_data.fillna(0,axis = 0, inplace=True)\n",
    "test_data.fillna(0,axis = 0, inplace=True)\n",
    "\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "#trian & test data\n",
    "X_val_data = val_data.iloc[:,:-1]\n",
    "y_val_data = val_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data), len(X_test_data.columns))\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert targets to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_data, num_classes=3)\n",
    "y_val_onehot = to_categorical(y_val_data, num_classes=3)\n",
    "\n",
    "y_train_data_onehot = to_categorical(y_train_data)\n",
    "y_test_data_onehot = to_categorical(y_test_data)\n",
    "y_val_data_onehot = to_categorical(y_val_data)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_val_data_normalizer = normalizer.fit_transform(X_val_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # # Reshape X_train_data_normalizer\n",
    "X_train_reshaped = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_val_reshaped = X_val_data_normalizer.reshape(X_val_data_normalizer.shape[0], X_val_data_normalizer.shape[1], 1)\n",
    "X_test_reshaped = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "\n",
    "def create_sequences(x,y,sequence_length,stride):\n",
    "    sequence_length  = sequence_length\n",
    "    X_test_data_normalizer_sequences = []\n",
    "    y_test_data_sequences = []\n",
    "    stride = stride\n",
    "    no_of_rows = len(x)\n",
    "    no_of_columns = len(x[0])\n",
    "    for i in range(sequence_length, no_of_rows-1 , stride):\n",
    "        X_test_data_normalizer_sequences.append(x[i-sequence_length: i])\n",
    "        y_test_data_sequences.append(y[i-1])\n",
    "    return np.array(X_test_data_normalizer_sequences),np.array(y_test_data_sequences)\n",
    "        \n",
    "X_train_data_normalizer_sequences,y_train_data_sequences = create_sequences(X_train_data_normalizer,y_train_data,sequence_length,stride)\n",
    "X_test_data_normalizer_sequences,y_test_data_sequences = create_sequences(X_test_data_normalizer,y_test_data,sequence_length,stride)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_train_data_normalizer,\n",
    "    y_train_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_val_data_normalizer,\n",
    "    y_val_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_test_data_normalizer,\n",
    "    y_test_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "xgb_model.fit(X_train_data_normalizer, y_train_data)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = xgb_model.predict(X_test_data_normalizer)\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test_data, y_pred))\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119 239\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-05-01'\n",
    "# Parameters\n",
    "batch_size = 256\n",
    "sequence_length = 30\n",
    "stride = 1\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1980-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['pr_change_on_last_day'] = data['Close'].pct_change()\n",
    "    data['pr_change_on_current_day'] = data['pr_change_on_last_day'].shift(-1)\n",
    "    data.iloc[0,-2] = 0\n",
    "    data['sentiment'] = pd.Series(np.where(data['pr_change_on_current_day'] > 0, 1, 0), index=data.index)\n",
    "    # data['perc_change'] = data['Percentage Change']\n",
    "    # # Drop any rows with missing values\n",
    "    # data.dropna(inplace=True)\n",
    "    data.drop('pr_change_on_current_day',axis=1 , inplace=True)\n",
    "    return data\n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# Calculate deltas, moving averages, and Bollinger Bands\n",
    "for i in range(1, 90,5):\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "    stock[f\"rolling_mean_open_{i}day\"] = stock[\"Open\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_high_{i}day\"] = stock[\"High\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_low_{i}day\"] = stock[\"Low\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_close_{i}day\"] = stock[\"Close\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_std_open_{i}day\"] = stock[\"Open\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_high_{i}day\"] = stock[\"High\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_low_{i}day\"] = stock[\"Low\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_close_{i}day\"] = stock[\"Close\"].rolling(window=i).std()\n",
    "\n",
    "stock['fast_ma'] = stock['Close'].rolling(window=20).mean()\n",
    "stock['slow_ma'] = stock['Close'].rolling(window=50).mean()\n",
    "stock['bollinger_high'] = stock['Close'].rolling(window=20).mean() + (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['bollinger_low'] = stock['Close'].rolling(window=20).mean() - (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['ema'] = stock['Close'].ewm(span=20, adjust=False).mean()\n",
    "stock['envelope_high'] = stock['Close'].rolling(window=20).mean() * (1 + 0.05)\n",
    "stock['envelope_low'] = stock['Close'].rolling(window=20).mean() * (1 - 0.05)\n",
    "stock['macd_line'] = stock['Close'].ewm(span=12, adjust=False).mean() - stock['Close'].ewm(span=26, adjust=False).mean()\n",
    "stock['macd_signal'] = stock['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# RSI calculation\n",
    "def calculate_rsi(data, rsi_period):\n",
    "    delta = data['Close'].diff().dropna()\n",
    "    gain = delta.where(delta > 0, 0).dropna()\n",
    "    loss = -delta.where(delta < 0, 0).dropna()\n",
    "    avg_gain = gain.rolling(window=rsi_period).mean()\n",
    "    avg_loss = loss.rolling(window=rsi_period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "stock['rsi'] = calculate_rsi(stock, 14)\n",
    "\n",
    "# # Stochastic Oscillator calculation\n",
    "# def calculate_stochastic(data, k_window, d_window):\n",
    "#     high_low = data[['High', 'Low']]\n",
    "#     c = data['Close']\n",
    "#     highest = high_low.rolling(window=k_window).max()\n",
    "#     lowest = high_low.rolling(window=k_window).min()\n",
    "#     print(((c - lowest) / (highest - lowest)) * 100)\n",
    "#     stochastic_k = ((c - lowest) / (highest - lowest)) * 100\n",
    "#     stochastic_d = stochastic_k.rolling(window=d_window).mean()\n",
    "#     return stochastic_k, stochastic_d\n",
    "# stock['stochastic_k'], stock['stochastic_d'] = calculate_stochastic(stock, 14, 3)\n",
    "\n",
    "# stock['stochastic_k']= calculate_stochastic(stock, 14, 3)[0]\n",
    "# stock['stochastic_d']= calculate_stochastic(stock, 14, 3)[1]\n",
    "stock['day'] = pd.to_datetime(stock.index).day\n",
    "stock['month'] = pd.to_datetime(stock.index).month\n",
    "stock['year'] = pd.to_datetime(stock.index).year\n",
    "stock['weekday'] = pd.to_datetime(stock.index).weekday\n",
    "stock['dayofyear'] = pd.to_datetime(stock.index).dayofyear\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "stock.index = stock.index.date\n",
    "# Split the data into training and test sets\n",
    "\n",
    "# df = stock.copy()\n",
    "\n",
    "# # Calculate pairwise correlation\n",
    "# corr_matrix = df.corr()\n",
    "\n",
    "# # Identify highly correlated columns\n",
    "# redundant_cols = set()\n",
    "# for i in range(5,len(corr_matrix.columns)-1):\n",
    "#     for j in range(i+1, len(corr_matrix.columns)):\n",
    "#         if corr_matrix.iloc[i,j] > 0.8 and corr_matrix.columns[i] not in redundant_cols:\n",
    "#             redundant_cols.add(corr_matrix.columns[j])\n",
    "\n",
    "# # Remove one of the redundant columns\n",
    "# for col in redundant_cols:\n",
    "#     df = df.drop(col, axis=1)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df)\n",
    "\n",
    "# stock = df.copy()\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:int(0.9*train_data_index)].copy()\n",
    "val_data  = stock.iloc[int(0.9*train_data_index)-sequence_length:train_data_index].copy()\n",
    "test_data = stock.iloc[train_data_index-sequence_length:].copy()\n",
    "train_data = label_data(train_data)\n",
    "val_data = label_data(val_data)\n",
    "test_data = label_data(test_data)\n",
    "train_data.fillna(0,axis = 0, inplace=True)\n",
    "val_data.fillna(0,axis = 0, inplace=True)\n",
    "test_data.fillna(0,axis = 0, inplace=True)\n",
    "\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "#trian & test data\n",
    "X_val_data = val_data.iloc[:,:-1]\n",
    "y_val_data = val_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data), len(X_test_data.columns))\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert targets to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_data, num_classes=3)\n",
    "y_val_onehot = to_categorical(y_val_data, num_classes=3)\n",
    "\n",
    "y_train_data_onehot = to_categorical(y_train_data)\n",
    "y_test_data_onehot = to_categorical(y_test_data)\n",
    "y_val_data_onehot = to_categorical(y_val_data)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_val_data_normalizer = normalizer.fit_transform(X_val_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # # Reshape X_train_data_normalizer\n",
    "X_train_reshaped = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_val_reshaped = X_val_data_normalizer.reshape(X_val_data_normalizer.shape[0], X_val_data_normalizer.shape[1], 1)\n",
    "X_test_reshaped = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "\n",
    "def create_sequences(x,y,sequence_length,stride):\n",
    "    sequence_length  = sequence_length\n",
    "    X_test_data_normalizer_sequences = []\n",
    "    y_test_data_sequences = []\n",
    "    stride = stride\n",
    "    no_of_rows = len(x)\n",
    "    no_of_columns = len(x[0])\n",
    "    for i in range(sequence_length, no_of_rows-1 , stride):\n",
    "        X_test_data_normalizer_sequences.append(x[i-sequence_length: i])\n",
    "        y_test_data_sequences.append(y[i-1])\n",
    "    return np.array(X_test_data_normalizer_sequences),np.array(y_test_data_sequences)\n",
    "        \n",
    "X_train_data_normalizer_sequences,y_train_data_sequences = create_sequences(X_train_data_normalizer,y_train_data,sequence_length,stride)\n",
    "X_test_data_normalizer_sequences,y_test_data_sequences = create_sequences(X_test_data_normalizer,y_test_data,sequence_length,stride)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_train_data_normalizer,\n",
    "    y_train_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_val_data_normalizer,\n",
    "    y_val_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_test_data_normalizer,\n",
    "    y_test_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
