{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-05-01'\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1980-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['Percentage Change'] = data['Close'].pct_change()\n",
    "    data['Percentage Change'] = data['Percentage Change'].shift(-1)\n",
    "    # data['Sentiment'] = pd.Series(np.where(data['Percentage Change'] > 0.025, 1, np.where(data['Percentage Change'] < -0.025, -1, 0)), index=data.index)\n",
    "    # data['perc_change'] = data['Percentage Change']\n",
    "    # # Drop any rows with missing values\n",
    "    # data.dropna(inplace=True)\n",
    "    # data.drop('Percentage Change',axis=1 , inplace=True)\n",
    "    return data\n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# Calculate deltas, moving averages, and Bollinger Bands\n",
    "for i in range(1, 30):\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "    stock[f\"rolling_mean_open_{i}day\"] = stock[\"Open\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_high_{i}day\"] = stock[\"High\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_low_{i}day\"] = stock[\"Low\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_close_{i}day\"] = stock[\"Close\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_std_open_{i}day\"] = stock[\"Open\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_high_{i}day\"] = stock[\"High\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_low_{i}day\"] = stock[\"Low\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_close_{i}day\"] = stock[\"Close\"].rolling(window=i).std()\n",
    "\n",
    "stock['fast_ma'] = stock['Close'].rolling(window=20).mean()\n",
    "stock['slow_ma'] = stock['Close'].rolling(window=50).mean()\n",
    "stock['bollinger_high'] = stock['Close'].rolling(window=20).mean() + (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['bollinger_low'] = stock['Close'].rolling(window=20).mean() - (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['ema'] = stock['Close'].ewm(span=20, adjust=False).mean()\n",
    "stock['envelope_high'] = stock['Close'].rolling(window=20).mean() * (1 + 0.05)\n",
    "stock['envelope_low'] = stock['Close'].rolling(window=20).mean() * (1 - 0.05)\n",
    "stock['macd_line'] = stock['Close'].ewm(span=12, adjust=False).mean() - stock['Close'].ewm(span=26, adjust=False).mean()\n",
    "stock['macd_signal'] = stock['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# RSI calculation\n",
    "def calculate_rsi(data, rsi_period):\n",
    "    delta = data['Close'].diff().dropna()\n",
    "    gain = delta.where(delta > 0, 0).dropna()\n",
    "    loss = -delta.where(delta < 0, 0).dropna()\n",
    "    avg_gain = gain.rolling(window=rsi_period).mean()\n",
    "    avg_loss = loss.rolling(window=rsi_period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "stock['rsi'] = calculate_rsi(stock, 14)\n",
    "\n",
    "# # Stochastic Oscillator calculation\n",
    "# def calculate_stochastic(data, k_window, d_window):\n",
    "#     high_low = data[['High', 'Low']]\n",
    "#     c = data['Close']\n",
    "#     highest = high_low.rolling(window=k_window).max()\n",
    "#     lowest = high_low.rolling(window=k_window).min()\n",
    "#     print(((c - lowest) / (highest - lowest)) * 100)\n",
    "#     stochastic_k = ((c - lowest) / (highest - lowest)) * 100\n",
    "#     stochastic_d = stochastic_k.rolling(window=d_window).mean()\n",
    "#     return stochastic_k, stochastic_d\n",
    "# stock['stochastic_k'], stock['stochastic_d'] = calculate_stochastic(stock, 14, 3)\n",
    "\n",
    "# stock['stochastic_k']= calculate_stochastic(stock, 14, 3)[0]\n",
    "# stock['stochastic_d']= calculate_stochastic(stock, 14, 3)[1]\n",
    "stock['day'] = pd.to_datetime(stock.index).day\n",
    "stock['month'] = pd.to_datetime(stock.index).month\n",
    "stock['year'] = pd.to_datetime(stock.index).year\n",
    "stock['weekday'] = pd.to_datetime(stock.index).weekday\n",
    "stock['dayofyear'] = pd.to_datetime(stock.index).dayofyear\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "stock.index = stock.index.date\n",
    "# Split the data into training and test sets\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:train_data_index]\n",
    "test_data = stock.iloc[train_data_index:]\n",
    "train_data = label_data(train_data)\n",
    "test_data = label_data(test_data)\n",
    "train_data.fillna(method='ffill',axis = 0, inplace=True)\n",
    "test_data.fillna(method='ffill',axis = 0, inplace=True)\n",
    "\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data))\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # # Reshape X_train_data_normalizer\n",
    "X_train_reshaped = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_test_reshaped = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "data = tf.cast(X_train_data_normalizer,tf.float32)\n",
    "targets = tf.cast(y_train_data,tf.float32)\n",
    "sample_length = 22\n",
    "input_dataset = tf.keras.utils.timeseries_dataset_from_array(data,targets, sequence_length=sample_length,batch_size=256, sequence_stride=sample_length)\n",
    "# target_dataset = tf.keras.utils.timeseries_dataset_from_array(targets, None, sequence_length=6,batch_size=256, sequence_stride=sample_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = tf.keras.utils.timeseries_dataset_from_array(data,targets, sequence_length=sample_length,batch_size=256, sequence_stride=sample_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_BatchDataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43minput_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: '_BatchDataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "input_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.87 GiB for an array with shape (7581, 370, 370, 1) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m y_train_seq, y_val_seq \u001b[38;5;241m=\u001b[39m y_train_sequences[:train_size], y_train_sequences[train_size:]\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Train the model on the training data\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mlstm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Load the best saved model\u001b[39;00m\n\u001b[0;32m     53\u001b[0m best_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:96\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m     98\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.87 GiB for an array with shape (7581, 370, 370, 1) and data type float32"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define the LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(256, input_shape=(input_dataset.shape[1], 370), return_sequences=True))\n",
    "lstm_model.add(Dropout(0.3))\n",
    "lstm_model.add(LSTM(128, return_sequences=True))\n",
    "lstm_model.add(Dropout(0.3))\n",
    "lstm_model.add(LSTM(64, return_sequences=True))\n",
    "lstm_model.add(Dropout(0.3))\n",
    "lstm_model.add(LSTM(32, return_sequences=True))\n",
    "lstm_model.add(Dropout(0.3))\n",
    "lstm_model.add(LSTM(16, return_sequences=True))\n",
    "lstm_model.add(Dropout(0.3))\n",
    "lstm_model.add(LSTM(8, return_sequences=True))\n",
    "lstm_model.add(Dropout(0.3))\n",
    "lstm_model.add(LSTM(4, return_sequences=True))\n",
    "lstm_model.add(Dropout(0.3))\n",
    "lstm_model.add(Dense(64, activation='relu'))\n",
    "lstm_model.add(Dropout(0.3))\n",
    "lstm_model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='min', restore_best_weights=True)\n",
    "\n",
    "# Define model checkpoint to save the best model during training\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Reshape the training data to have a sequence of values at each time step\n",
    "X_train_sequences = []\n",
    "y_train_sequences = []\n",
    "for i in range(X_train.shape[0] - X_train.shape[1]):\n",
    "    X_train_sequences.append(X_train[i:i+X_train.shape[1], :, :])\n",
    "    y_train_sequences.append(y_train_data[i+X_train.shape[1]])\n",
    "X_train_sequences = np.array(X_train_sequences)\n",
    "y_train_sequences = np.array(y_train_sequences)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_size = int(0.8 * X_train_sequences.shape[0])\n",
    "val_size = X_train_sequences.shape[0] - train_size\n",
    "X_train_seq, X_val_seq = X_train_sequences[:train_size], X_train_sequences[train_size:]\n",
    "y_train_seq, y_val_seq = y_train_sequences[:train_size], y_train_sequences[train_size:]\n",
    "\n",
    "# Train the model on the training data\n",
    "history = lstm_model.fit(X_train_seq, y_train_seq, epochs=100, batch_size=64, validation_data=(X_val_seq, y_val_seq), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Load the best saved model\n",
    "best_model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "# Predict labels for the test set using the best model\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = y_pred_probs.flatten()\n",
    "\n",
    "# Inverse transform the predictions to get the actual predicted values\n",
    "y_pred_inverse_transformed = normalizer.inverse_transform(y_pred.reshape(-1, 1))\n",
    "\n",
    "# Calculate the root mean squared error (RMSE) between the actual and predicted values\n",
    "rmse = np.sqrt(mean_squared_error(y_test_data, y_pred_inverse_transformed))\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# Plot the predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test_data, label='Actual')\n",
    "plt.plot(y_pred_inverse_transformed, label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerModel\n",
    "\n",
    "# Initializing a Time Series Transformer configuration with 12 time steps for prediction\n",
    "configuration = TimeSeriesTransformerConfig(prediction_length=12)\n",
    "\n",
    "# Randomly initializing a model (with random weights) from the configuration\n",
    "model = TimeSeriesTransformerModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "from transformers import TimeSeriesTransformerModel\n",
    "\n",
    "file = hf_hub_download(\n",
    "    repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=\"train-batch.pt\", repo_type=\"dataset\"\n",
    ")\n",
    "batch = torch.load(file)\n",
    "\n",
    "model = TimeSeriesTransformerModel.from_pretrained(\"huggingface/time-series-transformer-tourism-monthly\")\n",
    "\n",
    "# during training, one provides both past and future values\n",
    "# as well as possible additional features\n",
    "outputs = model(\n",
    "    past_values=batch[\"past_values\"],\n",
    "    past_time_features=batch[\"past_time_features\"],\n",
    "    past_observed_mask=batch[\"past_observed_mask\"],\n",
    "    static_categorical_features=batch[\"static_categorical_features\"],\n",
    "    static_real_features=batch[\"static_real_features\"],\n",
    "    future_values=batch[\"future_values\"],\n",
    "    future_time_features=batch[\"future_time_features\"],\n",
    ")\n",
    "\n",
    "last_hidden_state = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "from transformers import TimeSeriesTransformerForPrediction\n",
    "\n",
    "file = hf_hub_download(\n",
    "    repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=\"train-batch.pt\", repo_type=\"dataset\"\n",
    ")\n",
    "batch = torch.load(file)\n",
    "\n",
    "model = TimeSeriesTransformerForPrediction.from_pretrained(\n",
    "    \"huggingface/time-series-transformer-tourism-monthly\"\n",
    ")\n",
    "\n",
    "# during training, one provides both past and future values\n",
    "# as well as possible additional features\n",
    "outputs = model(\n",
    "    past_values=batch[\"past_values\"],\n",
    "    past_time_features=batch[\"past_time_features\"],\n",
    "    past_observed_mask=batch[\"past_observed_mask\"],\n",
    "    static_categorical_features=batch[\"static_categorical_features\"],\n",
    "    static_real_features=batch[\"static_real_features\"],\n",
    "    future_values=batch[\"future_values\"],\n",
    "    future_time_features=batch[\"future_time_features\"],\n",
    ")\n",
    "\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "\n",
    "# during inference, one only provides past values\n",
    "# as well as possible additional features\n",
    "# the model autoregressively generates future values\n",
    "outputs = model.generate(\n",
    "    past_values=batch[\"past_values\"],\n",
    "    past_time_features=batch[\"past_time_features\"],\n",
    "    past_observed_mask=batch[\"past_observed_mask\"],\n",
    "    static_categorical_features=batch[\"static_categorical_features\"],\n",
    "    static_real_features=batch[\"static_real_features\"],\n",
    "    future_time_features=batch[\"future_time_features\"],\n",
    ")\n",
    "\n",
    "mean_prediction = outputs.sequences.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
