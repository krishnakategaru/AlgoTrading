{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# Define the Transformer model\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = tf.keras.layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "# Build and train the model\n",
    "input_shape = (7,1)\n",
    "head_size = 46\n",
    "num_heads = 60\n",
    "ff_dim = 55\n",
    "num_transformer_blocks = 5\n",
    "mlp_units = [256]\n",
    "dropout = 0.14\n",
    "mlp_dropout = 0.4\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "\n",
    "    for _ in range(num_transformer_blocks):  # This is what stacks our transformer blocks\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "\n",
    "    for dim in mlp_units:\n",
    "        x = tf.keras.layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(mlp_dropout)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"softmax\")(x)  # this is a pass-through\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "def lr_scheduler(epoch, lr, warmup_epochs=30, decay_epochs=100, initial_lr=1e-6, base_lr=1e-3, min_lr=5e-5):\n",
    "    if epoch <= warmup_epochs:\n",
    "        pct = epoch / warmup_epochs\n",
    "        return ((base_lr - initial_lr) * pct) + initial_lr\n",
    "\n",
    "    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n",
    "        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n",
    "        return ((base_lr - min_lr) * pct) + min_lr\n",
    "\n",
    "    return min_lr\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='2000-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "# def label_data(data):\n",
    "#     # Calculate the percentage change in price from one day to the next\n",
    "#     data['Percentage Change'] = data['Close'].pct_change()\n",
    "#     data['Percentage Change'] = data['Percentage Change'].shift(-1)\n",
    "#     data['Sentiment'] = pd.Series(np.where(data['Percentage Change'] > 0.025, 1, np.where(data['Percentage Change'] < -0.025, -1, 0)), index=data.index)\n",
    "#     # Drop any rows with missing values\n",
    "#     data.dropna(inplace=True)\n",
    "#     data.drop('Percentage Change',axis=1 , inplace=True)\n",
    "#     return data\n",
    "\n",
    "def train_transformer(symbol_to_fetch,start_date ,end_date,no_model = None):\n",
    "    #fetching data \n",
    "    stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "    stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "    stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "    # stock.index = stock.index.date\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "    train_data = stock.iloc[:train_data_index]\n",
    "    test_data = stock.loc[start_date:]\n",
    "    train_data = label_data(train_data)\n",
    "    test_data = label_data(test_data)\n",
    "\n",
    "    #trian & test data\n",
    "    X_train_data = train_data.iloc[:,:-1]\n",
    "    y_train_data = train_data.iloc[:,-1]\n",
    "    X_test_data = test_data.iloc[:,:-1]\n",
    "    y_test_data = test_data.iloc[:,-1]\n",
    "    print(len(X_test_data))\n",
    "    # Normalize the data\n",
    "    normalizer = MinMaxScaler()\n",
    "    X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "    X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "    # # Reshape X_train_data_normalizer\n",
    "    X_train = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "    X_test = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "    if not no_model :\n",
    "        model = build_model(\n",
    "            input_shape,\n",
    "            head_size=head_size,\n",
    "            num_heads=num_heads,\n",
    "            ff_dim=ff_dim,\n",
    "            num_transformer_blocks=num_transformer_blocks,\n",
    "            mlp_units=mlp_units,\n",
    "            mlp_dropout=mlp_dropout,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            loss=\"mean_squared_error\",\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "            metrics=[\"mean_squared_error\"],\n",
    "        )\n",
    "\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "        ]\n",
    "\n",
    "        # model.summary()\n",
    "        history = model.fit(\n",
    "            X_train,\n",
    "            y_train_data,\n",
    "            validation_split=0.2,\n",
    "            epochs=100,\n",
    "            batch_size=20,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        model.save('models/transformer_'+f\"{symbol_to_fetch}\"+\"_model.h5\")\n",
    "        model.save('models/transformer_'+f\"{symbol_to_fetch}\"+\"_model.keras\")\n",
    "        no_model = model\n",
    "    return no_model,X_test,test_data\n",
    "    #predictions\n",
    "def prepare_sentiment_from_transformer(symbol_to_fetch,start_date,end_date):\n",
    "    try :\n",
    "        print(\"entered\")\n",
    "        model = tf.keras.models.load_model('models/transformer_'+f\"{symbol_to_fetch}\"+\"_model.keras\")\n",
    "        print(\"passed\")\n",
    "        _,X_test,test_data = train_transformer(symbol_to_fetch = symbol_to_fetch, start_date = start_date, end_date = end_date,no_model=model)\n",
    "    \n",
    "    except:\n",
    "        model,X_test,test_data = train_transformer(symbol_to_fetch = symbol_to_fetch, start_date = start_date, end_date = end_date)\n",
    "    y_pred = model.predict(X_test) # this is the sentiment data \n",
    "\n",
    "    test_data['transformer_sentiment'] = y_pred\n",
    "\n",
    "    test_data.index = test_data.index.date\n",
    "    test_data.to_csv('data/transformer_sentiment.csv')\n",
    "    return test_data,'data/transformer_sentiment.csv'\n",
    "    \"\"\"next steps :  we need to additionally train the model if model is already present\n",
    "    or take nearly 30 stocks and train the model with the huge data \n",
    "    or take every 5 mins data nad trian with it, and at last mix the test data with day wise\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['Percentage Change'] = data['Close'].pct_change()\n",
    "    data['Percentage Change'] = data['Percentage Change'].shift(-1)\n",
    "    data['Sentiment'] = pd.Series(np.where(data['Percentage Change'] > 0.025, 2, np.where(data['Percentage Change'] < -0.025, 0, 1)), index=data.index)\n",
    "    # Drop any rows with missing values\n",
    "    data.dropna(inplace=True)\n",
    "    data.drop('Percentage Change',axis=1 , inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "#fetching data \n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2022-05-01'\n",
    "end_date = '2022-08-01'\n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# Calculate deltas for open, high, low, and close columns\n",
    "for i in range(1, 90):  # Calculate deltas up to 5 days\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "        # Rolling mean and standard deviation of OHLC prices\n",
    "    stock['Rolling_Mean_Open_{i}day'] = stock['Open'].rolling(window=i).mean()\n",
    "    stock['Rolling_Mean_High_{i}day'] = stock['High'].rolling(window=i).mean()\n",
    "    stock['Rolling_Mean_Low_{i}day'] = stock['Low'].rolling(window=i).mean()\n",
    "    stock['Rolling_Mean_Close_{i}day'] = stock['Close'].rolling(window=i).mean()\n",
    "\n",
    "    stock['Rolling_Std_Open_{i}day'] = stock['Open'].rolling(window=i).std()\n",
    "    stock['Rolling_Std_High_{i}day'] = stock['High'].rolling(window=i).std()\n",
    "    stock['Rolling_Std_Low_{i}day'] = stock['Low'].rolling(window=i).std()\n",
    "    stock['Rolling_Std_Close_{i}day'] = stock['Close'].rolling(window=i).std()\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "# stock.index = stock.index.date\n",
    "# Add date-related features\n",
    "stock['Year'] = stock.index.year\n",
    "stock['Month'] = stock.index.month\n",
    "stock['Day'] = stock.index.day\n",
    "stock['Weekday'] = stock.index.weekday\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:train_data_index]\n",
    "test_data = stock.loc[start_date:]\n",
    "train_data = label_data(train_data)\n",
    "test_data = label_data(test_data)\n",
    "\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data))\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # Reshape X_train_data_normalizer\n",
    "X_train = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_test = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6229508196721312\n",
      "[0 1 1 1 1 1 1 1 2 2 2 1 2 2 2 1 1 2 1 1 1 1 1 1 1 1 1 1 2 2 1 1 2 2 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "xgb_model.fit(X_train_data_normalizer, y_train_data)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = xgb_model.predict(X_test_data_normalizer)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.36065573770491804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 0, 0, 0, 0, 2, 0, 0, 1, 2,\n",
       "       0, 2, 2, 2, 1, 2, 2, 2, 0, 0, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "logistic_model.fit(X_train_data_normalizer, y_train_data)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = logistic_model.predict(X_test_data_normalizer)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3770491803278688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train_data_normalizer, y_train_data)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = rf_model.predict(X_test_data_normalizer)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6721311475409836\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "svm_model.fit(X_train_data_normalizer, y_train_data)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = svm_model.predict(X_test_data_normalizer)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 94177\n",
      "[LightGBM] [Info] Number of data points in the train set: 5617, number of used features: 374\n",
      "[LightGBM] [Info] Start training from score -2.303832\n",
      "[LightGBM] [Info] Start training from score -0.244420\n",
      "[LightGBM] [Info] Start training from score -2.145869\n",
      "Accuracy: 0.6557377049180327\n",
      "[1 1 1 1 2 2 1 2 2 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 2 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a LightGBM classifier\n",
    "lgbm_model = LGBMClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "lgbm_model.fit(X_train_data_normalizer, y_train_data)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = lgbm_model.predict(X_test_data_normalizer)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2459016393442623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1,\n",
       "       1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a Gradient Boosting classifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "gb_model.fit(X_train_data_normalizer, y_train_data)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = gb_model.predict(X_test_data_normalizer)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 6s/step - accuracy: 0.7859 - loss: 5.9269 - val_accuracy: 0.7936 - val_loss: 5.9269\n",
      "Epoch 2/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 5s/step - accuracy: 0.7833 - loss: 5.9269 - val_accuracy: 0.7936 - val_loss: 5.9269\n",
      "Epoch 3/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 5s/step - accuracy: 0.7774 - loss: 5.9269 - val_accuracy: 0.7936 - val_loss: 5.9269\n",
      "Epoch 4/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 5s/step - accuracy: 0.7747 - loss: 5.9269 - val_accuracy: 0.7936 - val_loss: 5.9269\n",
      "Epoch 5/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 5s/step - accuracy: 0.7806 - loss: 5.9269 - val_accuracy: 0.7936 - val_loss: 5.9269\n",
      "Epoch 6/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 5s/step - accuracy: 0.7769 - loss: 5.9269 - val_accuracy: 0.7936 - val_loss: 5.9269\n",
      "Epoch 7/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 5s/step - accuracy: 0.7895 - loss: 5.9269 - val_accuracy: 0.7936 - val_loss: 5.9269\n",
      "Epoch 8/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 5s/step - accuracy: 0.7928 - loss: 5.9269 - val_accuracy: 0.7936 - val_loss: 5.9269\n",
      "Epoch 9/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 5s/step - accuracy: 0.7739 - loss: 5.9269 - val_accuracy: 0.7936 - val_loss: 5.9269\n",
      "Epoch 10/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83571s\u001b[0m 1071s/step - accuracy: 0.7847 - loss: 5.9269 - val_accuracy: 0.7936 - val_loss: 5.9269\n",
      "Epoch 11/100\n",
      "\u001b[1m57/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 4s/step - accuracy: 0.7791 - loss: 5.9269"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the LSTM model with more layers\n",
    "lstm_model = Sequential([\n",
    "    LSTM(256, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "     LSTM(16, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "     LSTM(8, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "     LSTM(4, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='softmax')  # Adjust num_classes according to your problem\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='min', restore_best_weights=True)\n",
    "\n",
    "# Define model checkpoint to save the best model during training\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Train the model\n",
    "history = lstm_model.fit(X_train_data_normalizer, y_train_data, epochs=100, batch_size=64, validation_split=0.1, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Load the best saved model\n",
    "best_model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "# Predict labels for the test set using the best model\n",
    "y_pred_probs = best_model.predict(X_test_data_normalizer)\n",
    "y_pred = tf.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5617, 27, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5617, 27)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
