{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class PScan(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def pscan(A, X):\n",
    "        # A : (B, D, L, N)\n",
    "        # X : (B, D, L, N)\n",
    "\n",
    "        # modifies X in place by doing a parallel scan.\n",
    "        # more formally, X will be populated by these values :\n",
    "        # H[t] = A[t] * H[t-1] + X[t] with H[0] = 0\n",
    "        # which are computed in parallel (2*log2(T) sequential steps (ideally), instead of T sequential steps)\n",
    "\n",
    "        # only supports L that is a power of two (mainly for a clearer code)\n",
    "        \n",
    "        B, D, L, _ = A.size()\n",
    "        num_steps = int(math.log2(L))\n",
    "\n",
    "        # up sweep (last 2 steps unfolded)\n",
    "        Aa = A\n",
    "        Xa = X\n",
    "        for _ in range(num_steps-2):\n",
    "            T = Xa.size(2)\n",
    "            Aa = Aa.view(B, D, T//2, 2, -1)\n",
    "            Xa = Xa.view(B, D, T//2, 2, -1)\n",
    "            \n",
    "            Xa[:, :, :, 1].add_(Aa[:, :, :, 1].mul(Xa[:, :, :, 0]))\n",
    "            Aa[:, :, :, 1].mul_(Aa[:, :, :, 0])\n",
    "\n",
    "            Aa = Aa[:, :, :, 1]\n",
    "            Xa = Xa[:, :, :, 1]\n",
    "\n",
    "        # we have only 4, 2 or 1 nodes left\n",
    "        if Xa.size(2) == 4:\n",
    "            Xa[:, :, 1].add_(Aa[:, :, 1].mul(Xa[:, :, 0]))\n",
    "            Aa[:, :, 1].mul_(Aa[:, :, 0])\n",
    "\n",
    "            Xa[:, :, 3].add_(Aa[:, :, 3].mul(Xa[:, :, 2] + Aa[:, :, 2].mul(Xa[:, :, 1])))\n",
    "        elif Xa.size(2) == 2:\n",
    "            Xa[:, :, 1].add_(Aa[:, :, 1].mul(Xa[:, :, 0]))\n",
    "            return\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        # down sweep (first 2 steps unfolded)\n",
    "        Aa = A[:, :, 2**(num_steps-2)-1:L:2**(num_steps-2)]\n",
    "        Xa = X[:, :, 2**(num_steps-2)-1:L:2**(num_steps-2)]\n",
    "        Xa[:, :, 2].add_(Aa[:, :, 2].mul(Xa[:, :, 1]))\n",
    "        Aa[:, :, 2].mul_(Aa[:, :, 1])\n",
    "\n",
    "        for k in range(num_steps-3, -1, -1):\n",
    "            Aa = A[:, :, 2**k-1:L:2**k]\n",
    "            Xa = X[:, :, 2**k-1:L:2**k]\n",
    "\n",
    "            T = Xa.size(2)\n",
    "            Aa = Aa.view(B, D, T//2, 2, -1)\n",
    "            Xa = Xa.view(B, D, T//2, 2, -1)\n",
    "\n",
    "            Xa[:, :, 1:, 0].add_(Aa[:, :, 1:, 0].mul(Xa[:, :, :-1, 1]))\n",
    "            Aa[:, :, 1:, 0].mul_(Aa[:, :, :-1, 1])\n",
    "\n",
    "    @staticmethod\n",
    "    def pscan_rev(A, X):\n",
    "        # A : (B, D, L, N)\n",
    "        # X : (B, D, L, N)\n",
    "\n",
    "        # the same function as above, but in reverse\n",
    "        # (if you flip the input, call pscan, then flip the output, you get what this function outputs)\n",
    "        # it is used in the backward pass\n",
    "\n",
    "        # only supports L that is a power of two (mainly for a clearer code)\n",
    "\n",
    "        B, D, L, _ = A.size()\n",
    "        num_steps = int(math.log2(L))\n",
    "\n",
    "        # up sweep (last 2 steps unfolded)\n",
    "        Aa = A\n",
    "        Xa = X\n",
    "        for _ in range(num_steps-2):\n",
    "            T = Xa.size(2)\n",
    "            Aa = Aa.view(B, D, T//2, 2, -1)\n",
    "            Xa = Xa.view(B, D, T//2, 2, -1)\n",
    "                    \n",
    "            Xa[:, :, :, 0].add_(Aa[:, :, :, 0].mul(Xa[:, :, :, 1]))\n",
    "            Aa[:, :, :, 0].mul_(Aa[:, :, :, 1])\n",
    "\n",
    "            Aa = Aa[:, :, :, 0]\n",
    "            Xa = Xa[:, :, :, 0]\n",
    "\n",
    "        # we have only 4, 2 or 1 nodes left\n",
    "        if Xa.size(2) == 4:\n",
    "            Xa[:, :, 2].add_(Aa[:, :, 2].mul(Xa[:, :, 3]))\n",
    "            Aa[:, :, 2].mul_(Aa[:, :, 3])\n",
    "\n",
    "            Xa[:, :, 0].add_(Aa[:, :, 0].mul(Xa[:, :, 1].add(Aa[:, :, 1].mul(Xa[:, :, 2]))))\n",
    "        elif Xa.size(2) == 2:\n",
    "            Xa[:, :, 0].add_(Aa[:, :, 0].mul(Xa[:, :, 1]))\n",
    "            return\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        # down sweep (first 2 steps unfolded)\n",
    "        Aa = A[:, :, 0:L:2**(num_steps-2)]\n",
    "        Xa = X[:, :, 0:L:2**(num_steps-2)]\n",
    "        Xa[:, :, 1].add_(Aa[:, :, 1].mul(Xa[:, :, 2]))\n",
    "        Aa[:, :, 1].mul_(Aa[:, :, 2])\n",
    "\n",
    "        for k in range(num_steps-3, -1, -1):\n",
    "            Aa = A[:, :, 0:L:2**k]\n",
    "            Xa = X[:, :, 0:L:2**k]\n",
    "\n",
    "            T = Xa.size(2)\n",
    "            Aa = Aa.view(B, D, T//2, 2, -1)\n",
    "            Xa = Xa.view(B, D, T//2, 2, -1)\n",
    "\n",
    "            Xa[:, :, :-1, 1].add_(Aa[:, :, :-1, 1].mul(Xa[:, :, 1:, 0]))\n",
    "            Aa[:, :, :-1, 1].mul_(Aa[:, :, 1:, 0])\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, A_in, X_in):\n",
    "        \"\"\"\n",
    "        Applies the parallel scan operation, as defined above. Returns a new tensor.\n",
    "        If you can, privilege sequence lengths that are powers of two.\n",
    "\n",
    "        Args:\n",
    "            A_in : (B, L, D, N)\n",
    "            X_in : (B, L, D, N)\n",
    "\n",
    "        Returns:\n",
    "            H : (B, L, D, N)\n",
    "        \"\"\"\n",
    "\n",
    "        L = X_in.size(1)\n",
    "\n",
    "        # cloning is requiered because of the in-place ops\n",
    "        if L == npo2(L):\n",
    "            A = A_in.clone()\n",
    "            X = X_in.clone()\n",
    "        else:\n",
    "            # pad tensors (and clone btw)\n",
    "            A = pad_npo2(A_in) # (B, npo2(L), D, N)\n",
    "            X = pad_npo2(X_in) # (B, npo2(L), D, N)\n",
    "        \n",
    "        # prepare tensors\n",
    "        A = A.transpose(2, 1) # (B, D, npo2(L), N)\n",
    "        X = X.transpose(2, 1) # (B, D, npo2(L), N)\n",
    "\n",
    "        # parallel scan (modifies X in-place)\n",
    "        PScan.pscan(A, X)\n",
    "\n",
    "        ctx.save_for_backward(A_in, X)\n",
    "        \n",
    "        # slice [:, :L] (cut if there was padding)\n",
    "        return X.transpose(2, 1)[:, :L]\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_in):\n",
    "        \"\"\"\n",
    "        Flows the gradient from the output to the input. Returns two new tensors.\n",
    "\n",
    "        Args:\n",
    "            ctx : A_in : (B, L, D, N), X : (B, D, L, N)\n",
    "            grad_output_in : (B, L, D, N)\n",
    "\n",
    "        Returns:\n",
    "            gradA : (B, L, D, N), gradX : (B, L, D, N)\n",
    "        \"\"\"\n",
    "\n",
    "        A_in, X = ctx.saved_tensors\n",
    "\n",
    "        L = grad_output_in.size(1)\n",
    "\n",
    "        # cloning is requiered because of the in-place ops\n",
    "        if L == npo2(L):\n",
    "            grad_output = grad_output_in.clone()\n",
    "            # the next padding will clone A_in\n",
    "        else:\n",
    "            grad_output = pad_npo2(grad_output_in) # (B, npo2(L), D, N)\n",
    "            A_in = pad_npo2(A_in) # (B, npo2(L), D, N)\n",
    "\n",
    "        # prepare tensors\n",
    "        grad_output = grad_output.transpose(2, 1)\n",
    "        A_in = A_in.transpose(2, 1) # (B, D, npo2(L), N)\n",
    "        A = torch.nn.functional.pad(A_in[:, :, 1:], (0, 0, 0, 1)) # (B, D, npo2(L), N) shift 1 to the left (see hand derivation)\n",
    "\n",
    "        # reverse parallel scan (modifies grad_output in-place)\n",
    "        PScan.pscan_rev(A, grad_output)\n",
    "\n",
    "        Q = torch.zeros_like(X)\n",
    "        Q[:, :, 1:].add_(X[:, :, :-1] * grad_output[:, :, 1:])\n",
    "\n",
    "        return Q.transpose(2, 1)[:, :L], grad_output.transpose(2, 1)[:, :L]\n",
    "    \n",
    "pscan = PScan.apply\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "This file closely follows the mamba_simple.py from the official Mamba implementation, and the mamba-minimal by @johnma2006.\n",
    "The major differences are :\n",
    "-the convolution is done with torch.nn.Conv1d\n",
    "-the selective scan is done in PyTorch\n",
    "\n",
    "A sequential version of the selective scan is also available for comparison.\n",
    "\n",
    "- A Mamba model is composed of several layers, which are ResidualBlock.\n",
    "- A ResidualBlock is composed of a MambaBlock, a normalization, and a residual connection : ResidualBlock(x) = mamba(norm(x)) + x\n",
    "- This leaves us with the MambaBlock : its input x is (B, L, D) and its outputs y is also (B, L, D) (B=batch size, L=seq len, D=model dim).\n",
    "First, we expand x into (B, L, 2*ED) (where E is usually 2) and split it into x and z, each (B, L, ED).\n",
    "Then, we apply the short 1d conv to x, followed by an activation function (silu), then the SSM.\n",
    "We then multiply it by silu(z).\n",
    "See Figure 3 of the paper (page 8) for a visual representation of a MambaBlock.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class MambaConfig:\n",
    "    d_model: int # D\n",
    "    n_layers: int\n",
    "    dt_rank: Union[int, str] = 'auto'\n",
    "    d_state: int = 16 # N in paper/comments\n",
    "    expand_factor: int = 2 # E in paper/comments\n",
    "    d_conv: int = 4\n",
    "\n",
    "    dt_min: float = 0.001\n",
    "    dt_max: float = 0.1\n",
    "    dt_init: str = \"random\" # \"random\" or \"constant\"\n",
    "    dt_scale: float = 1.0\n",
    "    dt_init_floor = 1e-4\n",
    "\n",
    "    bias: bool = False\n",
    "    conv_bias: bool = True\n",
    "\n",
    "    pscan: bool = True # use parallel scan mode or sequential mode when training\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.d_inner = self.expand_factor * self.d_model # E*D = ED in comments\n",
    "\n",
    "        if self.dt_rank == 'auto':\n",
    "            self.dt_rank = math.ceil(self.d_model / 16)\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    def __init__(self, config: MambaConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.layers = nn.ModuleList([ResidualBlock(config) for _ in range(config.n_layers)])\n",
    "        self.norm_f = RMSNorm(config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : (B, L, D)\n",
    "\n",
    "        # y : (B, L, D)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm_f(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def step(self, x, caches):\n",
    "        # x : (B, L, D)\n",
    "        # caches : [cache(layer) for all layers], cache : (h, inputs)\n",
    "\n",
    "        # y : (B, L, D)\n",
    "        # caches : [cache(layer) for all layers], cache : (h, inputs)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x, caches[i] = layer.step(x, caches[i])\n",
    "\n",
    "        return x, caches\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, config: MambaConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mixer = MambaBlock(config)\n",
    "        self.norm = RMSNorm(config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : (B, L, D)\n",
    "\n",
    "        # output : (B, L, D)\n",
    "\n",
    "        output = self.mixer(self.norm(x)) + x\n",
    "        return output\n",
    "    \n",
    "    def step(self, x, cache):\n",
    "        # x : (B, D)\n",
    "        # cache : (h, inputs)\n",
    "                # h : (B, ED, N)\n",
    "                # inputs: (B, ED, d_conv-1)\n",
    "\n",
    "        # output : (B, D)\n",
    "        # cache : (h, inputs)\n",
    "\n",
    "        output, cache = self.mixer.step(self.norm(x), cache)\n",
    "        output = output + x\n",
    "        return output, cache\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, config: MambaConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # projects block input from D to 2*ED (two branches)\n",
    "        self.in_proj = nn.Linear(config.d_model, 2 * config.d_inner, bias=config.bias)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=config.d_inner, out_channels=config.d_inner, \n",
    "                              kernel_size=config.d_conv, bias=config.conv_bias, \n",
    "                              groups=config.d_inner,\n",
    "                              padding=config.d_conv - 1)\n",
    "        \n",
    "        # projects x to input-dependent Δ, B, C\n",
    "        self.x_proj = nn.Linear(config.d_inner, config.dt_rank + 2 * config.d_state, bias=False)\n",
    "\n",
    "        # projects Δ from dt_rank to d_inner\n",
    "        self.dt_proj = nn.Linear(config.dt_rank, config.d_inner, bias=True)\n",
    "\n",
    "        # dt initialization\n",
    "        # dt weights\n",
    "        dt_init_std = config.dt_rank**-0.5 * config.dt_scale\n",
    "        if config.dt_init == \"constant\":\n",
    "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
    "        elif config.dt_init == \"random\":\n",
    "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        # dt bias\n",
    "        dt = torch.exp(\n",
    "            torch.rand(config.d_inner) * (math.log(config.dt_max) - math.log(config.dt_min)) + math.log(config.dt_min)\n",
    "        ).clamp(min=config.dt_init_floor)\n",
    "        inv_dt = dt + torch.log(-torch.expm1(-dt)) # inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "        with torch.no_grad():\n",
    "            self.dt_proj.bias.copy_(inv_dt)\n",
    "        #self.dt_proj.bias._no_reinit = True # initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
    "        # todo : explain why removed\n",
    "\n",
    "        # S4D real initialization\n",
    "        A = torch.arange(1, config.d_state + 1, dtype=torch.float32).repeat(config.d_inner, 1)\n",
    "        self.A_log = nn.Parameter(torch.log(A)) # why store A in log ? to keep A < 0 (cf -torch.exp(...)) ? for gradient stability ?\n",
    "        self.D = nn.Parameter(torch.ones(config.d_inner))\n",
    "\n",
    "        # projects block output from ED back to D\n",
    "        self.out_proj = nn.Linear(config.d_inner, config.d_model, bias=config.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : (B, L, D)\n",
    "        \n",
    "        # y : (B, L, D)\n",
    "\n",
    "        _, L, _ = x.shape\n",
    "\n",
    "        xz = self.in_proj(x) # (B, L, 2*ED)\n",
    "        x, z = xz.chunk(2, dim=-1) # (B, L, ED), (B, L, ED)\n",
    "\n",
    "        # x branch\n",
    "        x = x.transpose(1, 2) # (B, ED, L)\n",
    "        x = self.conv1d(x)[:, :, :L] # depthwise convolution over time, with a short filter\n",
    "        x = x.transpose(1, 2) # (B, L, ED)\n",
    "\n",
    "        x = F.silu(x)\n",
    "        y = self.ssm(x)\n",
    "\n",
    "        # z branch\n",
    "        z = F.silu(z)\n",
    "\n",
    "        output = y * z\n",
    "        output = self.out_proj(output) # (B, L, D)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def ssm(self, x):\n",
    "        # x : (B, L, ED)\n",
    "\n",
    "        # y : (B, L, ED)\n",
    "\n",
    "        A = -torch.exp(self.A_log.float()) # (ED, N)\n",
    "        D = self.D.float()\n",
    "        # TODO remove .float()\n",
    "\n",
    "        deltaBC = self.x_proj(x) # (B, L, dt_rank+2*N)\n",
    "\n",
    "        delta, B, C = torch.split(deltaBC, [self.config.dt_rank, self.config.d_state, self.config.d_state], dim=-1) # (B, L, dt_rank), (B, L, N), (B, L, N)\n",
    "        delta = F.softplus(self.dt_proj(delta)) # (B, L, ED)\n",
    "\n",
    "        if self.config.pscan:\n",
    "            y = self.selective_scan(x, delta, A, B, C, D)\n",
    "        else:\n",
    "            y = self.selective_scan_seq(x, delta, A, B, C, D)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def selective_scan(self, x, delta, A, B, C, D):\n",
    "        # x : (B, L, ED)\n",
    "        # Δ : (B, L, ED)\n",
    "        # A : (ED, N)\n",
    "        # B : (B, L, N)\n",
    "        # C : (B, L, N)\n",
    "        # D : (ED)\n",
    "\n",
    "        # y : (B, L, ED)\n",
    "\n",
    "        deltaA = torch.exp(delta.unsqueeze(-1) * A) # (B, L, ED, N)\n",
    "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2) # (B, L, ED, N)\n",
    "\n",
    "        BX = deltaB * (x.unsqueeze(-1)) # (B, L, ED, N)\n",
    "        \n",
    "        hs = pscan(deltaA, BX)\n",
    "\n",
    "        y = (hs @ C.unsqueeze(-1)).squeeze(3) # (B, L, ED, N) @ (B, L, N, 1) -> (B, L, ED, 1)\n",
    "\n",
    "        y = y + D * x\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def selective_scan_seq(self, x, delta, A, B, C, D):\n",
    "        # x : (B, L, ED)\n",
    "        # Δ : (B, L, ED)\n",
    "        # A : (ED, N)\n",
    "        # B : (B, L, N)\n",
    "        # C : (B, L, N)\n",
    "        # D : (ED)\n",
    "\n",
    "        # y : (B, L, ED)\n",
    "\n",
    "        _, L, _ = x.shape\n",
    "\n",
    "        deltaA = torch.exp(delta.unsqueeze(-1) * A) # (B, L, ED, N)\n",
    "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2) # (B, L, ED, N)\n",
    "\n",
    "        BX = deltaB * (x.unsqueeze(-1)) # (B, L, ED, N)\n",
    "\n",
    "        h = torch.zeros(x.size(0), self.config.d_inner, self.config.d_state, device=deltaA.device) # (B, ED, N)\n",
    "        hs = []\n",
    "\n",
    "        for t in range(0, L):\n",
    "            h = deltaA[:, t] * h + BX[:, t]\n",
    "            hs.append(h)\n",
    "            \n",
    "        hs = torch.stack(hs, dim=1) # (B, L, ED, N)\n",
    "\n",
    "        y = (hs @ C.unsqueeze(-1)).squeeze(3) # (B, L, ED, N) @ (B, L, N, 1) -> (B, L, ED, 1)\n",
    "\n",
    "        y = y + D * x\n",
    "\n",
    "        return y\n",
    "    \n",
    "    # -------------------------- inference -------------------------- #\n",
    "    \"\"\"\n",
    "    Concerning auto-regressive inference\n",
    "\n",
    "    The cool part of using Mamba : inference is constant wrt to sequence length\n",
    "    We just have to keep in cache, for each layer, two things :\n",
    "    - the hidden state h (which is (B, ED, N)), as you typically would when doing inference with a RNN\n",
    "    - the last d_conv-1 inputs of the layer, to be able to compute the 1D conv which is a convolution over the time dimension\n",
    "      (d_conv is fixed so this doesn't incur a growing cache as we progress on generating the sequence)\n",
    "      (and d_conv is usually very small, like 4, so we just have to \"remember\" the last 3 inputs)\n",
    "\n",
    "    Concretely, these two quantities are put inside a cache tuple, and are named h and inputs respectively.\n",
    "    h is (B, ED, N), and inputs is (B, ED, d_conv-1)\n",
    "    The MambaBlock.step() receives this cache, and, along with outputing the output, alos outputs the updated cache for the next call.\n",
    "\n",
    "    The cache object is initialized as follows : (None, torch.zeros()).\n",
    "    When h is None, the selective scan function detects it and start with h=0.\n",
    "    The torch.zeros() isn't a problem (it's same as just feeding the input, because the conv1d is padded)\n",
    "\n",
    "    As we need one such cache variable per layer, we store a caches object, which is simply a list of cache object. (See mamba_lm.py)\n",
    "    \"\"\"\n",
    "    \n",
    "    def step(self, x, cache):\n",
    "        # x : (B, D)\n",
    "        # cache : (h, inputs)\n",
    "                # h : (B, ED, N)\n",
    "                # inputs : (B, ED, d_conv-1)\n",
    "        \n",
    "        # y : (B, D)\n",
    "        # cache : (h, inputs)\n",
    "        \n",
    "        h, inputs = cache\n",
    "        \n",
    "        xz = self.in_proj(x) # (B, 2*ED)\n",
    "        x, z = xz.chunk(2, dim=1) # (B, ED), (B, ED)\n",
    "\n",
    "        # x branch\n",
    "        x_cache = x.unsqueeze(2)\n",
    "        x = self.conv1d(torch.cat([inputs, x_cache], dim=2))[:, :, self.config.d_conv-1] # (B, ED)\n",
    "\n",
    "        x = F.silu(x)\n",
    "        y, h = self.ssm_step(x, h)\n",
    "\n",
    "        # z branch\n",
    "        z = F.silu(z)\n",
    "\n",
    "        output = y * z\n",
    "        output = self.out_proj(output) # (B, D)\n",
    "\n",
    "        # prepare cache for next call\n",
    "        inputs = torch.cat([inputs[:, :, 1:], x_cache], dim=2) # (B, ED, d_conv-1)\n",
    "        cache = (h, inputs)\n",
    "        \n",
    "        return output, cache\n",
    "\n",
    "    def ssm_step(self, x, h):\n",
    "        # x : (B, ED)\n",
    "        # h : (B, ED, N)\n",
    "\n",
    "        # y : (B, ED)\n",
    "        # h : (B, ED, N)\n",
    "\n",
    "        A = -torch.exp(self.A_log.float()) # (ED, N) # todo : ne pas le faire tout le temps, puisque c'est indépendant de la timestep\n",
    "        D = self.D.float()\n",
    "        # TODO remove .float()\n",
    "\n",
    "        deltaBC = self.x_proj(x) # (B, dt_rank+2*N)\n",
    "\n",
    "        delta, B, C = torch.split(deltaBC, [self.config.dt_rank, self.config.d_state, self.config.d_state], dim=-1) # (B, dt_rank), (B, N), (B, N)\n",
    "        delta = F.softplus(self.dt_proj(delta)) # (B, ED)\n",
    "\n",
    "        deltaA = torch.exp(delta.unsqueeze(-1) * A) # (B, ED, N)\n",
    "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(1) # (B, ED, N)\n",
    "\n",
    "        BX = deltaB * (x.unsqueeze(-1)) # (B, ED, N)\n",
    "\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.size(0), self.config.d_inner, self.config.d_state, device=deltaA.device) # (B, ED, N)\n",
    "\n",
    "        h = deltaA * h + BX # (B, ED, N)\n",
    "\n",
    "        y = (h @ C.unsqueeze(-1)).squeeze(2) # (B, ED, N) @ (B, N, 1) -> (B, ED, 1)\n",
    "\n",
    "        y = y + D * x\n",
    "\n",
    "        # todo : pq h.squeeze(1) ??\n",
    "        return y, h.squeeze(1)\n",
    "\n",
    "# taken straight from https://github.com/johnma2006/mamba-minimal/blob/master/model.py\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "An implementation of the parallel scan operation in PyTorch (Blelloch version).\n",
    "Please see docs/pscan.ipynb for a detailed explanation of what happens here.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def npo2(len):\n",
    "    \"\"\"\n",
    "    Returns the next power of 2 above len\n",
    "    \"\"\"\n",
    "\n",
    "    return 2 ** math.ceil(math.log2(len))\n",
    "\n",
    "def pad_npo2(X):\n",
    "    \"\"\"\n",
    "    Pads input length dim to the next power of 2\n",
    "\n",
    "    Args:\n",
    "        X : (B, L, D, N)\n",
    "\n",
    "    Returns:\n",
    "        Y : (B, npo2(L), D, N)\n",
    "    \"\"\"\n",
    "\n",
    "    len_npo2 = npo2(X.size(1))\n",
    "    pad_tuple = (0, 0, 0, 0, 0, len_npo2 - X.size(1))\n",
    "    return F.pad(X, pad_tuple, \"constant\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from mamba import Mamba, MambaConfig\n",
    "import argparse\n",
    "seed = 1\n",
    "epochs = 100\n",
    "lr = 0.0001\n",
    "wd = 1e-5\n",
    "hidden = 16\n",
    "layer = 2\n",
    "n_test = 300\n",
    "ts_code = 601988\n",
    "use_cuda = False\n",
    "cuda = use_cuda and torch.cuda.is_available()\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--use-cuda', default=False,\n",
    "#                     help='CUDA training.')\n",
    "# parser.add_argument('--seed', type=int, default=1, help='Random seed.')\n",
    "# parser.add_argument('--epochs', type=int, default=100,\n",
    "#                     help='Number of epochs to train.')\n",
    "# parser.add_argument('--lr', type=float, default=0.01,\n",
    "#                     help='Learning rate.')\n",
    "# parser.add_argument('--wd', type=float, default=1e-5,\n",
    "#                     help='Weight decay (L2 loss on parameters).')\n",
    "# parser.add_argument('--hidden', type=int, default=16,\n",
    "#                     help='Dimension of representations')\n",
    "# parser.add_argument('--layer', type=int, default=2,\n",
    "#                     help='Num of layers')\n",
    "# parser.add_argument('--n-test', type=int, default=300,\n",
    "#                     help='Size of test set')\n",
    "# parser.add_argument('--ts-code', type=str, default='601988',\n",
    "#                     help='Stock code')                    \n",
    "\n",
    "# args = parser.parse_args()\n",
    "# cuda = use_cuda and torch.cuda.is_available()\n",
    "\n",
    "def evaluation_metric(y_test,y_hat):\n",
    "    MSE = mean_squared_error(y_test, y_hat)\n",
    "    RMSE = MSE**0.5\n",
    "    MAE = mean_absolute_error(y_test,y_hat)\n",
    "    R2 = r2_score(y_test,y_hat)\n",
    "    \n",
    "\n",
    "    print('%.4f %.4f %.4f %.4f' % (MSE,RMSE,MAE,R2))\n",
    "\n",
    "def set_seed(seed,cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "def dateinf(series, n_test):\n",
    "    lt = len(series)\n",
    "    print('Training start',series[0])\n",
    "    print('Training end',series[lt-n_test-1])\n",
    "    print('Testing start',series[lt-n_test])\n",
    "    print('Testing end',series[lt-1])\n",
    "\n",
    "set_seed(seed,cuda)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim):\n",
    "        super().__init__()\n",
    "        self.config = MambaConfig(d_model=hidden, n_layers=layer)\n",
    "        self.mamba = nn.Sequential(\n",
    "            nn.Linear(in_dim,hidden),\n",
    "            Mamba(self.config),\n",
    "            nn.Linear(hidden,out_dim),\n",
    "            nn.Tanh()\n",
    "            \n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.mamba(x)\n",
    "        return x.flatten()\n",
    "\n",
    "def PredictWithData(trainX, trainy, testX):\n",
    "    clf = Net(len(trainX[0]),1)\n",
    "    opt = torch.optim.Adam(clf.parameters(),lr=lr,weight_decay=wd)\n",
    "    xt = torch.from_numpy(trainX).float().unsqueeze(0)\n",
    "    xv = torch.from_numpy(testX).float().unsqueeze(0)\n",
    "    yt = torch.from_numpy(trainy).float()\n",
    "    if cuda:\n",
    "        clf = clf.cuda()\n",
    "        xt = xt.cuda()\n",
    "        xv = xv.cuda()\n",
    "        yt = yt.cuda()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        clf.train()\n",
    "        z = clf(xt)\n",
    "        loss = F.mse_loss(z,yt)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if e%10 == 0 and e!=0:\n",
    "            print('Epoch %d | Lossp: %.4f' % (e, loss.item()))\n",
    "\n",
    "    clf.eval()\n",
    "    mat = clf(xv)\n",
    "    if cuda: mat = mat.cpu()\n",
    "    yhat = mat.detach().numpy().flatten()\n",
    "    return yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-05-01'\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1980-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['Percentage Change'] = data['Close'].pct_change()\n",
    "    data['Percentage Change'] = data['Percentage Change'].shift(-1)\n",
    "    # data['Sentiment'] = pd.Series(np.where(data['Percentage Change'] > 0.025, 1, np.where(data['Percentage Change'] < -0.025, -1, 0)), index=data.index)\n",
    "    # data['perc_change'] = data['Percentage Change']\n",
    "    # # Drop any rows with missing values\n",
    "    # data.dropna(inplace=True)\n",
    "    # data.drop('Percentage Change',axis=1 , inplace=True)\n",
    "    return data\n",
    "\n",
    "#fetching data \n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "# Calculate deltas for open, high, low, and close columns\n",
    "for i in range(1, 30):  # Calculate deltas up to 5 days\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "        # Rolling mean and standard deviation of OHLC prices\n",
    "    stock['Rolling_Mean_Open_{i}day'] = stock['Open'].rolling(window=i).mean()\n",
    "    stock['Rolling_Mean_High_{i}day'] = stock['High'].rolling(window=i).mean()\n",
    "    stock['Rolling_Mean_Low_{i}day'] = stock['Low'].rolling(window=i).mean()\n",
    "    stock['Rolling_Mean_Close_{i}day'] = stock['Close'].rolling(window=i).mean()\n",
    "\n",
    "    stock['Rolling_Std_Open_{i}day'] = stock['Open'].rolling(window=i).std()\n",
    "    stock['Rolling_Std_High_{i}day'] = stock['High'].rolling(window=i).std()\n",
    "    stock['Rolling_Std_Low_{i}day'] = stock['Low'].rolling(window=i).std()\n",
    "    stock['Rolling_Std_Close_{i}day'] = stock['Close'].rolling(window=i).std()\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "# stock.index = stock.index.date\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:train_data_index]\n",
    "test_data = stock.loc[start_date:]\n",
    "train_data = label_data(train_data)\n",
    "test_data = label_data(test_data)\n",
    "train_data.fillna(method='ffill',axis = 0, inplace=True)\n",
    "test_data.fillna(method='ffill',axis = 0, inplace=True)\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data))\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # # Reshape X_train_data_normalizer\n",
    "# X_train = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "# X_test = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05217053, -0.07339807,  0.024751  , ...,  0.0059353 ,\n",
       "        0.00730624,  0.00730624])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainy\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainy' is not defined"
     ]
    }
   ],
   "source": [
    "trainy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Lossp: 0.0199\n",
      "Epoch 20 | Lossp: 0.0033\n",
      "Epoch 30 | Lossp: 0.0009\n",
      "Epoch 40 | Lossp: 0.0008\n",
      "Epoch 50 | Lossp: 0.0008\n",
      "Epoch 60 | Lossp: 0.0008\n",
      "Epoch 70 | Lossp: 0.0008\n",
      "Epoch 80 | Lossp: 0.0008\n",
      "Epoch 90 | Lossp: 0.0008\n"
     ]
    }
   ],
   "source": [
    "# predictions = PredictWithData(trainX, trainy, testX)\n",
    "predictions = PredictWithData(X_train_data_normalizer, y_train_data.values, X_test_data_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Sentiment'] = pd.Series(np.where(test_data['Percentage Change'] > 0, 1, -1), index=test_data.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['transformer_sentiment'] = predictions\n",
    "test_data['transformer_sentiment'] = test_data['transformer_sentiment'].apply(lambda x: -1 if x< 0 else 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>open_delta_1day</th>\n",
       "      <th>high_delta_1day</th>\n",
       "      <th>low_delta_1day</th>\n",
       "      <th>...</th>\n",
       "      <th>high_delta_28day</th>\n",
       "      <th>low_delta_28day</th>\n",
       "      <th>close_delta_28day</th>\n",
       "      <th>open_delta_29day</th>\n",
       "      <th>high_delta_29day</th>\n",
       "      <th>low_delta_29day</th>\n",
       "      <th>close_delta_29day</th>\n",
       "      <th>Percentage Change</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>transformer_sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-02 00:00:00-05:00</th>\n",
       "      <td>71.962075</td>\n",
       "      <td>73.021202</td>\n",
       "      <td>71.707013</td>\n",
       "      <td>72.960472</td>\n",
       "      <td>135480400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.532829</td>\n",
       "      <td>1.681013</td>\n",
       "      <td>1.377365</td>\n",
       "      <td>...</td>\n",
       "      <td>8.385544</td>\n",
       "      <td>8.451129</td>\n",
       "      <td>9.026844</td>\n",
       "      <td>6.884296</td>\n",
       "      <td>7.919130</td>\n",
       "      <td>7.238955</td>\n",
       "      <td>8.273788</td>\n",
       "      <td>-0.009722</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03 00:00:00-05:00</th>\n",
       "      <td>72.183128</td>\n",
       "      <td>73.016335</td>\n",
       "      <td>72.025232</td>\n",
       "      <td>72.251144</td>\n",
       "      <td>146322800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>-0.004867</td>\n",
       "      <td>0.318219</td>\n",
       "      <td>...</td>\n",
       "      <td>8.883519</td>\n",
       "      <td>8.579878</td>\n",
       "      <td>8.604164</td>\n",
       "      <td>7.678640</td>\n",
       "      <td>8.380677</td>\n",
       "      <td>8.769348</td>\n",
       "      <td>8.317516</td>\n",
       "      <td>0.007968</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-06 00:00:00-05:00</th>\n",
       "      <td>71.366911</td>\n",
       "      <td>72.865711</td>\n",
       "      <td>71.114274</td>\n",
       "      <td>72.826843</td>\n",
       "      <td>118387200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.816217</td>\n",
       "      <td>-0.150624</td>\n",
       "      <td>-0.910959</td>\n",
       "      <td>...</td>\n",
       "      <td>8.934521</td>\n",
       "      <td>7.751511</td>\n",
       "      <td>9.235737</td>\n",
       "      <td>7.311830</td>\n",
       "      <td>8.732895</td>\n",
       "      <td>7.668920</td>\n",
       "      <td>9.179863</td>\n",
       "      <td>-0.004703</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07 00:00:00-05:00</th>\n",
       "      <td>72.836571</td>\n",
       "      <td>73.094064</td>\n",
       "      <td>72.263288</td>\n",
       "      <td>72.484344</td>\n",
       "      <td>108872000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.469661</td>\n",
       "      <td>0.228353</td>\n",
       "      <td>1.149014</td>\n",
       "      <td>...</td>\n",
       "      <td>8.370950</td>\n",
       "      <td>8.492416</td>\n",
       "      <td>7.778236</td>\n",
       "      <td>9.048702</td>\n",
       "      <td>9.162874</td>\n",
       "      <td>8.900525</td>\n",
       "      <td>8.893238</td>\n",
       "      <td>0.016086</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08 00:00:00-05:00</th>\n",
       "      <td>72.185556</td>\n",
       "      <td>73.954000</td>\n",
       "      <td>72.185556</td>\n",
       "      <td>73.650352</td>\n",
       "      <td>132079200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.651015</td>\n",
       "      <td>0.859936</td>\n",
       "      <td>-0.077732</td>\n",
       "      <td>...</td>\n",
       "      <td>9.055998</td>\n",
       "      <td>8.419553</td>\n",
       "      <td>9.449524</td>\n",
       "      <td>8.368530</td>\n",
       "      <td>9.230886</td>\n",
       "      <td>8.414685</td>\n",
       "      <td>8.944244</td>\n",
       "      <td>0.021241</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-24 00:00:00-04:00</th>\n",
       "      <td>166.314410</td>\n",
       "      <td>169.070681</td>\n",
       "      <td>165.984870</td>\n",
       "      <td>168.791061</td>\n",
       "      <td>48251800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.188378</td>\n",
       "      <td>2.246955</td>\n",
       "      <td>1.288264</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.003213</td>\n",
       "      <td>-5.832091</td>\n",
       "      <td>-3.974609</td>\n",
       "      <td>-6.221577</td>\n",
       "      <td>-3.884735</td>\n",
       "      <td>-4.543829</td>\n",
       "      <td>-2.107147</td>\n",
       "      <td>0.005147</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-25 00:00:00-04:00</th>\n",
       "      <td>169.300369</td>\n",
       "      <td>170.378908</td>\n",
       "      <td>167.922233</td>\n",
       "      <td>169.659882</td>\n",
       "      <td>50558300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.985959</td>\n",
       "      <td>1.308227</td>\n",
       "      <td>1.937363</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.007277</td>\n",
       "      <td>-2.137106</td>\n",
       "      <td>-2.726303</td>\n",
       "      <td>-3.375428</td>\n",
       "      <td>-3.694986</td>\n",
       "      <td>-3.894728</td>\n",
       "      <td>-3.105789</td>\n",
       "      <td>-0.003473</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-26 00:00:00-04:00</th>\n",
       "      <td>169.649895</td>\n",
       "      <td>171.107909</td>\n",
       "      <td>168.950831</td>\n",
       "      <td>169.070679</td>\n",
       "      <td>44838400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.349526</td>\n",
       "      <td>0.729001</td>\n",
       "      <td>1.028598</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.361388</td>\n",
       "      <td>-4.334138</td>\n",
       "      <td>-4.414017</td>\n",
       "      <td>-1.288257</td>\n",
       "      <td>-1.278276</td>\n",
       "      <td>-1.108508</td>\n",
       "      <td>-3.315506</td>\n",
       "      <td>0.024808</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-29 00:00:00-04:00</th>\n",
       "      <td>173.135155</td>\n",
       "      <td>175.791556</td>\n",
       "      <td>172.865532</td>\n",
       "      <td>173.264984</td>\n",
       "      <td>68169400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.485260</td>\n",
       "      <td>4.683647</td>\n",
       "      <td>3.914701</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.579217</td>\n",
       "      <td>0.069912</td>\n",
       "      <td>-2.576508</td>\n",
       "      <td>-2.197040</td>\n",
       "      <td>-1.677741</td>\n",
       "      <td>-0.419437</td>\n",
       "      <td>-0.219711</td>\n",
       "      <td>-0.018271</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-30 00:00:00-04:00</th>\n",
       "      <td>173.095225</td>\n",
       "      <td>174.752981</td>\n",
       "      <td>169.769734</td>\n",
       "      <td>170.099289</td>\n",
       "      <td>65934800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.039930</td>\n",
       "      <td>-1.038575</td>\n",
       "      <td>-3.095798</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.675013</td>\n",
       "      <td>-5.083107</td>\n",
       "      <td>-8.328705</td>\n",
       "      <td>-1.008618</td>\n",
       "      <td>-1.617792</td>\n",
       "      <td>-3.025886</td>\n",
       "      <td>-5.742203</td>\n",
       "      <td>-0.018271</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1089 rows × 134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Open        High         Low       Close  \\\n",
       "Date                                                                        \n",
       "2020-01-02 00:00:00-05:00   71.962075   73.021202   71.707013   72.960472   \n",
       "2020-01-03 00:00:00-05:00   72.183128   73.016335   72.025232   72.251144   \n",
       "2020-01-06 00:00:00-05:00   71.366911   72.865711   71.114274   72.826843   \n",
       "2020-01-07 00:00:00-05:00   72.836571   73.094064   72.263288   72.484344   \n",
       "2020-01-08 00:00:00-05:00   72.185556   73.954000   72.185556   73.650352   \n",
       "...                               ...         ...         ...         ...   \n",
       "2024-04-24 00:00:00-04:00  166.314410  169.070681  165.984870  168.791061   \n",
       "2024-04-25 00:00:00-04:00  169.300369  170.378908  167.922233  169.659882   \n",
       "2024-04-26 00:00:00-04:00  169.649895  171.107909  168.950831  169.070679   \n",
       "2024-04-29 00:00:00-04:00  173.135155  175.791556  172.865532  173.264984   \n",
       "2024-04-30 00:00:00-04:00  173.095225  174.752981  169.769734  170.099289   \n",
       "\n",
       "                              Volume  Dividends  Stock Splits  \\\n",
       "Date                                                            \n",
       "2020-01-02 00:00:00-05:00  135480400        0.0           0.0   \n",
       "2020-01-03 00:00:00-05:00  146322800        0.0           0.0   \n",
       "2020-01-06 00:00:00-05:00  118387200        0.0           0.0   \n",
       "2020-01-07 00:00:00-05:00  108872000        0.0           0.0   \n",
       "2020-01-08 00:00:00-05:00  132079200        0.0           0.0   \n",
       "...                              ...        ...           ...   \n",
       "2024-04-24 00:00:00-04:00   48251800        0.0           0.0   \n",
       "2024-04-25 00:00:00-04:00   50558300        0.0           0.0   \n",
       "2024-04-26 00:00:00-04:00   44838400        0.0           0.0   \n",
       "2024-04-29 00:00:00-04:00   68169400        0.0           0.0   \n",
       "2024-04-30 00:00:00-04:00   65934800        0.0           0.0   \n",
       "\n",
       "                           open_delta_1day  high_delta_1day  low_delta_1day  \\\n",
       "Date                                                                          \n",
       "2020-01-02 00:00:00-05:00         1.532829         1.681013        1.377365   \n",
       "2020-01-03 00:00:00-05:00         0.221053        -0.004867        0.318219   \n",
       "2020-01-06 00:00:00-05:00        -0.816217        -0.150624       -0.910959   \n",
       "2020-01-07 00:00:00-05:00         1.469661         0.228353        1.149014   \n",
       "2020-01-08 00:00:00-05:00        -0.651015         0.859936       -0.077732   \n",
       "...                                    ...              ...             ...   \n",
       "2024-04-24 00:00:00-04:00         1.188378         2.246955        1.288264   \n",
       "2024-04-25 00:00:00-04:00         2.985959         1.308227        1.937363   \n",
       "2024-04-26 00:00:00-04:00         0.349526         0.729001        1.028598   \n",
       "2024-04-29 00:00:00-04:00         3.485260         4.683647        3.914701   \n",
       "2024-04-30 00:00:00-04:00        -0.039930        -1.038575       -3.095798   \n",
       "\n",
       "                           ...  high_delta_28day  low_delta_28day  \\\n",
       "Date                       ...                                      \n",
       "2020-01-02 00:00:00-05:00  ...          8.385544         8.451129   \n",
       "2020-01-03 00:00:00-05:00  ...          8.883519         8.579878   \n",
       "2020-01-06 00:00:00-05:00  ...          8.934521         7.751511   \n",
       "2020-01-07 00:00:00-05:00  ...          8.370950         8.492416   \n",
       "2020-01-08 00:00:00-05:00  ...          9.055998         8.419553   \n",
       "...                        ...               ...              ...   \n",
       "2024-04-24 00:00:00-04:00  ...         -5.003213        -5.832091   \n",
       "2024-04-25 00:00:00-04:00  ...         -2.007277        -2.137106   \n",
       "2024-04-26 00:00:00-04:00  ...         -6.361388        -4.334138   \n",
       "2024-04-29 00:00:00-04:00  ...         -0.579217         0.069912   \n",
       "2024-04-30 00:00:00-04:00  ...         -3.675013        -5.083107   \n",
       "\n",
       "                           close_delta_28day  open_delta_29day  \\\n",
       "Date                                                             \n",
       "2020-01-02 00:00:00-05:00           9.026844          6.884296   \n",
       "2020-01-03 00:00:00-05:00           8.604164          7.678640   \n",
       "2020-01-06 00:00:00-05:00           9.235737          7.311830   \n",
       "2020-01-07 00:00:00-05:00           7.778236          9.048702   \n",
       "2020-01-08 00:00:00-05:00           9.449524          8.368530   \n",
       "...                                      ...               ...   \n",
       "2024-04-24 00:00:00-04:00          -3.974609         -6.221577   \n",
       "2024-04-25 00:00:00-04:00          -2.726303         -3.375428   \n",
       "2024-04-26 00:00:00-04:00          -4.414017         -1.288257   \n",
       "2024-04-29 00:00:00-04:00          -2.576508         -2.197040   \n",
       "2024-04-30 00:00:00-04:00          -8.328705         -1.008618   \n",
       "\n",
       "                           high_delta_29day  low_delta_29day  \\\n",
       "Date                                                           \n",
       "2020-01-02 00:00:00-05:00          7.919130         7.238955   \n",
       "2020-01-03 00:00:00-05:00          8.380677         8.769348   \n",
       "2020-01-06 00:00:00-05:00          8.732895         7.668920   \n",
       "2020-01-07 00:00:00-05:00          9.162874         8.900525   \n",
       "2020-01-08 00:00:00-05:00          9.230886         8.414685   \n",
       "...                                     ...              ...   \n",
       "2024-04-24 00:00:00-04:00         -3.884735        -4.543829   \n",
       "2024-04-25 00:00:00-04:00         -3.694986        -3.894728   \n",
       "2024-04-26 00:00:00-04:00         -1.278276        -1.108508   \n",
       "2024-04-29 00:00:00-04:00         -1.677741        -0.419437   \n",
       "2024-04-30 00:00:00-04:00         -1.617792        -3.025886   \n",
       "\n",
       "                           close_delta_29day  Percentage Change  Sentiment  \\\n",
       "Date                                                                         \n",
       "2020-01-02 00:00:00-05:00           8.273788          -0.009722         -1   \n",
       "2020-01-03 00:00:00-05:00           8.317516           0.007968          1   \n",
       "2020-01-06 00:00:00-05:00           9.179863          -0.004703         -1   \n",
       "2020-01-07 00:00:00-05:00           8.893238           0.016086          1   \n",
       "2020-01-08 00:00:00-05:00           8.944244           0.021241          1   \n",
       "...                                      ...                ...        ...   \n",
       "2024-04-24 00:00:00-04:00          -2.107147           0.005147          1   \n",
       "2024-04-25 00:00:00-04:00          -3.105789          -0.003473         -1   \n",
       "2024-04-26 00:00:00-04:00          -3.315506           0.024808          1   \n",
       "2024-04-29 00:00:00-04:00          -0.219711          -0.018271         -1   \n",
       "2024-04-30 00:00:00-04:00          -5.742203          -0.018271         -1   \n",
       "\n",
       "                           transformer_sentiment  \n",
       "Date                                              \n",
       "2020-01-02 00:00:00-05:00                     -1  \n",
       "2020-01-03 00:00:00-05:00                      1  \n",
       "2020-01-06 00:00:00-05:00                      1  \n",
       "2020-01-07 00:00:00-05:00                      1  \n",
       "2020-01-08 00:00:00-05:00                      1  \n",
       "...                                          ...  \n",
       "2024-04-24 00:00:00-04:00                     -1  \n",
       "2024-04-25 00:00:00-04:00                     -1  \n",
       "2024-04-26 00:00:00-04:00                     -1  \n",
       "2024-04-29 00:00:00-04:00                     -1  \n",
       "2024-04-30 00:00:00-04:00                     -1  \n",
       "\n",
       "[1089 rows x 134 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016 0.0404 0.0218 -2.8157\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluation_metric(test_data['Percentage Change'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.48      0.64      0.55       527\n",
      "           1       0.52      0.36      0.42       562\n",
      "\n",
      "    accuracy                           0.50      1089\n",
      "   macro avg       0.50      0.50      0.49      1089\n",
      "weighted avg       0.50      0.50      0.49      1089\n",
      "\n",
      "Accuracy: 0.50 , Precision : 0.52 , Recall : 0.36\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_data['Sentiment'],test_data['transformer_sentiment']))\n",
    "\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score\n",
    "\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = accuracy_score(test_data['Sentiment'],test_data['transformer_sentiment'])\n",
    "precision = precision_score(test_data['Sentiment'],test_data['transformer_sentiment'])\n",
    "recall = recall_score(test_data['Sentiment'],test_data['transformer_sentiment'])\n",
    "print(f\"Accuracy: {accuracy:.2f} , Precision : {precision:.2f} , Recall : {recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba Architecture + Technical Indicators + Feature Engineering + Sequencing of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119 239\n",
      "Epoch 10 | Lossp: 0.1654\n",
      "Epoch 20 | Lossp: 0.0518\n",
      "Epoch 30 | Lossp: 0.0274\n",
      "Epoch 40 | Lossp: 0.0188\n",
      "Epoch 50 | Lossp: 0.0148\n",
      "Epoch 60 | Lossp: 0.0123\n",
      "Epoch 70 | Lossp: 0.0106\n",
      "Epoch 80 | Lossp: 0.0093\n",
      "Epoch 90 | Lossp: 0.0083\n",
      "0.7508 0.8665 0.8657 -1792.7887\n",
      "None\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       539\n",
      "           1       0.52      1.00      0.68       580\n",
      "\n",
      "    accuracy                           0.52      1119\n",
      "   macro avg       0.26      0.50      0.34      1119\n",
      "weighted avg       0.27      0.52      0.35      1119\n",
      "\n",
      "Accuracy: 0.52 , Precision : 0.52 , Recall : 1.00\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-05-01'\n",
    "# Parameters\n",
    "batch_size = 256\n",
    "sequence_length = 30\n",
    "stride = 1\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1980-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['pr_change_on_last_day'] = data['Close'].pct_change()\n",
    "    data['pr_change_on_current_day'] = data['pr_change_on_last_day'].shift(-1)\n",
    "    data.iloc[0,-2] = 0\n",
    "    # data['sentiment'] = pd.Series(np.where(data['pr_change_on_current_day'] > 0, 1, np.where(data['pr_change_on_current_day'] < 0, -1, 0)), index=data.index)\n",
    "    # data['perc_change'] = data['Percentage Change']\n",
    "    # # Drop any rows with missing values\n",
    "    # data.dropna(inplace=True)\n",
    "    # data.drop('pr_change_on_current_day',axis=1 , inplace=True)\n",
    "    return data\n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# Calculate deltas, moving averages, and Bollinger Bands\n",
    "for i in range(1, 90,5):\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "    stock[f\"rolling_mean_open_{i}day\"] = stock[\"Open\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_high_{i}day\"] = stock[\"High\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_low_{i}day\"] = stock[\"Low\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_mean_close_{i}day\"] = stock[\"Close\"].rolling(window=i).mean()\n",
    "    stock[f\"rolling_std_open_{i}day\"] = stock[\"Open\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_high_{i}day\"] = stock[\"High\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_low_{i}day\"] = stock[\"Low\"].rolling(window=i).std()\n",
    "    stock[f\"rolling_std_close_{i}day\"] = stock[\"Close\"].rolling(window=i).std()\n",
    "\n",
    "stock['fast_ma'] = stock['Close'].rolling(window=20).mean()\n",
    "stock['slow_ma'] = stock['Close'].rolling(window=50).mean()\n",
    "stock['bollinger_high'] = stock['Close'].rolling(window=20).mean() + (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['bollinger_low'] = stock['Close'].rolling(window=20).mean() - (2 * stock['Close'].rolling(window=20).std())\n",
    "stock['ema'] = stock['Close'].ewm(span=20, adjust=False).mean()\n",
    "stock['envelope_high'] = stock['Close'].rolling(window=20).mean() * (1 + 0.05)\n",
    "stock['envelope_low'] = stock['Close'].rolling(window=20).mean() * (1 - 0.05)\n",
    "stock['macd_line'] = stock['Close'].ewm(span=12, adjust=False).mean() - stock['Close'].ewm(span=26, adjust=False).mean()\n",
    "stock['macd_signal'] = stock['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# RSI calculation\n",
    "def calculate_rsi(data, rsi_period):\n",
    "    delta = data['Close'].diff().dropna()\n",
    "    gain = delta.where(delta > 0, 0).dropna()\n",
    "    loss = -delta.where(delta < 0, 0).dropna()\n",
    "    avg_gain = gain.rolling(window=rsi_period).mean()\n",
    "    avg_loss = loss.rolling(window=rsi_period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "stock['rsi'] = calculate_rsi(stock, 14)\n",
    "\n",
    "# # Stochastic Oscillator calculation\n",
    "# def calculate_stochastic(data, k_window, d_window):\n",
    "#     high_low = data[['High', 'Low']]\n",
    "#     c = data['Close']\n",
    "#     highest = high_low.rolling(window=k_window).max()\n",
    "#     lowest = high_low.rolling(window=k_window).min()\n",
    "#     print(((c - lowest) / (highest - lowest)) * 100)\n",
    "#     stochastic_k = ((c - lowest) / (highest - lowest)) * 100\n",
    "#     stochastic_d = stochastic_k.rolling(window=d_window).mean()\n",
    "#     return stochastic_k, stochastic_d\n",
    "# stock['stochastic_k'], stock['stochastic_d'] = calculate_stochastic(stock, 14, 3)\n",
    "\n",
    "# stock['stochastic_k']= calculate_stochastic(stock, 14, 3)[0]\n",
    "# stock['stochastic_d']= calculate_stochastic(stock, 14, 3)[1]\n",
    "stock['day'] = pd.to_datetime(stock.index).day\n",
    "stock['month'] = pd.to_datetime(stock.index).month\n",
    "stock['year'] = pd.to_datetime(stock.index).year\n",
    "stock['weekday'] = pd.to_datetime(stock.index).weekday\n",
    "stock['dayofyear'] = pd.to_datetime(stock.index).dayofyear\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "stock.index = stock.index.date\n",
    "# Split the data into training and test sets\n",
    "\n",
    "# df = stock.copy()\n",
    "\n",
    "# # Calculate pairwise correlation\n",
    "# corr_matrix = df.corr()\n",
    "\n",
    "# # Identify highly correlated columns\n",
    "# redundant_cols = set()\n",
    "# for i in range(5,len(corr_matrix.columns)-1):\n",
    "#     for j in range(i+1, len(corr_matrix.columns)):\n",
    "#         if corr_matrix.iloc[i,j] > 0.8 and corr_matrix.columns[i] not in redundant_cols:\n",
    "#             redundant_cols.add(corr_matrix.columns[j])\n",
    "\n",
    "# # Remove one of the redundant columns\n",
    "# for col in redundant_cols:\n",
    "#     df = df.drop(col, axis=1)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df)\n",
    "\n",
    "# stock = df.copy()\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:int(0.9*train_data_index)].copy()\n",
    "val_data  = stock.iloc[int(0.9*train_data_index)-sequence_length:train_data_index].copy()\n",
    "test_data = stock.iloc[train_data_index-sequence_length:].copy()\n",
    "train_data = label_data(train_data)\n",
    "val_data = label_data(val_data)\n",
    "test_data = label_data(test_data)\n",
    "train_data.fillna(0,axis = 0, inplace=True)\n",
    "val_data.fillna(0,axis = 0, inplace=True)\n",
    "test_data.fillna(0,axis = 0, inplace=True)\n",
    "\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "#trian & test data\n",
    "X_val_data = val_data.iloc[:,:-1]\n",
    "y_val_data = val_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data), len(X_test_data.columns))\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert targets to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_data, num_classes=3)\n",
    "y_val_onehot = to_categorical(y_val_data, num_classes=3)\n",
    "\n",
    "y_train_data = to_categorical(y_train_data)\n",
    "y_test_data = to_categorical(y_test_data)\n",
    "y_val_data = to_categorical(y_val_data)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_val_data_normalizer = normalizer.fit_transform(X_val_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # # Reshape X_train_data_normalizer\n",
    "X_train_reshaped = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_val_reshaped = X_val_data_normalizer.reshape(X_val_data_normalizer.shape[0], X_val_data_normalizer.shape[1], 1)\n",
    "X_test_reshaped = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "\n",
    "def create_sequences(x,y,sequence_length,stride):\n",
    "    sequence_length  = sequence_length\n",
    "    X_test_data_normalizer_sequences = []\n",
    "    y_test_data_sequences = []\n",
    "    stride = stride\n",
    "    no_of_rows = len(x)\n",
    "    no_of_columns = len(x[0])\n",
    "    for i in range(sequence_length, no_of_rows-1 , stride):\n",
    "        X_test_data_normalizer_sequences.append(x[i-sequence_length: i])\n",
    "        y_test_data_sequences.append(y[i-1])\n",
    "    return np.array(X_test_data_normalizer_sequences),np.array(y_test_data_sequences)\n",
    "        \n",
    "X_train_data_normalizer_sequences,y_train_data_sequences = create_sequences(X_train_data_normalizer,y_train_data,sequence_length,stride)\n",
    "X_test_data_normalizer_sequences,y_test_data_sequences = create_sequences(X_test_data_normalizer,y_test_data,sequence_length,stride)\n",
    "train_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_train_data_normalizer,\n",
    "    y_train_data,\n",
    "    length = 3,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_val_data_normalizer,\n",
    "    y_val_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "    X_test_data_normalizer,\n",
    "    y_test_data,\n",
    "    length = sequence_length,\n",
    "    sampling_rate=1,\n",
    "    stride=1,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    shuffle=False,\n",
    "    reverse=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# predictions = PredictWithData(trainX, trainy, testX)\n",
    "predictions = PredictWithData(X_train_data_normalizer, y_train_data, X_test_data_normalizer)\n",
    "test_data['Sentiment'] = pd.Series(np.where(test_data['pr_change_on_current_day'] > 0, 1, -1), index=test_data.index)\n",
    "test_data['transformer_sentiment'] = predictions\n",
    "test_data['transformer_sentiment'] = test_data['transformer_sentiment'].apply(lambda x: -1 if x< 0 else 1)\n",
    "print(evaluation_metric(test_data['pr_change_on_current_day'], predictions))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_data['Sentiment'],test_data['transformer_sentiment']))\n",
    "\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score\n",
    "\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = accuracy_score(test_data['Sentiment'],test_data['transformer_sentiment'])\n",
    "precision = precision_score(test_data['Sentiment'],test_data['transformer_sentiment'])\n",
    "recall = recall_score(test_data['Sentiment'],test_data['transformer_sentiment'])\n",
    "print(f\"Accuracy: {accuracy:.2f} , Precision : {precision:.2f} , Recall : {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
