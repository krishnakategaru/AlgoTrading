{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define the GRU model\n",
    "def build_gru_model(input_shape, gru_units, mlp_units, dropout=0.2):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    # GRU layers (you can add more if needed)\n",
    "    x = tf.keras.layers.GRU(gru_units, return_sequences=True)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = tf.keras.layers.GRU(gru_units, return_sequences=True)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    # Additional GRU layer\n",
    "    x = tf.keras.layers.GRU(gru_units, return_sequences=True)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    # MLP layers\n",
    "    for units in mlp_units:\n",
    "        x = tf.keras.layers.Dense(units, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    # Output layer with 3 classes (positive, neutral, negative)\n",
    "    outputs = tf.keras.layers.Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Define the Transformer model\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = tf.keras.layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "# Build and train the model\n",
    "input_shape = (27,1)\n",
    "head_size = 46\n",
    "num_heads = 60\n",
    "ff_dim = 55\n",
    "num_transformer_blocks = 5\n",
    "mlp_units = [256]\n",
    "dropout = 0.14\n",
    "mlp_dropout = 0.4\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "\n",
    "    for _ in range(num_transformer_blocks):  # This is what stacks our transformer blocks\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "\n",
    "    for dim in mlp_units:\n",
    "        x = tf.keras.layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(mlp_dropout)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"softmax\")(x)  # this is a pass-through\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "def lr_scheduler(epoch, lr, warmup_epochs=30, decay_epochs=100, initial_lr=1e-6, base_lr=1e-3, min_lr=5e-5):\n",
    "    if epoch <= warmup_epochs:\n",
    "        pct = epoch / warmup_epochs\n",
    "        return ((base_lr - initial_lr) * pct) + initial_lr\n",
    "\n",
    "    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n",
    "        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n",
    "        return ((base_lr - min_lr) * pct) + min_lr\n",
    "\n",
    "    return min_lr\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1900-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['Percentage Change'] = data['Close'].pct_change()\n",
    "    data['Percentage Change'] = data['Percentage Change'].shift(-1)\n",
    "    data['Sentiment'] = pd.Series(np.where(data['Percentage Change'] > 0.025, 1, np.where(data['Percentage Change'] < -0.025, -1, 0)), index=data.index)\n",
    "    # Drop any rows with missing values\n",
    "    data.dropna(inplace=True)\n",
    "    data.drop('Percentage Change',axis=1 , inplace=True)\n",
    "    return data\n",
    "\n",
    "def train_timeseriestransformer_model():\n",
    "    from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerModel\n",
    "\n",
    "    # Initializing a Time Series Transformer configuration with 12 time steps for prediction\n",
    "    configuration = TimeSeriesTransformerConfig(prediction_length=12)\n",
    "\n",
    "    # Randomly initializing a model (with random weights) from the configuration\n",
    "    model = TimeSeriesTransformerModel(configuration)\n",
    "\n",
    "    # Accessing the model configuration\n",
    "    configuration = model.config\n",
    "    return model\n",
    "def train_transformer(symbol_to_fetch,start_date ,end_date,no_model = None):\n",
    "    #fetching data \n",
    "    stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "    # Calculate deltas for open, high, low, and close columns\n",
    "    for i in range(1, 6):  # Calculate deltas up to 5 days\n",
    "        stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "        stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "        stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "        stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "        \n",
    "    stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "    stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "    # stock.index = stock.index.date\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "    train_data = stock.iloc[:train_data_index]\n",
    "    test_data = stock.loc[start_date:]\n",
    "    train_data = label_data(train_data)\n",
    "    test_data = label_data(test_data)\n",
    "\n",
    "    #trian & test data\n",
    "    X_train_data = train_data.iloc[:,:-1]\n",
    "    y_train_data = train_data.iloc[:,-1]\n",
    "    X_test_data = test_data.iloc[:,:-1]\n",
    "    y_test_data = test_data.iloc[:,-1]\n",
    "    print(len(X_test_data))\n",
    "    # Normalize the data\n",
    "    normalizer = MinMaxScaler()\n",
    "    X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "    X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "    # # Reshape X_train_data_normalizer\n",
    "    X_train = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "    X_test = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "    if not no_model :\n",
    "        \n",
    "        # model = build_model(\n",
    "        #     input_shape,\n",
    "        #     head_size=head_size,\n",
    "        #     num_heads=num_heads,\n",
    "        #     ff_dim=ff_dim,\n",
    "        #     num_transformer_blocks=num_transformer_blocks,\n",
    "        #     mlp_units=mlp_units,\n",
    "        #     mlp_dropout=mlp_dropout,\n",
    "        #     dropout=dropout,\n",
    "        # )\n",
    "        # model = train_timeseriestransformer_model()\n",
    "\n",
    "        model = build_gru_model(input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                            gru_units=128, mlp_units=[256], dropout=dropout)\n",
    "\n",
    "        model.compile(\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "            metrics=[\"mean_squared_error\"],\n",
    "        )\n",
    "\n",
    "\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "            tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "        ]\n",
    "\n",
    "        # model.summary()\n",
    "        history = model.fit(\n",
    "            X_train,\n",
    "            y_train_data,\n",
    "            validation_split=0.2,\n",
    "            epochs=100,\n",
    "            batch_size=20,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        model.save('models/gru_'+f\"{symbol_to_fetch}\"+\"_model.h5\")\n",
    "        model.save('models/gru_'+f\"{symbol_to_fetch}\"+\"_model.keras\")\n",
    "        no_model = model\n",
    "    return no_model,X_test,test_data\n",
    "    #predictions\n",
    "def prepare_sentiment_from_transformer(symbol_to_fetch,start_date,end_date):\n",
    "    try :\n",
    "        \n",
    "        print(\"entered\")\n",
    "        model = tf.keras.models.load_model('models/gru_'+f\"{symbol_to_fetch}\"+\"_model.keras\")\n",
    "        print(\"passed\")\n",
    "        _,X_test,test_data = train_transformer(symbol_to_fetch = symbol_to_fetch, start_date = start_date, end_date = end_date,no_model=model)\n",
    "    \n",
    "    except:\n",
    "        model,X_test,test_data = train_transformer(symbol_to_fetch = symbol_to_fetch, start_date = start_date, end_date = end_date)\n",
    "    y_pred = model.predict(X_test) # this is the sentiment data \n",
    "    # Assuming you have X_test and y_test_data\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, test_data.iloc[:,-1])\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    test_data['transformer_sentiment'] = y_pred\n",
    "\n",
    "    test_data.index = test_data.index.date\n",
    "    test_data.to_csv('data/transformer_sentiment.csv')\n",
    "    return test_data,'data/transformer_sentiment.csv'\n",
    "    \"\"\"next steps :  we need to additionally train the model if model is already present\n",
    "    or take nearly 30 stocks and train the model with the huge data \n",
    "    or take every 5 mins data nad trian with it, and at last mix the test data with day wise\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "from transformers import pipeline\n",
    "import sys\n",
    "sys.path.append(\"D:\\krishna\\msdsm//trimister 6\\Project\\KrishnaProject\\AlgoTrading\")\n",
    "from alpaca.client import AlpacaNewsFetcher,NewsAPI\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class NewsSentimentAnalysis:\n",
    "    \"\"\"\n",
    "  A class for sentiment analysis of news articles using the Transformers library.\n",
    "\n",
    "  Attributes:\n",
    "  - classifier (pipeline): Sentiment analysis pipeline from Transformers.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "    Initializes the NewsSentimentAnalysis object.\n",
    "    \"\"\"\n",
    "        self.classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "    def analyze_sentiment(self, news_article):\n",
    "        \"\"\"\n",
    "    Analyzes the sentiment of a given news article.\n",
    "\n",
    "    Args:\n",
    "    - news_article (dict): Dictionary containing 'summary', 'headline', and 'created_at' keys.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing sentiment analysis results.\n",
    "    \"\"\"\n",
    "        summary = news_article['summary']\n",
    "        title = news_article['title']\n",
    "        timestamp = news_article['timestamp']\n",
    "\n",
    "        relevant_text = summary + title\n",
    "        sentiment_result = self.classifier(relevant_text)\n",
    "\n",
    "        analysis_result = {\n",
    "            'timestamp': timestamp,\n",
    "            'title': title,\n",
    "            'summary': summary,\n",
    "            'sentiment': sentiment_result\n",
    "        }\n",
    "\n",
    "        return analysis_result\n",
    "    \n",
    "    def analyze_sentiment_using_mistral(self, symbol,news_article):\n",
    "        summary = news_article['summary']\n",
    "        title = news_article['title']\n",
    "        timestamp = news_article['timestamp']\n",
    "        relevant_text = summary + title\n",
    "        prompt = f'''[INST]\n",
    "        You are a sentiment analysis model that can classify news articles as having a positive or negative sentiment. You will be given a news article, and your task is to determine its sentiment based on the content of the article. Please provide a one-word answer, either \"POSITIVE\" or \"NEGATIVE\"\n",
    "        [/INST]\n",
    "\n",
    "        [userINST]\n",
    "        {relevant_text}\n",
    "        [userINST]\n",
    "\n",
    "        \"[Insert one-word sentiment classification here: \"POSITIVE\" or \"NEGATIVE\"]\"\n",
    "         '''\n",
    "\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": relevant_text}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        assistant_response = completion.choices[0].message.content.strip()\n",
    "        sentiment_result = assistant_response.split()[0]\n",
    "        analysis_result = {\n",
    "        'timestamp': timestamp,\n",
    "        'title': title,\n",
    "        'summary': summary,\n",
    "        'sentiment': sentiment_result\n",
    "    }\n",
    "\n",
    "        return analysis_result\n",
    "\n",
    "\n",
    "\n",
    "def do_sentiment_analysis(symbol,start_date,end_date):\n",
    "    news_fetcher = AlpacaNewsFetcher()\n",
    "    news_api_fetcher = NewsAPI()\n",
    "\n",
    "    # Fetch news for AAPL from 2021-01-01 to 2021-12-31\n",
    "    news_data = news_fetcher.fetch_news(symbol=symbol, start_date=start_date, end_date=end_date)\n",
    "\n",
    "    try : \n",
    "        news_api_data = news_api_fetcher.get_stock_news(symbol, start_date, end_date)\n",
    "\n",
    "        combined_news_data = pd.concat([news_data,news_api_data])\n",
    "    except : \n",
    "        combined_news_data = news_data\n",
    "        \n",
    "    import pandas as pd\n",
    "    complete_news = []\n",
    "\n",
    "    # Initialize the NewsSentimentAnalysis object\n",
    "    news_sentiment_analyzer = NewsSentimentAnalysis()\n",
    "\n",
    "    # Assume 'news_data' is a list of news articles (each as a dictionary)\n",
    "    for article in combined_news_data:\n",
    "\n",
    "        sentiment_analysis_result = news_sentiment_analyzer.analyze_sentiment(article)\n",
    "        complete_news.append({'timestamp': sentiment_analysis_result[\"timestamp\"],'title': sentiment_analysis_result[\"title\"],'sentiment': sentiment_analysis_result['sentiment'][0]['label']})\n",
    "    pd.DataFrame(complete_news).set_index('timestamp').to_csv('data/sentiment_analysis.csv')\n",
    "\n",
    "    return pd.DataFrame(complete_news),'data/sentiment_analysis.csv'\n",
    "\n",
    "def do_sentiment_analysis_using_mistral(symbol,start_date,end_date):\n",
    "    news_fetcher = AlpacaNewsFetcher()\n",
    "\n",
    "    # Fetch news for AAPL from 2021-01-01 to 2021-12-31\n",
    "    news_data = news_fetcher.fetch_news(symbol=symbol, start_date=start_date, end_date=end_date)\n",
    "\n",
    "    import pandas as pd\n",
    "    complete_news = []\n",
    "\n",
    "    # Initialize the NewsSentimentAnalysis object\n",
    "    news_sentiment_analyzer = NewsSentimentAnalysis()\n",
    "\n",
    "    # Assume 'news_data' is a list of news articles (each as a dictionary)\n",
    "    for article in news_data:\n",
    "\n",
    "        sentiment_analysis_result = news_sentiment_analyzer.analyze_sentiment(article)\n",
    "        complete_news.append({'timestamp': sentiment_analysis_result[\"timestamp\"],'title': sentiment_analysis_result[\"title\"],'sentiment': sentiment_analysis_result['sentiment'][0]['label']})\n",
    "    pd.DataFrame(complete_news).set_index('timestamp').to_csv('data/sentiment_analysis.csv')\n",
    "\n",
    "    return 'data/sentiment_analysis.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backtrader as bt\n",
    "\n",
    "class Customstrategy(bt.Strategy):\n",
    "    \"\"\"\n",
    "    Custom Backtrader strategy with advanced technical indicators and sentiment analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - fast_ma (int): Period for the fast moving average.\n",
    "    - slow_ma (int): Period for the slow moving average.\n",
    "    - rsi_period (int): Period for the Relative Strength Index (RSI).\n",
    "    - rsi_oversold (float): RSI level considered as oversold for buying.\n",
    "    - rsi_overbought (float): RSI level considered as overbought for selling.\n",
    "    - bollinger_window (int): Period for Bollinger Bands.\n",
    "    - bollinger_dev (float): Standard deviation factor for Bollinger Bands.\n",
    "    - ema_window (int): Period for Exponential Moving Average (EMA).\n",
    "    - envelopes_ema_window (int): Period for EMA used in Envelopes indicator.\n",
    "    - envelopes_percentage (float): Percentage for Envelopes indicator.\n",
    "    - macd_short_window (int): Short window period for MACD.\n",
    "    - macd_long_window (int): Long window period for MACD.\n",
    "    - macd_signal_window (int): Signal window period for MACD.\n",
    "    - stochastic_k_window (int): Window period for Stochastic Oscillator %K.\n",
    "    - stochastic_d_window (int): Window period for Stochastic Oscillator %D.\n",
    "    \"\"\"\n",
    "\n",
    "    params = (\n",
    "        (\"fast_ma\", 20),\n",
    "        (\"slow_ma\", 50),\n",
    "        (\"rsi_period\", 14),\n",
    "        (\"rsi_oversold\", 30),\n",
    "        (\"rsi_overbought\", 70),\n",
    "        (\"bollinger_window\", 20),\n",
    "        (\"bollinger_dev\", 2),\n",
    "        (\"ema_window\", 20),\n",
    "        (\"envelopes_ema_window\", 20),\n",
    "        (\"envelopes_percentage\", 5),\n",
    "        (\"macd_short_window\", 12),\n",
    "        (\"macd_long_window\", 26),\n",
    "        (\"macd_signal_window\", 9),\n",
    "        (\"stochastic_k_window\", 14),\n",
    "        (\"stochastic_d_window\", 3),\n",
    "    )\n",
    "\n",
    "    def __init__(self, indicators=None):\n",
    "        \"\"\"\n",
    "        Initializes the AdvancedStrategy.\n",
    "\n",
    "        Creates and initializes the required technical indicators and sentiment data based on the selected indicators:\n",
    "        - fast_ma: Fast Simple Moving Average (SMA)\n",
    "        - slow_ma: Slow Simple Moving Average (SMA)\n",
    "        - rsi: Relative Strength Index (RSI)\n",
    "        - bollinger: Bollinger Bands\n",
    "        - ema: Exponential Moving Average (EMA)\n",
    "        - macd: Moving Average Convergence Divergence (MACD)\n",
    "        - stochastic: Stochastic Oscillator\n",
    "        - envelopes: Envelopes\n",
    "        \"\"\"\n",
    "        self.indicators = indicators or {}\n",
    "        \n",
    "        if \"ma\" in self.indicators:\n",
    "            self.fast_ma = bt.indicators.SimpleMovingAverage(self.data.close, period=self.params.fast_ma)\n",
    "        if \"ma\" in self.indicators:\n",
    "            self.slow_ma = bt.indicators.SimpleMovingAverage(self.data.close, period=self.params.slow_ma)\n",
    "        if \"rsi\" in self.indicators:\n",
    "            self.rsi = bt.indicators.RelativeStrengthIndex(self.data.close, period=self.params.rsi_period)\n",
    "        if \"bollinger\" in self.indicators:\n",
    "            self.bollinger = bt.indicators.BollingerBands(self.data.close, period=self.params.bollinger_window, devfactor=self.params.bollinger_dev)\n",
    "        if \"ema\" in self.indicators:\n",
    "            self.ema = bt.indicators.ExponentialMovingAverage(self.data.close, period=self.params.ema_window)\n",
    "        if \"macd\" in self.indicators:\n",
    "            self.macd = bt.indicators.MACD(self.data.close, period_me1=self.params.macd_short_window, period_me2=self.params.macd_long_window, period_signal=self.params.macd_signal_window)\n",
    "        if \"stochastic\" in self.indicators:\n",
    "            self.stochastic = bt.indicators.Stochastic(self.data, period=self.params.stochastic_k_window, period_dfast=self.params.stochastic_d_window)\n",
    "        if \"envelopes\" in self.indicators:\n",
    "            self.envelopes = bt.indicators.Envelope(self.data.close, period=self.params.envelopes_ema_window, devfactor=self.params.envelopes_percentage/100)\n",
    "\n",
    "        self.sentiment = self.datas[0].signal if len(self.datas) > 0 else None\n",
    "        self.transformer_sentiment = self.datas[0].transformer_sentiment if len(self.datas) > 0 else None\n",
    "    def next(self):\n",
    "        \"\"\"\n",
    "        Executes the trading logic on each iteration.\n",
    "        \"\"\"\n",
    "        buy_signal = sell_signal = 0\n",
    "\n",
    "        if \"ma\" in self.indicators and self.fast_ma > self.slow_ma:\n",
    "            buy_signal += 1\n",
    "        else:\n",
    "            sell_signal += 1\n",
    "\n",
    "        if \"rsi\" in self.indicators and self.rsi[0] < self.params.rsi_oversold:\n",
    "            buy_signal += 1\n",
    "        elif \"rsi\" in self.indicators and self.rsi[0] > self.params.rsi_overbought:\n",
    "            sell_signal += 1\n",
    "\n",
    "        if \"bollinger\" in self.indicators and self.data.close[0] < self.bollinger.lines.bot[0]:\n",
    "            buy_signal += 1\n",
    "        elif \"bollinger\" in self.indicators and self.data.close[0] > self.bollinger.lines.top[0]:\n",
    "            sell_signal += 1\n",
    "\n",
    "        if \"ema\" in self.indicators and self.data.close[0] > self.ema[0]:\n",
    "            buy_signal += 1\n",
    "        elif \"ema\" in self.indicators and self.data.close[0] < self.ema[0]:\n",
    "            sell_signal += 1\n",
    "\n",
    "        if \"macd\" in self.indicators and self.macd[0] > 0:\n",
    "            buy_signal += 1\n",
    "        elif \"macd\" in self.indicators and self.macd[0] < 0:\n",
    "            sell_signal += 1\n",
    "\n",
    "        if \"stochastic\" in self.indicators and self.stochastic[0] < self.stochastic.lines.d[0]:\n",
    "            buy_signal += 1\n",
    "        elif \"stochastic\" in self.indicators and self.stochastic[0] > self.stochastic.lines.d[0]:\n",
    "            sell_signal += 1\n",
    "\n",
    "        if \"envelopes\" in self.indicators and self.data.close[0] > self.envelopes.lines.erveh[0]:\n",
    "            sell_signal += 1\n",
    "        elif \"envelopes\" in self.indicators and self.data.close[0] < self.envelopes.lines.ervlo[0]:\n",
    "            buy_signal += 1\n",
    "        \n",
    "        if \"news_sentiment\" in self.indicators and self.sentiment is not None and self.sentiment[0] > 0:\n",
    "            buy_signal += 1\n",
    "        elif \"news_sentiment\" in self.indicators and self.sentiment is not None and self.sentiment[0] < 0:\n",
    "            sell_signal += 1\n",
    "        \n",
    "        if \"transformer_sentiment\" in self.indicators and self.transformer_sentiment is not None and self.transformer_sentiment[0] > 0:\n",
    "            buy_signal += 1\n",
    "        elif \"transformer_sentiment\" in self.indicators and self.transformer_sentiment is not None and self.transformer_sentiment[0] < 0:\n",
    "            sell_signal += 1\n",
    "\n",
    "        if buy_signal > sell_signal:\n",
    "            self.buy()\n",
    "        elif sell_signal > buy_signal:\n",
    "            self.sell()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backtrader as bt\n",
    "\n",
    "# from strategies.technical_with_sentiment_strategy.optimized_strategy import OptimizedStrategy\n",
    "# from strategies.technical_with_sentiment_strategy.custom_strategy import Customstrategy\n",
    "# from strategies.technical_with_sentiment_strategy.sentiment_data import SentimentData\n",
    "\n",
    "import backtrader as bt\n",
    "class SentimentData(bt.feeds.GenericCSVData):\n",
    "    \"\"\"\n",
    "    Custom Backtrader data feed class for sentiment data.\n",
    "\n",
    "    Parameters:\n",
    "    - dtformat (str): Date format for parsing the date column.\n",
    "    - date (int): Column index for the date in the CSV file.\n",
    "    - signal (int): Column index for the sentiment signal in the CSV file.\n",
    "    - transformer_sentiment (int): Column index for the sentiment signal from the transformer model in the CSV file.\n",
    "    - openinterest (int): Column index for the open interest in the CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    lines = ('signal', 'transformer_sentiment')\n",
    "\n",
    "    params = (\n",
    "        ('dtformat', '%Y-%m-%d'),\n",
    "        ('date', 0),\n",
    "        ('signal', 7),\n",
    "        ('transformer_sentiment', 8),\n",
    "        ('openinterest', -1)\n",
    "    )\n",
    "\n",
    "\n",
    "class BacktestRunner:\n",
    "    @staticmethod\n",
    "    def run_backtest(data, stock_ticker, start_date, end_date,indicators):\n",
    "        \"\"\"\n",
    "        Run Backtrader backtest with the provided data.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): Merged stock and sentiment data.\n",
    "            stock_ticker (str): Stock Ticker name.\n",
    "            start_date (str): Start date for backtesting.\n",
    "            end_date (str): End date for backtesting.\n",
    "        \"\"\"\n",
    "        cerebro = bt.Cerebro()\n",
    "\n",
    "        # Convert data to Backtrader format\n",
    "        data_feed = SentimentData(dataname= data)\n",
    "\n",
    "\n",
    "        # Add data to cerebro\n",
    "        cerebro.adddata(data_feed)\n",
    "\n",
    "        # # Add strategy with parameters\n",
    "        # cerebro.addstrategy(OptimizedStrategy)\n",
    "        # Add strategy with parameters\n",
    "        cerebro.addstrategy(Customstrategy, indicators=indicators)\n",
    "\n",
    "        # Set initial cash and commission\n",
    "        cerebro.broker.set_cash(100000)\n",
    "        cerebro.broker.setcommission(commission=0.001)\n",
    "\n",
    "        # Add built-in analyzers\n",
    "        cerebro.addanalyzer(bt.analyzers.Returns)\n",
    "        cerebro.addanalyzer(bt.analyzers.SharpeRatio, riskfreerate=0.0)\n",
    "        cerebro.addanalyzer(bt.analyzers.DrawDown)\n",
    "        cerebro.addanalyzer(bt.analyzers.TradeAnalyzer)\n",
    "        cerebro.addanalyzer(bt.analyzers.SQN)\n",
    "        cerebro.addanalyzer(bt.analyzers.VWR)\n",
    "        cerebro.addanalyzer(bt.analyzers.PyFolio)\n",
    "\n",
    "        thestrats = cerebro.run()\n",
    "        thestrat = thestrats[0]\n",
    "\n",
    "        # Get results from analyzers\n",
    "        returns = thestrat.analyzers.returns.get_analysis()\n",
    "        # returns = returns - 0.005\n",
    "        sharpe_ratio = thestrat.analyzers.sharperatio.get_analysis()\n",
    "        drawdown = thestrat.analyzers.drawdown.get_analysis()\n",
    "        trades = thestrat.analyzers.tradeanalyzer.get_analysis()\n",
    "        sqn = thestrat.analyzers.sqn.get_analysis()\n",
    "        vwr = thestrat.analyzers.vwr.get_analysis()\n",
    "        pyfolio = thestrat.analyzers.getbyname('pyfolio')\n",
    "\n",
    "        pyfolio_returns, positions, transactions, gross_lev = pyfolio.get_pf_items()\n",
    "\n",
    "        # Print the backtesting report\n",
    "        print(\"\\n--- Backtesting Report ---\")\n",
    "        print(f\"Indicators: {', '.join(indicators)}\")\n",
    "        print(\"Stock Ticker: {}\".format(stock_ticker))\n",
    "        print(\"Start Date: {}\".format(start_date))\n",
    "        print(\"End Date: {}\".format(end_date))\n",
    "        print(\"Initial Portfolio Value: ${:.2f}\".format(cerebro.broker.startingcash))\n",
    "        print(\"Final Portfolio Value: ${:.2f}\".format(cerebro.broker.getvalue()))\n",
    "        print(\"Total Return: {:.2f}%\".format(returns['rtot'] * 100))\n",
    "        print(\"Annualized Return: {:.2f}%\".format(returns['ravg'] * 100 * 252))  # Assuming 252 trading days in a year\n",
    "        print(\"Max Drawdown: {:.2f}%\".format(drawdown['max']['drawdown'] * 100))\n",
    "\n",
    "        # Print Additional Metrics\n",
    "        print(\"\\n--- Additional Metrics ---\")\n",
    "        print(\"{:<15} {:<15} {:<15}\".format(\"Value at Risk\", \"VWR\", \"Total Trades\"))\n",
    "        print(\"{:<15.2f} {:<15.4f} {:<15}\".format(vwr['vwr'], vwr['vwr'], trades.total.total))\n",
    "\n",
    "        # Create a dictionary to store the results\n",
    "        results = {\n",
    "            'Indicators': ', '.join(indicators),\n",
    "            'Stock Ticker': stock_ticker,\n",
    "            'Start Date': start_date,\n",
    "            'End Date': end_date,\n",
    "            'Initial Portfolio Value': cerebro.broker.startingcash,\n",
    "            'Final Portfolio Value': cerebro.broker.getvalue(),\n",
    "            'Total Return': returns['rtot'] * 100,\n",
    "            'Annualized Return': returns['ravg'] * 100 * 252,\n",
    "            'Max Drawdown': drawdown['max']['drawdown'] * 100,\n",
    "            'Value at Risk': vwr['vwr'],\n",
    "            'VWR': vwr['vwr'],\n",
    "            'Total Trades': trades.total.total\n",
    "        }\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class StockDataProcessor:\n",
    "    def __init__(self, stock_ticker, start_date, end_date, news_sentiment,ohlc_sentiment):\n",
    "        self.stock_ticker = stock_ticker\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        # self.sentiment_data_path = sentiment_data_path\n",
    "        self.news_sentiment = news_sentiment\n",
    "        self.ohlc_sentiment = ohlc_sentiment\n",
    "        self.data = self.download_stock_data()\n",
    "\n",
    "    def download_stock_data(self):\n",
    "        \"\"\"\n",
    "        Download stock data from Yahoo Finance.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Stock data.\n",
    "        \"\"\"\n",
    "        return yf.download(self.stock_ticker, start=self.start_date, end=self.end_date)\n",
    "\n",
    "    def preprocess_sentiment_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess sentiment data and merge with stock data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Merged DataFrame.\n",
    "        \"\"\"\n",
    "        if self.news_sentiment :\n",
    "            news_sentiment_data,_ = do_sentiment_analysis(self.stock_ticker, self.start_date, self.end_date)\n",
    "            # Create a column for buy/sell signals based on sentiment\n",
    "            news_sentiment_data['signal'] = 0\n",
    "            news_sentiment_data.loc[news_sentiment_data['sentiment'] == 'POSITIVE', 'signal'] = 1\n",
    "            news_sentiment_data.loc[news_sentiment_data['sentiment'] == 'NEGATIVE', 'signal'] = -1\n",
    "\n",
    "            # Assuming df is your existing DataFrame\n",
    "            news_sentiment_data['timestamp'] = pd.to_datetime(news_sentiment_data['timestamp']).dt.date\n",
    "\n",
    "            # Group by day and sum up 'Signal' values\n",
    "            sentiment_daily_sum = news_sentiment_data.groupby('timestamp')['signal'].sum().reset_index()\n",
    "            sentiment_daily_sum = sentiment_daily_sum.rename(columns={'timestamp': 'date', 'signal': 'signal'})\n",
    "\n",
    "            sentiment_daily_sum['date'] = pd.to_datetime(sentiment_daily_sum['date'])\n",
    "\n",
    "            sentiment_daily_sum.to_csv('data/sentiment_daily_sum.csv')\n",
    "            # Merge DataFrames on 'Date'\n",
    "            merged_df = pd.merge(self.data, sentiment_daily_sum, left_index=True, right_on='date', how='left')\n",
    "            merged_df.set_index('date', inplace=True)\n",
    "        \n",
    "        #sentiment of transformer model\n",
    "        if self.ohlc_sentiment:\n",
    "            transformer_sentiment_data,_ = prepare_sentiment_from_transformer(self.stock_ticker, self.start_date, self.end_date)\n",
    "            print(transformer_sentiment_data.columns)\n",
    "            transformer_sentiment_data = transformer_sentiment_data[\"transformer_sentiment\"].copy()\n",
    "            # transformer_sentiment_data['date'] = transformer_sentiment_data.iloc[:,0]\n",
    "            if self.news_sentiment:\n",
    "                merged_df = pd.merge(merged_df, transformer_sentiment_data,left_index=True, right_index = True, how ='left')\n",
    "            else :\n",
    "                merged_df = pd.merge(self.data, transformer_sentiment_data, left_index=True, right_index = True, how='left')\n",
    "                # merged_df.set_index('date', inplace=True)\n",
    "        \n",
    "            \n",
    "\n",
    "        merged_df.to_csv('data/merged_df.csv')\n",
    "\n",
    "        return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "entered\n",
      "82\n",
      "WARNING:tensorflow:From d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\backend.py:1400: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1370, in run_step  *\n        outputs = model.train_step(data)\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1148, in train_step  *\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1207, in compute_loss  *\n        y, y_pred, sample_weight, regularization_losses=self.losses\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\engine\\compile_utils.py\", line 277, in __call__  *\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 143, in __call__  *\n        losses = call_fn(y_true, y_pred)\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 270, in call  *\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 2454, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\backend.py\", line 5777, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(20, 1) and logits.shape=(20, 27, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m stock_data \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mdownload_stock_data()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Preprocess sentiment data and merge with stock data\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_sentiment_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Run backtest\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# BacktestRunner.run_backtest('data/merged_df.csv', STOCK_TICKER, START_DATE, END_DATE)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m merged_df\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[6], line 54\u001b[0m, in \u001b[0;36mStockDataProcessor.preprocess_sentiment_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m#sentiment of transformer model\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mohlc_sentiment:\n\u001b[1;32m---> 54\u001b[0m     transformer_sentiment_data,_ \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_sentiment_from_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstock_ticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(transformer_sentiment_data\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     56\u001b[0m     transformer_sentiment_data \u001b[38;5;241m=\u001b[39m transformer_sentiment_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer_sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[1;32mIn[1], line 219\u001b[0m, in \u001b[0;36mprepare_sentiment_from_transformer\u001b[1;34m(symbol_to_fetch, start_date, end_date)\u001b[0m\n\u001b[0;32m    216\u001b[0m     _,X_test,test_data \u001b[38;5;241m=\u001b[39m train_transformer(symbol_to_fetch \u001b[38;5;241m=\u001b[39m symbol_to_fetch, start_date \u001b[38;5;241m=\u001b[39m start_date, end_date \u001b[38;5;241m=\u001b[39m end_date,no_model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 219\u001b[0m     model,X_test,test_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbol_to_fetch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msymbol_to_fetch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;66;03m# this is the sentiment data \u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# Assuming you have X_test and y_test_data\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 197\u001b[0m, in \u001b[0;36mtrain_transformer\u001b[1;34m(symbol_to_fetch, start_date, end_date, no_model)\u001b[0m\n\u001b[0;32m    191\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    192\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    193\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mLearningRateScheduler(lr_scheduler)\n\u001b[0;32m    194\u001b[0m ]\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# model.summary()\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/gru_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol_to_fetch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    206\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/gru_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol_to_fetch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filejlc6cy3x.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filej56co81a.py:45\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m     43\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mjit_compile, if_body, else_body, get_state, set_state, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_step\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     44\u001b[0m data \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mnext\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(iterator),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 45\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(reduce_per_replica), (ag__\u001b[38;5;241m.\u001b[39mld(outputs), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdistribute_strategy), \u001b[38;5;28mdict\u001b[39m(reduction\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdistribute_reduction_method), fscope)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filej56co81a.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     16\u001b[0m do_return_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m retval__1 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcontrol_dependencies(ag__\u001b[38;5;241m.\u001b[39mld(_minimum_control_deps)(ag__\u001b[38;5;241m.\u001b[39mld(outputs))):\n\u001b[0;32m     20\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39m_train_counter\u001b[38;5;241m.\u001b[39massign_add, (\u001b[38;5;241m1\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope_1)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filevm9hbta8.py:38\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     37\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[1;32m---> 38\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_validate_target_and_loss, (ag__\u001b[38;5;241m.\u001b[39mld(y), ag__\u001b[38;5;241m.\u001b[39mld(loss)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     40\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mminimize, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28mdict\u001b[39m(tape\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(tape)), fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileaksj2ibo.py:63\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss\u001b[1;34m(self, x, y, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcompiled_loss, (ag__\u001b[38;5;241m.\u001b[39mld(y), ag__\u001b[38;5;241m.\u001b[39mld(y_pred), ag__\u001b[38;5;241m.\u001b[39mld(sample_weight)), \u001b[38;5;28mdict\u001b[39m(regularization_losses\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mlosses), fscope)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filey7lr5zcx.py:203\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[1;34m(self, y_true, y_pred, sample_weight, regularization_losses)\u001b[0m\n\u001b[0;32m    201\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_value\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    202\u001b[0m y_p \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_p\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 203\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_9\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_9\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(y_t, y_p, sw, loss_obj, loss_weight, metric_obj)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_10\u001b[39m():\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (regularization_losses,)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filey7lr5zcx.py:193\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__.<locals>.loop_body\u001b[1;34m(itr)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m batch_dim, y_t, y_p, sw\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontinue_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_p\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filey7lr5zcx.py:89\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__.<locals>.loop_body.<locals>.if_body_8\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m y_t, y_p, sw \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(match_dtype_and_rank), (ag__\u001b[38;5;241m.\u001b[39mld(y_t), ag__\u001b[38;5;241m.\u001b[39mld(y_p), ag__\u001b[38;5;241m.\u001b[39mld(sw)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     88\u001b[0m sw \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(losses_utils)\u001b[38;5;241m.\u001b[39mapply_mask, (ag__\u001b[38;5;241m.\u001b[39mld(y_p), ag__\u001b[38;5;241m.\u001b[39mld(sw), ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(losses_utils)\u001b[38;5;241m.\u001b[39mget_mask, (ag__\u001b[38;5;241m.\u001b[39mld(y_p),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 89\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_obj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_p\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m total_loss_mean_value \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(loss_value)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_2\u001b[39m():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileydgdqs7_.py:56\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m     54\u001b[0m call_fn \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall_fn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     55\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mexecuting_eagerly, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), if_body, else_body, get_state, set_state, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall_fn\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m in_mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(losses_utils)\u001b[38;5;241m.\u001b[39mget_mask, (ag__\u001b[38;5;241m.\u001b[39mld(y_pred),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     58\u001b[0m out_mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(losses_utils)\u001b[38;5;241m.\u001b[39mget_mask, (ag__\u001b[38;5;241m.\u001b[39mld(losses),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filef5bw8k_8.py:38\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(ag_fn), (ag__\u001b[38;5;241m.\u001b[39mld(y_true), ag__\u001b[38;5;241m.\u001b[39mld(y_pred)), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_fn_kwargs), fscope)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1370, in run_step  *\n        outputs = model.train_step(data)\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1148, in train_step  *\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1207, in compute_loss  *\n        y, y_pred, sample_weight, regularization_losses=self.losses\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\engine\\compile_utils.py\", line 277, in __call__  *\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 143, in __call__  *\n        losses = call_fn(y_true, y_pred)\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 270, in call  *\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 2454, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"d:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\tf_keras\\src\\backend.py\", line 5777, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(20, 1) and logits.shape=(20, 27, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# from processor.stock_data_processor import StockDataProcessor\n",
    "# from runner.backtest_runner import BacktestRunner\n",
    "# from sentiment_analysis.sentiment_analysis_pipeline import do_sentiment_analysis\n",
    "# from sentiment_analysis.sentiment_analysis_trasformer_model import preprae_sentiment_from_transformer\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "# Configuration\n",
    "STOCK_TICKER = 'MSFT'\n",
    "START_DATE = '2024-01-01'\n",
    "END_DATE = '2024-05-01'\n",
    "news_sentiment = True\n",
    "ohlc_sentiment = True\n",
    "SENTIMENT_DATA_PATH = 'data/stock_sentiment_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Initialize the StockDataProcessor\n",
    "processor = StockDataProcessor(STOCK_TICKER, START_DATE, END_DATE, news_sentiment,ohlc_sentiment)\n",
    "\n",
    "# Download stock data\n",
    "stock_data = processor.download_stock_data()\n",
    "# Preprocess sentiment data and merge with stock data\n",
    "merged_df = processor.preprocess_sentiment_data()\n",
    "# Run backtest\n",
    "# BacktestRunner.run_backtest('data/merged_df.csv', STOCK_TICKER, START_DATE, END_DATE)\n",
    "merged_df.fillna(0,inplace=True)\n",
    "\n",
    "results_df = []\n",
    "\n",
    "# Define the list of indicators to use\n",
    "indicators = ['rsi','bollinger','macd','ema','fast_ma','news_sentiment','transformer_sentiment']\n",
    "# indicators =['transformer_sentiment']\n",
    "\n",
    "# Generate all possible combinations of indicators\n",
    "indicator_combinations = list(itertools.chain.from_iterable(itertools.combinations(indicators, r) for r in range(1, len(indicators)+1)))\n",
    "\n",
    "# Run backtest for each combination of indicators\n",
    "for indicators in indicator_combinations:\n",
    "    print(f\"Running backtest for indicators: {', '.join(indicators)}\")\n",
    "    results = BacktestRunner.run_backtest('data/merged_df.csv', STOCK_TICKER, START_DATE, END_DATE, indicators)\n",
    "    \n",
    "    # Convert the results dictionary to a DataFrame\n",
    "    results_df.append(results)\n",
    "\n",
    "    # Save the results DataFrame to an Excel file\n",
    "    pd.DataFrame(results_df).to_excel('output/backtest_results_'+STOCK_TICKER+'_'+str(START_DATE)+'_'+str(END_DATE)+'.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`TransfoXL` was deprecated due to security issues linked to `pickle.load` in `TransfoXLTokenizer`. See more details on this model's documentation page: `https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/transfo-xl.md`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransfo-xl/transfo-xl-wt103\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m revision \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m40a186da79458c9f9de846edfaea79c412137f97\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTransfoXLTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m TransfoXLLMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint, revision\u001b[38;5;241m=\u001b[39mrevision)\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2089\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2086\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2087\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2089\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2092\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2093\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2097\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2098\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2100\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2311\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2309\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2311\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2312\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2314\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2315\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2316\u001b[0m     )\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\transformers\\models\\deprecated\\transfo_xl\\tokenization_transfo_xl.py:214\u001b[0m, in \u001b[0;36mTransfoXLTokenizer.__init__\u001b[1;34m(self, special, min_freq, max_size, lower_case, delimiter, vocab_file, pretrained_vocab_file, never_split, unk_token, eos_token, additional_special_tokens, language, **kwargs)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis part uses `pickle.load` which is insecure and will execute arbitrary code that is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpotentially malicious. It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms recommended to never unpickle data that could have come from an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`TRUST_REMOTE_CODE` to `True` to allow it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    212\u001b[0m     )\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(pretrained_vocab_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 214\u001b[0m     vocab_dict \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# Loading a torch-saved transfo-xl vocab dict with pickle results in an integer\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# Entering this if statement means that we tried to load a torch-saved file with pickle, and we failed.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# We therefore load it with torch, if it's available.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vocab_dict, \u001b[38;5;28mint\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TransfoXLTokenizer, TransfoXLLMHeadModel\n",
    "\n",
    "os.environ[\"TRUST_REMOTE_CODE\"] = \"True\"\n",
    "\n",
    "checkpoint = 'transfo-xl/transfo-xl-wt103'\n",
    "revision = '40a186da79458c9f9de846edfaea79c412137f97'\n",
    "\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained(checkpoint, revision=revision)\n",
    "model = TransfoXLLMHeadModel.from_pretrained(checkpoint, revision=revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerModel\n",
    "\n",
    "# Initializing a Time Series Transformer configuration with 12 time steps for prediction\n",
    "configuration = TimeSeriesTransformerConfig(prediction_length=12)\n",
    "\n",
    "# Randomly initializing a model (with random weights) from the configuration\n",
    "model = TimeSeriesTransformerModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stock.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching data \n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2023-02-02'\n",
    "end_date = '2023-04-02'\n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# stock.index = stock.index.date\n",
    "# Calculate deltas for open, high, low, and close columns\n",
    "for i in range(1, 6):  # Calculate deltas up to 5 days\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:train_data_index]\n",
    "test_data = stock.loc[start_date:]\n",
    "train_data = label_data(train_data)\n",
    "test_data = label_data(test_data)\n",
    "\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data))\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # Reshape X_train_data_normalizer\n",
    "X_train = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_test = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import InformerConfig, InformerModel\n",
    "\n",
    "# Assuming you have already defined your data (X_train, y_train_data, X_test, y_test_data)\n",
    "# and your loss function (loss_fn)\n",
    "\n",
    "# Initialize an Informer configuration (you've already done this)\n",
    "configuration = InformerConfig(prediction_length=12)\n",
    "\n",
    "# Initialize an Informer model\n",
    "model = InformerModel(configuration)\n",
    "\n",
    "# Define an optimizer (e.g., Adam) and a learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    predictions = model(X_train)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(predictions, y_train_data)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss.item()}\")\n",
    "\n",
    "# Evaluate the model (if you have test data)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test)\n",
    "    test_loss = loss_fn(test_predictions, y_test_data)\n",
    "    print(f\"Test Loss: {test_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca_trade_api import REST\n",
    "import yaml\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"D:\\krishna\\msdsm//trimister 6\\Project\\KrishnaProject\\AlgoTrading\")\n",
    "\n",
    "with open('configuration.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "class NewsAPI:\n",
    "    def __init__(self):\n",
    "        self.api_key = config['newsapi_api_key']\n",
    "        \n",
    "    \n",
    "    def get_stock_news(self,stock_ticker, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Fetches financial news related to a specific stock ticker within a date range.\n",
    "\n",
    "        Args:\n",
    "            stock_ticker (str): The stock ticker symbol (e.g., AAPL, MSFT).\n",
    "            start_date (str): Start date in YYYY-MM-DD format.\n",
    "            end_date (str): End date in YYYY-MM-DD format.\n",
    "            api_key (str): Your NewsAPI API key.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dictionaries containing news articles.\n",
    "        \"\"\"\n",
    "       \n",
    "        \n",
    "        formatted_news = []\n",
    "        current_date = start_date\n",
    "        next_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "        # Loop through the date range in increments of one day\n",
    "        while current_date <= end_date:\n",
    "\n",
    "            url = \"https://newsapi.org/v2/everything\"\n",
    "            params = {\n",
    "                \"q\": stock_ticker,\n",
    "                \"source\" : \"cnbc\",\n",
    "                \"from\": current_date,\n",
    "                \"to\": next_date,\n",
    "                \"apiKey\": self.api_key\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "            news_articles = response.json()\n",
    "            #\n",
    "\n",
    "            for article in news_articles:\n",
    "                summary = article['Description']\n",
    "                title = article['title']\n",
    "                timestamp = article['publishedAt']\n",
    "\n",
    "                relevant_info = {\n",
    "                    'timestamp': timestamp,\n",
    "                    'title': title,\n",
    "                    'summary': summary\n",
    "                }\n",
    "\n",
    "                formatted_news.append(relevant_info)\n",
    "\n",
    "            # Move to the next day\n",
    "            current_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "            next_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=2)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        return formatted_news\n",
    "\n",
    "class AlpacaNewsFetcher:\n",
    "    \"\"\"\n",
    "    A class for fetching news articles related to a specific stock from Alpaca API.\n",
    "\n",
    "    Attributes:\n",
    "    - api_key (str): Alpaca API key for authentication.\n",
    "    - api_secret (str): Alpaca API secret for authentication.\n",
    "    - rest_client (alpaca_trade_api.REST): Alpaca REST API client.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the AlpacaNewsFetcher object.\n",
    "\n",
    "        Args:\n",
    "        - api_key (str): Alpaca API key for authentication.\n",
    "        - api_secret (str): Alpaca API secret for authentication.\n",
    "        \"\"\"\n",
    "        self.api_key = config['alpaca_api_key']\n",
    "        self.api_secret = config['alpaca_api_secret']\n",
    "        self.rest_client = REST(self.api_key, self.api_secret)\n",
    "\n",
    "    def fetch_news(self, symbol, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Fetches news articles for a given stock symbol within a specified date range.\n",
    "\n",
    "        Args:\n",
    "        - symbol (str): Stock symbol for which news articles are to be fetched (e.g., \"AAPL\").\n",
    "        - start_date (str): Start date of the range in the format \"YYYY-MM-DD\".\n",
    "        - end_date (str): End date of the range in the format \"YYYY-MM-DD\".\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of dictionaries containing relevant information for each news article.\n",
    "        \"\"\"\n",
    "        formatted_news = []\n",
    "        current_date = start_date\n",
    "        next_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "        # Loop through the date range in increments of one day\n",
    "        while current_date <= end_date:\n",
    "            # Fetch news articles for the current date\n",
    "            news_articles = self.rest_client.get_news(symbol, current_date, next_date)\n",
    "\n",
    "            for article in news_articles:\n",
    "                summary = article.summary\n",
    "                title = article.headline\n",
    "                timestamp = article.created_at\n",
    "\n",
    "                relevant_info = {\n",
    "                    'timestamp': timestamp,\n",
    "                    'title': title,\n",
    "                    'summary': summary\n",
    "                }\n",
    "\n",
    "                formatted_news.append(relevant_info)\n",
    "\n",
    "            # Move to the next day\n",
    "            current_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "            next_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=2)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        return formatted_news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m end_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2021-03-01\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m news_data \u001b[38;5;241m=\u001b[39m news_fetcher\u001b[38;5;241m.\u001b[39mfetch_news(symbol\u001b[38;5;241m=\u001b[39msymbol, start_date\u001b[38;5;241m=\u001b[39mstart_date, end_date\u001b[38;5;241m=\u001b[39mend_date)\n\u001b[1;32m----> 8\u001b[0m news_api_data \u001b[38;5;241m=\u001b[39m \u001b[43mnews_api_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_stock_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m combined_news_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([news_data,news_api_data])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 54\u001b[0m, in \u001b[0;36mNewsAPI.get_stock_news\u001b[1;34m(self, stock_ticker, start_date, end_date)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m news_articles:\n\u001b[1;32m---> 54\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     55\u001b[0m     title \u001b[38;5;241m=\u001b[39m article[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     56\u001b[0m     timestamp \u001b[38;5;241m=\u001b[39m article[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublishedAt\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "news_fetcher = AlpacaNewsFetcher()\n",
    "news_api_fetcher = NewsAPI()\n",
    "symbol = 'AAPL'\n",
    "# Fetch news for AAPL from 2021-01-01 to 2021-12-31\n",
    "start_date = '2021-01-01'\n",
    "end_date = '2021-03-01'\n",
    "news_data = news_fetcher.fetch_news(symbol=symbol, start_date=start_date, end_date=end_date)\n",
    "news_api_data = news_api_fetcher.get_stock_news(symbol, start_date, end_date)\n",
    "\n",
    "combined_news_data = pd.concat([news_data,news_api_data])\n",
    "\n",
    "import pandas as pd\n",
    "complete_news = []\n",
    "\n",
    "# Initialize the NewsSentimentAnalysis object\n",
    "news_sentiment_analyzer = NewsSentimentAnalysis()\n",
    "\n",
    "# Assume 'news_data' is a list of news articles (each as a dictionary)\n",
    "for article in combined_news_data:\n",
    "\n",
    "    sentiment_analysis_result = news_sentiment_analyzer.analyze_sentiment(article)\n",
    "    complete_news.append({'timestamp': sentiment_analysis_result[\"timestamp\"],'title': sentiment_analysis_result[\"title\"],'sentiment': sentiment_analysis_result['sentiment'][0]['label']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m news_articles:\n\u001b[1;32m---> 23\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     24\u001b[0m     title \u001b[38;5;241m=\u001b[39m article[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     25\u001b[0m     timestamp \u001b[38;5;241m=\u001b[39m article[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublishedAt\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "formatted_news = []\n",
    "current_date = start_date\n",
    "next_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "# Loop through the date range in increments of one day\n",
    "while current_date <= end_date:\n",
    "\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    params = {\n",
    "        \"q\": 'AAPL',\n",
    "        \"source\" : \"cnbc\",\n",
    "        \"from\": current_date,\n",
    "        \"to\": next_date,\n",
    "        \"apiKey\": 'e0ec181e8a6a468d9c46510ad01613f2'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    news_articles = response.json()\n",
    "    #\n",
    "\n",
    "    for article in news_articles:\n",
    "        summary = article['Description']\n",
    "        title = article['title']\n",
    "        timestamp = article['publishedAt']\n",
    "\n",
    "        relevant_info = {\n",
    "            'timestamp': timestamp,\n",
    "            'title': title,\n",
    "            'summary': summary\n",
    "        }\n",
    "\n",
    "        formatted_news.append(relevant_info)\n",
    "\n",
    "    # Move to the next day\n",
    "    current_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    next_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=2)).strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'error',\n",
       " 'code': 'parameterInvalid',\n",
       " 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2024-04-06, but you have requested 2021-01-01. You may need to upgrade to a paid plan.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# Define the Transformer model\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = tf.keras.layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "# Build and train the model\n",
    "input_shape = (27,1)\n",
    "head_size = 46\n",
    "num_heads = 60\n",
    "ff_dim = 55\n",
    "num_transformer_blocks = 5\n",
    "mlp_units = [256]\n",
    "dropout = 0.14\n",
    "mlp_dropout = 0.4\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "\n",
    "    for _ in range(num_transformer_blocks):  # This is what stacks our transformer blocks\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "\n",
    "    for dim in mlp_units:\n",
    "        x = tf.keras.layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(mlp_dropout)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"softmax\")(x)  # this is a pass-through\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "def lr_scheduler(epoch, lr, warmup_epochs=30, decay_epochs=100, initial_lr=1e-6, base_lr=1e-3, min_lr=5e-5):\n",
    "    if epoch <= warmup_epochs:\n",
    "        pct = epoch / warmup_epochs\n",
    "        return ((base_lr - initial_lr) * pct) + initial_lr\n",
    "\n",
    "    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n",
    "        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n",
    "        return ((base_lr - min_lr) * pct) + min_lr\n",
    "\n",
    "    return min_lr\n",
    "\n",
    "def fetch_ticker_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetches stock data for a given symbol using yfinance.\"\"\"\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start='1900-01-01', end=end_date)\n",
    "    return data\n",
    "\n",
    "def label_data(data):\n",
    "    # Calculate the percentage change in price from one day to the next\n",
    "    data['Percentage Change'] = data['Close'].pct_change()\n",
    "    data['Percentage Change'] = data['Percentage Change'].shift(-1)\n",
    "    data['Sentiment'] = pd.Series(np.where(data['Percentage Change'] > 0.025, 2, np.where(data['Percentage Change'] < -0.025, 0, 1)), index=data.index)\n",
    "    # Drop any rows with missing values\n",
    "    data.dropna(inplace=True)\n",
    "    data.drop('Percentage Change',axis=1 , inplace=True)\n",
    "    return data\n",
    "\n",
    "def train_timeseriestransformer_model():\n",
    "    from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerModel\n",
    "\n",
    "    # Initializing a Time Series Transformer configuration with 12 time steps for prediction\n",
    "    configuration = TimeSeriesTransformerConfig(prediction_length=12)\n",
    "\n",
    "    # Randomly initializing a model (with random weights) from the configuration\n",
    "    model = TimeSeriesTransformerModel(configuration)\n",
    "\n",
    "    # Accessing the model configuration\n",
    "    configuration = model.config\n",
    "    return model\n",
    "# Define the GRU model\n",
    "def build_gru_model(input_shape, gru_units, mlp_units, dropout=0.2):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    x = inputs\n",
    "\n",
    "    # GRU layers (you can add more if needed)\n",
    "    x = tf.keras.layers.GRU(gru_units, return_sequences=True,input_shape=input_shape, activation='tanh')(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = tf.keras.layers.GRU(gru_units, return_sequences=True,input_shape=input_shape, activation='tanh')(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    # Additional GRU layer\n",
    "    x = tf.keras.layers.GRU(gru_units, return_sequences=True,input_shape=input_shape, activation='tanh')(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    # MLP layers\n",
    "    for units in mlp_units:\n",
    "        x = tf.keras.layers.Dense(units, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    # Output layer with 3 classes (positive, neutral, negative)\n",
    "    outputs = tf.keras.layers.Dense(1,activation = \"softmax\")(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "def train_transformer(symbol_to_fetch,start_date ,end_date,no_model = None):\n",
    "    #fetching data \n",
    "    stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "    # Calculate deltas for open, high, low, and close columns\n",
    "    for i in range(1, 6):  # Calculate deltas up to 5 days\n",
    "        stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "        stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "        stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "        stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "        \n",
    "    stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "    stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "    # stock.index = stock.index.date\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "    train_data = stock.iloc[:train_data_index]\n",
    "    test_data = stock.loc[start_date:]\n",
    "    train_data = label_data(train_data)\n",
    "    test_data = label_data(test_data)\n",
    "\n",
    "    #trian & test data\n",
    "    X_train_data = train_data.iloc[:,:-1]\n",
    "    y_train_data = train_data.iloc[:,-1]\n",
    "    X_test_data = test_data.iloc[:,:-1]\n",
    "    y_test_data = test_data.iloc[:,-1]\n",
    "    print(len(X_test_data))\n",
    "    # Normalize the data\n",
    "    normalizer = MinMaxScaler()\n",
    "    X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "    X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "    # # Reshape X_train_data_normalizer\n",
    "    X_train = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "    X_test = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)\n",
    "    if not no_model :\n",
    "        \n",
    "        # model = build_model(\n",
    "        #     input_shape,\n",
    "        #     head_size=head_size,\n",
    "        #     num_heads=num_heads,\n",
    "        #     ff_dim=ff_dim,\n",
    "        #     num_transformer_blocks=num_transformer_blocks,\n",
    "        #     mlp_units=mlp_units,\n",
    "        #     mlp_dropout=mlp_dropout,\n",
    "        #     dropout=dropout,\n",
    "        # )\n",
    "        # model = train_timeseriestransformer_model()\n",
    "#(X_train.shape[1], X_train.shape[2])\n",
    "        model = build_gru_model(input_shape=(X_train.shape[1],1),\n",
    "                            gru_units=128, mlp_units=[256], dropout=dropout)\n",
    "\n",
    "        model.compile(\n",
    "            loss='mse',\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "            metrics=[\"mse\"],\n",
    "        )\n",
    "\n",
    "\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "            tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "        ]\n",
    "\n",
    "        model.summary()\n",
    "        history = model.fit(\n",
    "            X_train,\n",
    "            y_train_data,\n",
    "            validation_split=0.1,\n",
    "            epochs=100,\n",
    "            batch_size=50,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        model.save('models/gru_'+f\"{symbol_to_fetch}\"+\"_model.h5\")\n",
    "        model.save('models/gru_'+f\"{symbol_to_fetch}\"+\"_model.keras\")\n",
    "        no_model = model\n",
    "    return no_model,X_test,test_data\n",
    "    #predictions\n",
    "def prepare_sentiment_from_transformer(symbol_to_fetch,start_date,end_date):\n",
    "    try :\n",
    "        \n",
    "        print(\"entered\")\n",
    "        model = tf.keras.models.load_model('models/gru_'+f\"{symbol_to_fetch}\"+\"_model.keras\")\n",
    "        print(\"passed\")\n",
    "        _,X_test,test_data = train_transformer(symbol_to_fetch = symbol_to_fetch, start_date = start_date, end_date = end_date,no_model=model)\n",
    "    \n",
    "    except:\n",
    "        model,X_test,test_data = train_transformer(symbol_to_fetch = symbol_to_fetch, start_date = start_date, end_date = end_date)\n",
    "    y_pred = model.predict(X_test) # this is the sentiment data \n",
    "    # Assuming you have X_test and y_test_data\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, test_data.iloc[:,-1])\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    test_data['transformer_sentiment'] = y_pred\n",
    "    \n",
    "    test_data.index = test_data.index.date\n",
    "    test_data.to_csv('data/transformer_sentiment.csv')\n",
    "    return test_data,'data/transformer_sentiment.csv'\n",
    "    \"\"\"next steps :  we need to additionally train the model if model is already present\n",
    "    or take nearly 30 stocks and train the model with the huge data \n",
    "    or take every 5 mins data nad trian with it, and at last mix the test data with day wise\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered\n",
      "41\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_18 (InputLayer)       [(None, 27, 1)]           0         \n",
      "                                                                 \n",
      " gru_44 (GRU)                (None, 27, 128)           50304     \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 27, 128)           0         \n",
      "                                                                 \n",
      " gru_45 (GRU)                (None, 27, 128)           99072     \n",
      "                                                                 \n",
      " dropout_55 (Dropout)        (None, 27, 128)           0         \n",
      "                                                                 \n",
      " gru_46 (GRU)                (None, 27, 128)           99072     \n",
      "                                                                 \n",
      " dropout_56 (Dropout)        (None, 27, 128)           0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 27, 256)           33024     \n",
      "                                                                 \n",
      " dropout_57 (Dropout)        (None, 27, 256)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6912)              0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1)                 6913      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 288385 (1.10 MB)\n",
      "Trainable params: 288385 (1.10 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "179/179 [==============================] - 63s 280ms/step - loss: 0.2894 - mse: 0.2894 - val_loss: 0.1098 - val_mse: 0.1098 - lr: 1.0000e-06\n",
      "Epoch 2/100\n",
      "179/179 [==============================] - 53s 298ms/step - loss: 0.2894 - mse: 0.2894 - val_loss: 0.1098 - val_mse: 0.1098 - lr: 3.4300e-05\n",
      "Epoch 3/100\n",
      "179/179 [==============================] - 41s 230ms/step - loss: 0.2894 - mse: 0.2894 - val_loss: 0.1098 - val_mse: 0.1098 - lr: 6.7600e-05\n",
      "Epoch 4/100\n",
      "179/179 [==============================] - 48s 270ms/step - loss: 0.2894 - mse: 0.2894 - val_loss: 0.1098 - val_mse: 0.1098 - lr: 1.0090e-04\n",
      "Epoch 5/100\n",
      "179/179 [==============================] - 47s 260ms/step - loss: 0.2894 - mse: 0.2894 - val_loss: 0.1098 - val_mse: 0.1098 - lr: 1.3420e-04\n",
      "Epoch 6/100\n",
      "179/179 [==============================] - 47s 263ms/step - loss: 0.2894 - mse: 0.2894 - val_loss: 0.1098 - val_mse: 0.1098 - lr: 1.6750e-04\n",
      "2/2 [==============================] - 4s 13ms/step\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.1707 - mse: 0.1707\n",
      "Test Loss: 0.1707\n",
      "Test Accuracy: 0.1707\n"
     ]
    }
   ],
   "source": [
    "# prepare_sentiment_from_transformer('AAPL','2020-05-01','2020-07-01')\n",
    "symbol_to_fetch = 'AAPL'\n",
    "start_date = '2020-05-01'\n",
    "end_date = '2020-07-01'\n",
    "try :\n",
    "    \n",
    "    print(\"entered\")\n",
    "    model = tf.keras.models.load_model('models/gru_'+f\"{symbol_to_fetch}\"+\"_model.keras\")\n",
    "    print(\"passed\")\n",
    "    _,X_test,test_data = train_transformer(symbol_to_fetch = symbol_to_fetch, start_date = start_date, end_date = end_date,no_model=model)\n",
    "\n",
    "except:\n",
    "    model,X_test,test_data = train_transformer(symbol_to_fetch = symbol_to_fetch, start_date = start_date, end_date = end_date)\n",
    "y_pred = model.predict(X_test) # this is the sentiment data \n",
    "# Assuming you have X_test and y_test_data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, test_data.iloc[:,-1])\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('models/gru_AAPL_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_18 (InputLayer)       [(None, 27, 1)]           0         \n",
      "                                                                 \n",
      " gru_44 (GRU)                (None, 27, 128)           50304     \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 27, 128)           0         \n",
      "                                                                 \n",
      " gru_45 (GRU)                (None, 27, 128)           99072     \n",
      "                                                                 \n",
      " dropout_55 (Dropout)        (None, 27, 128)           0         \n",
      "                                                                 \n",
      " gru_46 (GRU)                (None, 27, 128)           99072     \n",
      "                                                                 \n",
      " dropout_56 (Dropout)        (None, 27, 128)           0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 27, 256)           33024     \n",
      "                                                                 \n",
      " dropout_57 (Dropout)        (None, 27, 256)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6912)              0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1)                 6913      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 288385 (1.10 MB)\n",
      "Trainable params: 288385 (1.10 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'next steps :  we need to additionally train the model if model is already present\\nor take nearly 30 stocks and train the model with the huge data \\nor take every 5 mins data nad trian with it, and at last mix the test data with day wise'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_data['transformer_sentiment'] = y_pred\n",
    "\n",
    "test_data.index = test_data.index.date\n",
    "test_data.to_csv('data/transformer_sentiment.csv')\n",
    "# return test_data,'data/transformer_sentiment.csv'\n",
    "\"\"\"next steps :  we need to additionally train the model if model is already present\n",
    "or take nearly 30 stocks and train the model with the huge data \n",
    "or take every 5 mins data nad trian with it, and at last mix the test data with day wise\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (20, 27, 1), indices imply (41, 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\IPython\\core\\formatters.py:711\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    704\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[0;32m    705\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[0;32m    707\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[0;32m    708\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[0;32m    709\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[0;32m    710\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[1;32m--> 711\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    712\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\IPython\\lib\\pretty.py:411\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    408\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[0;32m    409\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[0;32m    410\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m--> 411\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\IPython\\lib\\pretty.py:779\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[1;34m(obj, p, cycle)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[1;32m--> 779\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\frame.py:1214\u001b[0m, in \u001b[0;36mDataFrame.__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m   1213\u001b[0m repr_params \u001b[38;5;241m=\u001b[39m fmt\u001b[38;5;241m.\u001b[39mget_dataframe_repr_params()\n\u001b[1;32m-> 1214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_string\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrepr_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\frame.py:1376\u001b[0m, in \u001b[0;36mDataFrame.to_string\u001b[1;34m(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, max_cols, show_dimensions, decimal, line_width, min_rows, max_colwidth, encoding)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m option_context\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.max_colwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_colwidth):\n\u001b[1;32m-> 1376\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43mfmt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrameFormatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcol_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformatters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfloat_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfloat_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparsify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparsify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjustify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjustify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_dimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_dimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1393\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fmt\u001b[38;5;241m.\u001b[39mDataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_string(\n\u001b[0;32m   1395\u001b[0m         buf\u001b[38;5;241m=\u001b[39mbuf,\n\u001b[0;32m   1396\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   1397\u001b[0m         line_width\u001b[38;5;241m=\u001b[39mline_width,\n\u001b[0;32m   1398\u001b[0m     )\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\io\\formats\\format.py:469\u001b[0m, in \u001b[0;36mDataFrameFormatter.__init__\u001b[1;34m(self, frame, columns, col_space, header, index, na_rep, formatters, justify, float_format, sparsify, index_names, max_rows, min_rows, max_cols, show_dimensions, decimal, bold_rows, escape)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rows_fitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_max_rows_fitted()\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtr_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\n\u001b[1;32m--> 469\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj \u001b[38;5;241m=\u001b[39m printing\u001b[38;5;241m.\u001b[39mget_adjustment()\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\io\\formats\\format.py:655\u001b[0m, in \u001b[0;36mDataFrameFormatter.truncate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;124;03mCheck whether the frame should be truncated. If so, slice the frame up.\u001b[39;00m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_truncated_horizontally:\n\u001b[1;32m--> 655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_truncate_horizontally\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_truncated_vertically:\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_vertically()\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\io\\formats\\format.py:673\u001b[0m, in \u001b[0;36mDataFrameFormatter._truncate_horizontally\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtr_frame\u001b[38;5;241m.\u001b[39miloc[:, :col_num]\n\u001b[0;32m    672\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtr_frame\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39mcol_num:]\n\u001b[1;32m--> 673\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtr_frame \u001b[38;5;241m=\u001b[39m \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;66;03m# truncate formatter\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatters, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:132\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concat_axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    131\u001b[0m     mgrs \u001b[38;5;241m=\u001b[39m _maybe_reindex_columns_na_proxy(axes, mgrs_indexers, needs_copy)\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmgrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat_horizontal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmgrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnblocks \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    135\u001b[0m     first_dtype \u001b[38;5;241m=\u001b[39m mgrs_indexers[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1814\u001b[0m, in \u001b[0;36mBlockManager.concat_horizontal\u001b[1;34m(cls, mgrs, axes)\u001b[0m\n\u001b[0;32m   1810\u001b[0m         blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m   1812\u001b[0m     offset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(mgr\u001b[38;5;241m.\u001b[39mitems)\n\u001b[1;32m-> 1814\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgr\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:933\u001b[0m, in \u001b[0;36mBlockManager.__init__\u001b[1;34m(self, blocks, axes, verify_integrity)\u001b[0m\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    926\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Block dimensions (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must equal \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    927\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of axes (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    928\u001b[0m         )\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;66;03m# As of 2.0, the caller is responsible for ensuring that\u001b[39;00m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;66;03m#  DatetimeTZBlock with block.ndim == 2 has block.values.ndim ==2;\u001b[39;00m\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;66;03m#  previously there was a special check for fastparquet compat.\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_verify_integrity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:940\u001b[0m, in \u001b[0;36mBlockManager._verify_integrity\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m block\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m!=\u001b[39m mgr_shape[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m--> 940\u001b[0m         \u001b[43mraise_construction_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtot_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems) \u001b[38;5;241m!=\u001b[39m tot_items:\n\u001b[0;32m    942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    943\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of manager items must equal union of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    944\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock items\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# manager items: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, # \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    945\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtot_items: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtot_items\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    946\u001b[0m     )\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2171\u001b[0m, in \u001b[0;36mraise_construction_error\u001b[1;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[0;32m   2169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty data passed with indices specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (20, 27, 1), indices imply (41, 20)"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (20, 27, 1), indices imply (41, 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\IPython\\core\\formatters.py:347\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    345\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 347\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\frame.py:1236\u001b[0m, in \u001b[0;36mDataFrame._repr_html_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1233\u001b[0m     max_cols \u001b[38;5;241m=\u001b[39m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1234\u001b[0m     show_dimensions \u001b[38;5;241m=\u001b[39m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.show_dimensions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1236\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43mfmt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrameFormatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNaN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformatters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfloat_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparsify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjustify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbold_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mescape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_dimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_dimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fmt\u001b[38;5;241m.\u001b[39mDataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_html(notebook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\io\\formats\\format.py:469\u001b[0m, in \u001b[0;36mDataFrameFormatter.__init__\u001b[1;34m(self, frame, columns, col_space, header, index, na_rep, formatters, justify, float_format, sparsify, index_names, max_rows, min_rows, max_cols, show_dimensions, decimal, bold_rows, escape)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rows_fitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_max_rows_fitted()\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtr_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\n\u001b[1;32m--> 469\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj \u001b[38;5;241m=\u001b[39m printing\u001b[38;5;241m.\u001b[39mget_adjustment()\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\io\\formats\\format.py:655\u001b[0m, in \u001b[0;36mDataFrameFormatter.truncate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;124;03mCheck whether the frame should be truncated. If so, slice the frame up.\u001b[39;00m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_truncated_horizontally:\n\u001b[1;32m--> 655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_truncate_horizontally\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_truncated_vertically:\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_vertically()\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\io\\formats\\format.py:673\u001b[0m, in \u001b[0;36mDataFrameFormatter._truncate_horizontally\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtr_frame\u001b[38;5;241m.\u001b[39miloc[:, :col_num]\n\u001b[0;32m    672\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtr_frame\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39mcol_num:]\n\u001b[1;32m--> 673\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtr_frame \u001b[38;5;241m=\u001b[39m \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;66;03m# truncate formatter\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatters, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:132\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concat_axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    131\u001b[0m     mgrs \u001b[38;5;241m=\u001b[39m _maybe_reindex_columns_na_proxy(axes, mgrs_indexers, needs_copy)\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmgrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat_horizontal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmgrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnblocks \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    135\u001b[0m     first_dtype \u001b[38;5;241m=\u001b[39m mgrs_indexers[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1814\u001b[0m, in \u001b[0;36mBlockManager.concat_horizontal\u001b[1;34m(cls, mgrs, axes)\u001b[0m\n\u001b[0;32m   1810\u001b[0m         blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m   1812\u001b[0m     offset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(mgr\u001b[38;5;241m.\u001b[39mitems)\n\u001b[1;32m-> 1814\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgr\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:933\u001b[0m, in \u001b[0;36mBlockManager.__init__\u001b[1;34m(self, blocks, axes, verify_integrity)\u001b[0m\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    926\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Block dimensions (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must equal \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    927\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of axes (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    928\u001b[0m         )\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;66;03m# As of 2.0, the caller is responsible for ensuring that\u001b[39;00m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;66;03m#  DatetimeTZBlock with block.ndim == 2 has block.values.ndim ==2;\u001b[39;00m\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;66;03m#  previously there was a special check for fastparquet compat.\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_verify_integrity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:940\u001b[0m, in \u001b[0;36mBlockManager._verify_integrity\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m block\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m!=\u001b[39m mgr_shape[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m--> 940\u001b[0m         \u001b[43mraise_construction_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtot_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems) \u001b[38;5;241m!=\u001b[39m tot_items:\n\u001b[0;32m    942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    943\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of manager items must equal union of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    944\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock items\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# manager items: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, # \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    945\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtot_items: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtot_items\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    946\u001b[0m     )\n",
      "File \u001b[1;32md:\\softwares\\anaconda\\envs\\project\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2171\u001b[0m, in \u001b[0;36mraise_construction_error\u001b[1;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[0;32m   2169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty data passed with indices specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (20, 27, 1), indices imply (41, 20)"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "#fetching data \n",
    "stock = fetch_ticker_data(symbol_to_fetch, start_date, end_date)\n",
    "\n",
    "# Calculate deltas for open, high, low, and close columns\n",
    "for i in range(1, 6):  # Calculate deltas up to 5 days\n",
    "    stock[f\"open_delta_{i}day\"] = stock[\"Open\"].diff(periods=i)\n",
    "    stock[f\"high_delta_{i}day\"] = stock[\"High\"].diff(periods=i)\n",
    "    stock[f\"low_delta_{i}day\"] = stock[\"Low\"].diff(periods=i)\n",
    "    stock[f\"close_delta_{i}day\"] = stock[\"Close\"].diff(periods=i)\n",
    "    \n",
    "stock = stock.fillna(method=\"ffill\", axis=0)\n",
    "stock = stock.fillna(method=\"bfill\", axis=0)\n",
    "# stock.index = stock.index.date\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data_index = np.searchsorted(stock.index.values, np.datetime64(start_date))\n",
    "train_data = stock.iloc[:train_data_index]\n",
    "test_data = stock.loc[start_date:]\n",
    "train_data = label_data(train_data)\n",
    "test_data = label_data(test_data)\n",
    "\n",
    "#trian & test data\n",
    "X_train_data = train_data.iloc[:,:-1]\n",
    "y_train_data = train_data.iloc[:,-1]\n",
    "X_test_data = test_data.iloc[:,:-1]\n",
    "y_test_data = test_data.iloc[:,-1]\n",
    "print(len(X_test_data))\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_train_data_normalizer = normalizer.fit_transform(X_train_data)\n",
    "X_test_data_normalizer = normalizer.transform(X_test_data)\n",
    "\n",
    "# # Reshape X_train_data_normalizer\n",
    "X_train = X_train_data_normalizer.reshape(X_train_data_normalizer.shape[0], X_train_data_normalizer.shape[1], 1)\n",
    "X_test = X_test_data_normalizer.reshape(X_test_data_normalizer.shape[0], X_test_data_normalizer.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "1980-12-12 00:00:00-05:00    0\n",
       "1980-12-15 00:00:00-05:00    0\n",
       "1980-12-16 00:00:00-05:00    1\n",
       "1980-12-17 00:00:00-05:00    2\n",
       "1980-12-18 00:00:00-05:00    2\n",
       "                            ..\n",
       "2020-04-23 00:00:00-04:00    2\n",
       "2020-04-24 00:00:00-04:00    1\n",
       "2020-04-27 00:00:00-04:00    1\n",
       "2020-04-28 00:00:00-04:00    2\n",
       "2020-04-29 00:00:00-04:00    1\n",
       "Name: Sentiment, Length: 9929, dtype: int32"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43my_test_data\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test_data' is not defined"
     ]
    }
   ],
   "source": [
    "y_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
